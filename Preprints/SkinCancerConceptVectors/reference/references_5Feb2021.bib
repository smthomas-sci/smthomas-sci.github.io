
@article{10.1001/jama.2018.11029,
  title = {Clinical Implications and Challenges of Artificial Intelligence and Deep {{LearningClinical}} Implications and Challenges of Artificial Intelligence and Deep {{LearningEditorial}}},
  author = {Stead, William W},
  date = {2018},
  journaltitle = {JAMA},
  volume = {320},
  pages = {1107--1108},
  issn = {0098-7484},
  doi = {10.1001/jama.2018.11029},
  url = {https://doi.org/10.1001/jama.2018.11029},
  abstract = {Artificial intelligence (AI) and deep learning are entering the mainstream of clinical medicine. For example, in December 2016, Gulshan et al reported development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. An accompanying editorial by Wong and Bressler pointed out limits of the study, the need for further validation of the algorithm in different populations, and unresolved challenges (eg, incorporating the algorithm into clinical work flows and convincing clinicians and patients to “trust a ‘black box'”). Sixteen months later, the Food and Drug Administration (FDA) permitted marketing of the first medical device to use AI to detect diabetic retinopathy. FDA reduced the risk of releasing the device by limiting the indication for use to screening adults who do not have visual symptoms for greater than mild retinopathy, to refer them to an eye care specialist.},
  number = {11}
}

@article{10.1001/jama.292.20.2500,
  title = {Ownership and Use of Tissue Specimens for Research},
  author = {Hakimian, Rina and Korn, David},
  date = {2004},
  journaltitle = {JAMA},
  volume = {292},
  pages = {2500--2505},
  issn = {0098-7484},
  doi = {10.1001/jama.292.20.2500},
  url = {https://doi.org/10.1001/jama.292.20.2500},
  abstract = {Academic and industrial scientists have sharply increased their demand for properly prepared and clinically annotated tissue samples that yield valuable insights into the origins and expressions of human disease. Historically, research on human tissue samples has been relatively unencumbered by federal regulations and occurred without delineation of ownership rights to the specimens, patient data, or research products. As regulations have become increasingly restrictive, and because clear ownership interests have never been established, the presumed right of researchers and institutions to collect, use, and dispose of specimens and their associated patient data has remained undefined and occasionally contentious. Recent examination of these issues by a US federal court resulted in a ruling that individuals do not retain rights of ownership or control of biological materials contributed for research, regardless of whether commercial benefit accrues. This article examines the legal, regulatory, and ethical framework within which human tissue research is currently conducted. We contend that because the benefits of medical knowledge derived from tissue research potentially accrue to all individuals and future generations (rather than a single recipient), society may justify an expansive use of these valuable resources for future studies.},
  number = {20}
}

@inproceedings{10.1007/978-981-10-8944-2_31,
  title = {Entity Recognition Approach of Clinical Documents Based on Self-Training Framework},
  booktitle = {Recent Developments in Intelligent Computing, Communication and Devices},
  author = {Che, Nannan and Chen, Dehua and Le, Jiajin},
  editor = {Patnaik, Srikanta and Jain, Vipul},
  date = {2019},
  pages = {259--265},
  publisher = {{Springer Singapore}},
  location = {{Singapore}},
  abstract = {Entity recognition of clinical documents is a primary task to extract information from unstructured clinical documents. Traditional entity recognition methods extract entities in a supervised learning framework which needs a large scale of labeled corpus as the training samples. However, clinical documents in real world are unlabeled. To construct a large scale of labeled corpus by manual is time-consuming. Semi-supervised learning that relies on small-scale corpus can solve such problem. Thus, this paper proposes an entity recognition model of clinical documents based on self-training framework. In such framework, we first establish partial annotation corpus through the way of dependency syntax analysis and the medical statement rule unifies. Then, a hybrid model of CNN-LSTM-CRF is proposed to label the unlabeled data in an end-to-end way. Specially, we will use CNN to embed characters in clinical document and use Bi-LSTM to extract the sentence-level feature. At the moment, we use CRF remedies the shortage of LSTM which further combined with the combination probability of CRF and the advantages of optimizing the whole sequence. Finally, the results of entity recognition with higher confidence level are fed back by self-training to expend size of corpus which improves the accuracy of the document entity recognition. The experiment result proves the availability and high efficiency of this model.},
  isbn = {978-981-10-8944-2}
}

@article{10.1093/oxfordjournals.bmb.a072482,
  title = {Changing Role of the Pathologist},
  author = {Sloane, J P},
  date = {1991-06},
  journaltitle = {British Medical Bulletin},
  volume = {47},
  pages = {433--454},
  issn = {0007-1420},
  doi = {10.1093/oxfordjournals.bmb.a072482},
  url = {https://doi.org/10.1093/oxfordjournals.bmb.a072482},
  abstract = {The pathologist's role in diagnosing and managing patients with breast disorders is changing. The introduction of mammographic screening has altered the spectrum of histological changes presenting to histopathologists for diagnosis, with a greater emphasis on ‘borderline’ lesions, and has increased the need for specimen radiography as part of the gross examination of biopsy specimens. Reporting prognostic features of carcinomas is required as they may give an early indication of the impact of screening and increased cytological services are needed to reduce the need for open biopsy. Together with a recent Government White Paper, screening has also stimulated various quality assurance initiatives in breast pathology. Increased therapeutic options with the associated need to counsel patients demand more comprehensive pathological services—particularly pre-opervative cytological diagnosis, reporting of prognostic features and determining the adequacy of excision of carcinomas. Finally, the management of patients with primarily benign biopsies may be influenced by the detection of various neoplastic or pre-neoplastic lesions of microscopic size by the pathologist.},
  eprint = {https://academic.oup.com/bmb/article-pdf/47/2/433/7292333/47-2-433.pdf},
  number = {2}
}

@inproceedings{Abadi2016,
  title = {{{TensorFlow}}: {{A}} System for Large-Scale Machine Learning},
  booktitle = {{{OSDI}}},
  author = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael},
  date = {2016},
  volume = {16},
  pages = {265--283}
}

@inproceedings{abdal2019image2stylegan,
  title = {Image2stylegan: {{How}} to Embed Images into the Stylegan Latent Space?},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision},
  author = {Abdal, Rameen and Qin, Yipeng and Wonka, Peter},
  date = {2019},
  pages = {4432--4441}
}

@article{Abels2017,
  title = {Current State of the Regulatory Trajectory for Whole Slide Imaging Devices in the {{USA}}},
  author = {Abels, Esther and Pantanowitz, Liron},
  date = {2017-01},
  journaltitle = {Journal of Pathology Informatics},
  volume = {8},
  pages = {23},
  doi = {10.4103/jpi.jpi_11_17},
  url = {http://www.jpathinformatics.org/article.asp?issn=2153-3539 year=2017 volume=8 issue=1 spage=23 epage=23 aulast=Abels},
  abstract = {The regulatory field for digital pathology (DP) has advanced significantly. A major milestone was accomplished when the FDA allowed the first vendor to market their device for primary diagnostic use in the USA and published in the classification order that this device, and substantially equivalent devices of this generic type, should be classified into class II instead of class III as previously proposed. The Digital Pathology Association (DPA) regulatory task force had a major role in the accomplishment of getting the application request for Whole Slide Imaging (WSI) Systems recommended for a \emph{de novo}. This article reviews the past and emerging regulatory environment of WSI for clinical use in the USA. A WSI system with integrated subsystems is defined in the context of medical device regulations. The FDA technical performance assessment guideline is also discussed as well as parameters involved in analytical testing and clinical studies to demonstrate that WSI devices are safe and effective for clinical use.},
  number = {1}
}

@article{Acs2018,
  title = {Not Just Digital Pathology, Intelligent Digital Pathology},
  author = {Acs, Balazs and Rimm, David L},
  date = {2018},
  journaltitle = {JAMA oncology},
  volume = {4},
  pages = {403--404},
  publisher = {{American Medical Association}},
  issn = {2374-2437},
  number = {3}
}

@inproceedings{Adebayo2018,
  title = {Sanity Checks for Saliency Maps},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  date = {2018},
  pages = {9505--9515}
}

@inproceedings{Andrearczyk,
  title = {Deep Learning for Biomedical Texture Image Analysis},
  booktitle = {Proceedings of the Irish Machine Vision \& Image Processing Conference},
  author = {Andrearczyk, Vincent and Whelan, Paul F},
  publisher = {{Irish Pattern Recognition \& Classification Society (IPRCS)}},
  isbn = {0-9934207-2-9}
}

@article{Antoniou2010,
  title = {Reflections of the {{Hippocratic Oath}} in Modern Medicine.},
  author = {Antoniou, Stavros A and Antoniou, George A and Granderath, Frank A and Mavroforou, Anna and Giannoukas, Athanasios D and Antoniou, Athanasios I},
  date = {2010-12},
  journaltitle = {World journal of surgery},
  volume = {34},
  pages = {3075--3079},
  issn = {1432-2323 (Electronic)},
  doi = {10.1007/s00268-010-0604-3},
  abstract = {Hippocratic Oath indicates a prevailing ethos rather than a professional approach, and it is still regarded as the cornerstone and foundation of the medical profession. Medicine in Ancient Greece was strongly influenced by the values of classical philosophy as introduced by its main representatives: Plato and Aristotle. Hippocrates himself has been recognized not only as a pioneering physician, but also as an outstanding philosopher. In his writings, he claimed that "the physician must insert wisdom in medicine" and denounced the technocratic aspect of the medical profession. The Hippocratic Oath constitutes a synopsis of the moral code of Ancient Greek medicine and contributes to the stabilization of the tri-part relationship among the physician, the patient, and the illness, as described by Hippocrates. The harmony of this interactive triangle has been deranged by several factors, such as technological evolution, public media, and cost-effective modalities with multiple consequences. In these terms, the reevaluation of the Hippocratic Oath and its time-enduring messages seems essential to reinstate the relationship between the physician and the patient under a new philosophico-medical prism.},
  eprint = {20814679},
  eprinttype = {pmid},
  keywords = {Ancient,Ethics,Hippocratic Oath,History,Humans,Medical,Moral Obligations,Philosophy},
  langid = {english},
  number = {12}
}

@inproceedings{Antony2016-wk,
  title = {Quantifying Radiographic Knee Osteoarthritis Severity Using Deep Convolutional Neural Networks},
  booktitle = {2016 23rd International Conference on Pattern Recognition (\{\vphantom\}{{ICPR}}\vphantom\{\})},
  author = {Antony, J and McGuinness, K and O'Connor, N E and Moran, K},
  date = {2016-12},
  pages = {1195--1200},
  abstract = {This paper proposes a new approach to automatically quantify the severity of knee osteoarthritis (OA) from radiographs using deep convolutional neural networks (CNN). Clinically, knee OA severity is assessed using Kellgren \& Lawrence (KL) grades, a five point scale. Previous work on automatically predicting KL grades from radiograph images were based on training shallow classifiers using a variety of hand engineered features. We demonstrate that classification accuracy can be significantly improved using deep convolutional neural network models pre-trained on ImageNet and fine-tuned on knee OA images. Furthermore, we argue that it is more appropriate to assess the accuracy of automatic knee OA severity predictions using a continuous distance-based evaluation metric like mean squared error than it is to use classification accuracy. This leads to the formulation of the prediction of KL grades as a regression problem and further improves accuracy. Results on a dataset of X-ray images and KL grades from the Osteoarthritis Initiative (OAI) show a sizable improvement over the current state-of-the-art.},
  keywords = {image classification,medical image p,X-ray imaging}
}

@article{Anwar2017,
  title = {Structured Pruning of Deep Convolutional Neural Networks},
  author = {Anwar, Sajid and Hwang, Kyuyeon and Sung, Wonyong},
  date = {2017},
  journaltitle = {ACM Journal on Emerging Technologies in Computing Systems (JETC)},
  volume = {13},
  pages = {32},
  publisher = {{ACM}},
  issn = {1550-4832},
  number = {3}
}

@article{Arevalo2015-ww,
  title = {An Unsupervised Feature Learning Framework for Basal Cell Carcinoma Image Analysis},
  author = {Arevalo, John and Cruz-Roa, Angel and Arias, Viviana and Romero, Eduardo and González, Fabio A},
  date = {2015-06},
  journaltitle = {Artificial Intelligence in Medicine},
  shortjournal = {Artif. Intell. Med.},
  volume = {64},
  pages = {131--145},
  abstract = {OBJECTIVE: The paper addresses the problem of automatic detection of basal cell carcinoma (BCC) in histopathology images. In particular, it proposes a framework to both, learn the image representation in an unsupervised way and visualize discriminative features supported by the learned model. MATERIALS AND METHODS: This paper presents an integrated unsupervised feature learning (UFL) framework for histopathology image analysis that comprises three main stages: (1) local (patch) representation learning using different strategies (sparse autoencoders, reconstruct independent component analysis and topographic independent component analysis (TICA), (2) global (image) representation learning using a bag-of-features representation or a convolutional neural network, and (3) a visual interpretation layer to highlight the most discriminant regions detected by the model. The integrated unsupervised feature learning framework was exhaustively evaluated in a histopathology image dataset for BCC diagnosis. RESULTS: The experimental evaluation produced a classification performance of 98.1\%, in terms of the area under receiver-operating-characteristic curve, for the proposed framework outperforming by 7\% the state-of-the-art discrete cosine transform patch-based representation. CONCLUSIONS: The proposed UFL-representation-based approach outperforms state-of-the-art methods for BCC detection. Thanks to its visual interpretation layer, the method is able to highlight discriminative tissue regions providing a better diagnosis support. Among the different UFL strategies tested, TICA-learned features exhibited the best performance thanks to its ability to capture low-level invariances, which are inherent to the nature of the problem.},
  keywords = {Basal cell carcinoma,Digital pathology,Represent},
  number = {2}
}

@online{Arjovsky2017,
  title = {Wasserstein Gan},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
  date = {2017},
  archivePrefix = {arXiv},
  eprint = {1701.07875},
  eprinttype = {arxiv}
}

@article{ArXiv2019,
  title = {{{arXiv}} Usage Statistics},
  author = {{ArXiv}},
  date = {2019},
  url = {https://arxiv.org/help/stats},
  urldate = {2019-08-08}
}

@report{Australian_Institute_of_Health_and_Welfare2016-bw,
  title = {Skin Cancer in Australia},
  author = {{Australian Institute of Health and Welfare}},
  date = {2016},
  volume = {Cat. no. C},
  institution = {{Australian Institute of Health and Welfare}},
  location = {{Canberra}},
  number = {CAN 96 AIHW}
}

@article{AustralianMedicalAssociation2014,
  title = {Clinical Images and the Use of Personal Mobile Devices. {{A}} Guide for Medical Students and Doctors.},
  author = {{Australian Medical Association} and {Medical Indemnity Industry Association of Australia}},
  date = {2014},
  pages = {18},
  url = {https://ama.com.au/sites/default/files/documents/FINAL\{_\}AMA\{_\}Clinical\{_\}Images\{_\}Guide.pdf}
}

@article{ayhan2018test,
  title = {Test-Time Data Augmentation for Estimation of Heteroscedastic Aleatoric Uncertainty in Deep Neural Networks},
  author = {Ayhan, Murat Seckin and Berens, Philipp},
  date = {2018}
}

@inproceedings{Babaie2017-qm,
  title = {Classification and Retrieval of Digital Pathology Scans: {{A}} New Dataset},
  booktitle = {2017 \{\vphantom\}{{IEEE}}\vphantom\{\} Conference on Computer Vision and Pattern Recognition Workshops (\{\vphantom\}{{CVPRW}}\vphantom\{\})},
  author = {Babaie, M and Kalra, S and Sriram, A and Mitcheltree, C and Zhu, S and Khatami, A and Rahnamayan, S and Tizhoosh, H R},
  date = {2017-07},
  pages = {760--768},
  abstract = {In this paper, we introduce a new dataset, Kimia Path24, for image classification and retrieval in digital pathology. We use the whole scan images of 24 different tissue textures to generate 1,325 test patches of size 1000x1000 (0.5mm x 0.5mm). Training data can be generated according to preferences of algorithm designer and can range from approximately 27,000 to over 50,000 patches if the preset parameters are adopted. We propose a compound patch-and-scan accuracy measurement that makes achieving high accuracies quite challenging. In addition, we set the benchmarking line by applying LBP, dictionary approach and convolutional neural nets (CNNs) and report their results. The highest accuracy was 41.80\% for CNN.},
  keywords = {biological tissues,feedforward neural nets,image c}
}

@inproceedings{Babaie2017-wo,
  title = {Classification and Retrieval of Digital Pathology Scans: {{A}} New Dataset},
  booktitle = {2017 \{\vphantom\}{{IEEE}}\vphantom\{\} Conference on Computer Vision and Pattern Recognition Workshops (\{\vphantom\}{{CVPRW}}\vphantom\{\})},
  author = {Babaie, M and Kalra, S and Sriram, A and Mitcheltree, C and Zhu, S and Khatami, A and Rahnamayan, S and Tizhoosh, H R},
  date = {2017-07},
  pages = {760--768},
  abstract = {In this paper, we introduce a new dataset, Kimia Path24, for image classification and retrieval in digital pathology. We use the whole scan images of 24 different tissue textures to generate 1,325 test patches of size 1000x1000 (0.5mm x 0.5mm). Training data can be generated according to preferences of algorithm designer and can range from approximately 27,000 to over 50,000 patches if the preset parameters are adopted. We propose a compound patch-and-scan accuracy measurement that makes achieving high accuracies quite challenging. In addition, we set the benchmarking line by applying LBP, dictionary approach and convolutional neural nets (CNNs) and report their results. The highest accuracy was 41.80\% for CNN.},
  keywords = {biological tissues,feedforward neural nets,image c}
}

@article{badgeley2019deep,
  title = {Deep Learning Predicts Hip Fracture Using Confounding Patient and Healthcare Variables},
  author = {Badgeley, Marcus A and Zech, John R and Oakden-Rayner, Luke and Glicksberg, Benjamin S and Liu, Manway and Gale, William and McConnell, Michael V and Percha, Bethany and Snyder, Thomas M and Dudley, Joel T},
  date = {2019},
  journaltitle = {NPJ digital medicine},
  volume = {2},
  pages = {1--10},
  publisher = {{Nature Publishing Group}},
  number = {1}
}

@book{Bancroft2008-ze,
  title = {Theory and Practice of Histological Techniques},
  author = {Bancroft, John D},
  date = {2008-01},
  publisher = {{Elsevier Health Sciences}},
  abstract = {Here's the latest edition of the leading reference work on histological techniques. This book is an essential and invaluable resource no matter what part you play in histological preparations and applications, whether you're a student or a highly experienced laboratory professional. This is a one stop reference book that will help you at the beginning of your career and which will remain valuable to you, even as you gain experience in daily practice.Contributions from almost 30 expert contributors involved in teaching, research and practice provide a well-rounded perspective.Extensive summary tables, charts and boxes make reference easy Color photomicrographs provide visual guidance on the latest techniques.Expanded coverage of immunohistochemistry helps you make optimal use of this crucial approach.New coverage of molecular genetics as it applies to histopathological diagnoses. 5 brand new chapters on Ergonomics, Laser Capture, Tissue Microarray, GrossingTechniques, and Genetics keep you up to speed on the latest techniques. New material on Quality Assurance and Audit facilitates day-to-day laboratory management.}
}

@article{Bankier2010,
  title = {Consensus Interpretation in Imaging Research: {{Is}} There a Better Way?},
  author = {Bankier, Alexander A and Levine, Deborah and Halpern, Elkan F and Kressel, Herbert Y},
  date = {2010},
  journaltitle = {Radiology},
  volume = {257},
  pages = {14--17},
  doi = {10.1148/radiol.10100252},
  url = {http://pubs.rsna.org/doi/abs/10.1148/radiol.10100252},
  number = {1}
}

@inproceedings{Bar,
  title = {Chest Pathology Detection Using Deep Learning with Non-Medical Training},
  booktitle = {Biomedical Imaging ({{ISBI}}), 2015 {{IEEE}} 12th International Symposium On},
  author = {Bar, Yaniv and Diamant, Idit and Wolf, Lior and Lieberman, Sivan and Konen, Eli and Greenspan, Hayit},
  pages = {294--297},
  publisher = {{IEEE}},
  isbn = {1-4799-2374-5}
}

@article{Basavanhally2013-gz,
  title = {Multi-Field-of-View Framework for Distinguishing Tumor Grade in \{\vphantom\}{{ER}}+\vphantom\{\} Breast Cancer from Entire Histopathology Slides},
  author = {Basavanhally, Ajay and Ganesan, Shridar and Feldman, Michael and Shih, Natalie and Mies, Carolyn and Tomaszewski, John and Madabhushi, Anant},
  date = {2013-08},
  journaltitle = {IEEE Transactions on Biomedical Engineering},
  shortjournal = {IEEE Trans. Biomed. Eng.},
  volume = {60},
  pages = {2089--2099},
  abstract = {Modified Bloom-Richardson (mBR) grading is known to have prognostic value in breast cancer (BCa), yet its use in clinical practice has been limited by intra- and interobserver variability. The development of a computerized system to distinguish mBR grade from entire estrogen receptor-positive (ER+) BCa histopathology slides will help clinicians identify grading discrepancies and improve overall confidence in the diagnostic result. In this paper, we isolate salient image features characterizing tumor morphology and texture to differentiate entire hematoxylin and eosin (H and E) stained histopathology slides based on mBR grade. The features are used in conjunction with a novel multi-field-of-view (multi-FOV) classifier–a whole-slide classifier that extracts features from a multitude of FOVs of varying sizes–to identify important image features at different FOV sizes. Image features utilized include those related to the spatial arrangement of cancer nuclei (i.e., nuclear architecture) and the textural patterns within nuclei (i.e., nuclear texture). Using slides from 126 ER+ patients (46 low, 60 intermediate, and 20 high mBR grade), our grading system was able to distinguish low versus high, low versus intermediate, and intermediate versus high grade patients with area under curve values of 0.93, 0.72, and 0.74, respectively. Our results suggest that the multi-FOV classifier is able to 1) successfully discriminate low, medium, and high mBR grade and 2) identify specific image features at different FOV sizes that are important for distinguishing mBR grade in H and E stained ER+ BCa histology slides.},
  number = {8}
}

@inproceedings{Bayramoglu2016-is,
  title = {Transfer Learning for Cell Nuclei Classification in Histopathology Images},
  booktitle = {Computer Vision – \{\vphantom\}{{ECCV}}\vphantom\{\} 2016 Workshops},
  author = {Bayramoglu, Neslihan and Heikkilä, Janne},
  date = {2016-10},
  pages = {532--539},
  publisher = {{Springer, Cham}},
  abstract = {In histopathological image assessment, there is a high demand to obtain fast and precise quantification automatically. Such automation could be beneficial to find clinical assessment clues to produce correct diagnoses, to reduce observer variability, and to increase objectivity. Due to its success in other areas, deep learning could be the key method to obtain clinical acceptance. However, the major bottleneck is how to train a deep CNN model with a limited amount of training data. There is one important question of critical importance: Could it be possible to use transfer learning and fine-tuning in biomedical image analysis to reduce the effort of manual data labeling and still obtain a full deep representation for the target task? In this study, we address this question quantitatively by comparing the performances of transfer learning and learning from scratch for cell nuclei classification. We evaluate four different CNN architectures trained on natural images and facial images.},
  series = {Lecture Notes in Computer Science}
}

@article{Beck2011-af,
  title = {Systematic Analysis of Breast Cancer Morphology Uncovers Stromal Features Associated with Survival},
  author = {Beck, Andrew H and Sangoi, Ankur R and Leung, Samuel and Marinelli, Robert J and Nielsen, Torsten O and van de Vijver, Marc J and West, Robert B and van de Rijn, Matt and Koller, Daphne},
  date = {2011-11},
  journaltitle = {Science Translational Medicine},
  shortjournal = {Sci. Transl. Med.},
  volume = {3},
  pages = {108ra113},
  abstract = {The morphological interpretation of histologic sections forms the basis of diagnosis and prognostication for cancer. In the diagnosis of carcinomas, pathologists perform a semiquantitative analysis of a small set of morphological features to determine the cancer's histologic grade. Physicians use histologic grade to inform their assessment of a carcinoma's aggressiveness and a patient's prognosis. Nevertheless, the determination of grade in breast cancer examines only a small set of morphological features of breast cancer epithelial cells, which has been largely unchanged since the 1920s. A comprehensive analysis of automatically quantitated morphological features could identify characteristics of prognostic relevance and provide an accurate and reproducible means for assessing prognosis from microscopic image data. We developed the C-Path (Computational Pathologist) system to measure a rich quantitative feature set from the breast cancer epithelium and stroma (6642 features), including both standard morphometric descriptors of image objects and higher-level contextual, relational, and global image features. These measurements were used to construct a prognostic model. We applied the C-Path system to microscopic images from two independent cohorts of breast cancer patients [from the Netherlands Cancer Institute (NKI) cohort, n = 248, and the Vancouver General Hospital (VGH) cohort, n = 328]. The prognostic model score generated by our system was strongly associated with overall survival in both the NKI and the VGH cohorts (both log-rank P \$\$ 0.001). This association was independent of clinical, pathological, and molecular factors. Three stromal features were significantly associated with survival, and this association was stronger than the association of survival with epithelial characteristics in the model. These findings implicate stromal morphologic structure as a previously unrecognized prognostic determinant for breast cancer.},
  number = {108},
  options = {useprefix=true}
}

@online{Beers2018,
  title = {High-Resolution Medical Image Synthesis Using Progressively Grown Generative Adversarial Networks},
  author = {Beers, Andrew and Brown, James and Chang, Ken and Campbell, J. Peter and Ostmo, Susan and Chiang, Michael F. and Kalpathy-Cramer, Jayashree},
  date = {2018},
  url = {http://arxiv.org/abs/1805.03144},
  abstract = {Generative adversarial networks (GANs) are a class of unsupervised machine learning algorithms that can produce realistic images from randomly-sampled vectors in a multi-dimensional space. Until recently, it was not possible to generate realistic high-resolution images using GANs, which has limited their applicability to medical images that contain biomarkers only detectable at native resolution. Progressive growing of GANs is an approach wherein an image generator is trained to initially synthesize low resolution synthetic images (8x8 pixels), which are then fed to a discriminator that distinguishes these synthetic images from real downsampled images. Additional convolutional layers are then iteratively introduced to produce images at twice the previous resolution until the desired resolution is reached. In this work, we demonstrate that this approach can produce realistic medical images in two different domains; fundus photographs exhibiting vascular pathology associated with retinopathy of prematurity (ROP), and multi-modal magnetic resonance images of glioma. We also show that fine-grained details associated with pathology, such as retinal vessels or tumor heterogeneity, can be preserved and enhanced by including segmentation maps as additional channels. We envisage several applications of the approach, including image augmentation and unsupervised classification of pathology.},
  archivePrefix = {arXiv},
  arxivid = {1805.03144},
  eprint = {1805.03144},
  eprinttype = {arxiv},
  keywords = {deep learning,generative adversarial networks,glioma,image synthesis,retina,unsuper-,vised learning}
}

@article{Bejnordi2016-fy,
  title = {Stain Specific Standardization of \{\vphantom\}{{Whole}}-{{Slide}}\vphantom\{\} Histopathological Images},
  author = {Bejnordi, Babak Ehteshami and Litjens, Geert and Timofeeva, Nadya and Otte-Höller, Irene and Homeyer, André and Karssemeijer, Nico and van der Laak, Jeroen A W M},
  date = {2016-02},
  journaltitle = {IEEE Transactions on Medical Imaging},
  shortjournal = {IEEE Trans. Med. Imaging},
  volume = {35},
  pages = {404--415},
  abstract = {Variations in the color and intensity of hematoxylin and eosin (H\&E) stained histological slides can potentially hamper the effectiveness of quantitative image analysis. This paper presents a fully automated algorithm for standardization of whole-slide histopathological images to reduce the effect of these variations. The proposed algorithm, called whole-slide image color standardizer (WSICS), utilizes color and spatial information to classify the image pixels into different stain components. The chromatic and density distributions for each of the stain components in the hue-saturation-density color model are aligned to match the corresponding distributions from a template whole-slide image (WSI). The performance of the WSICS algorithm was evaluated on two datasets. The first originated from 125 H\&E stained WSIs of lymph nodes, sampled from 3 patients, and stained in 5 different laboratories on different days of the week. The second comprised 30 H\&E stained WSIs of rat liver sections. The result of qualitative and quantitative evaluations using the first dataset demonstrate that the WSICS algorithm outperforms competing methods in terms of achieving color constancy. The WSICS algorithm consistently yields the smallest standard deviation and coefficient of variation of the normalized median intensity measure. Using the second dataset, we evaluated the impact of our algorithm on the performance of an already published necrosis quantification system. The performance of this system was significantly improved by utilizing the WSICS algorithm. The results of the empirical evaluations collectively demonstrate the potential contribution of the proposed standardization algorithm to improved diagnostic accuracy and consistency in computer-aided diagnosis for histopathology data.},
  number = {2},
  options = {useprefix=true}
}

@article{Bejnordi2017,
  title = {Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women with Breast Cancer},
  author = {Bejnordi, Babak Ehteshami and Veta, Mitko and Van Diest, Paul Johannes and Van Ginneken, Bram and Karssemeijer, Nico and Litjens, Geert and Van Der Laak, Jeroen A.W.M. and Hermsen, Meyke and Manson, Quirine F. and Balkenhol, Maschenka and Geessink, Oscar and Stathonikos, Nikolaos and Van Dijk, Marcory C.R.F. and Bult, Peter and Beca, Francisco and Beck, Andrew H. and Wang, Dayong and Khosla, Aditya and Gargeya, Rishab and Irshad, Humayun and Zhong, Aoxiao and Dou, Qi and Li, Quanzheng and Chen, Hao and Lin, Huang Jing and Heng, Pheng Ann and Haß, Christian and Bruni, Elia and Wong, Quincy and Halici, Ugur and Öner, Mustafa Ümit and Cetin-Atalay, Rengul and Berseth, Matt and Khvatkov, Vitali and Vylegzhanin, Alexei and Kraus, Oren and Shaban, Muhammad and Rajpoot, Nasir and Awan, Ruqayya and Sirinukunwattana, Korsuk and Qaiser, Talha and Tsang, Yee Wah and Tellez, David and Annuscheit, Jonas and Hufnagl, Peter and Valkonen, Mira and Kartasalo, Kimmo and Latonen, Leena and Ruusuvuori, Pekka and Liimatainen, Kaisa and Albarqouni, Shadi and Mungal, Bharti and George, Ami and Demirci, Stefanie and Navab, Nassir and Watanabe, Seiryo and Seno, Shigeto and Takenaka, Yoichi and Matsuda, Hideo and Phoulady, Hady Ahmady and Kovalev, Vassili and Kalinovsky, Alexander and Liauchuk, Vitali and Bueno, Gloria and Fernandez-Carrobles, M. Milagro and Serrano, Ismael and Deniz, Oscar and Racoceanu, Daniel and Venâncio, Rui},
  date = {2017-12},
  journaltitle = {JAMA - Journal of the American Medical Association},
  volume = {318},
  pages = {2199--2210},
  publisher = {{American Medical Association}},
  issn = {15383598},
  doi = {10.1001/jama.2017.14585},
  abstract = {Importance Application of deep learning algorithms to whole-slide pathology images can potentially improve diagnostic accuracy and efficiency. Objective Assess the performance of automated deep learning algorithms at detecting metastases in hematoxylin and eosin-stained tissue sections of lymph nodes of women with breast cancer and compare it with pathologists' diagnoses in a diagnostic setting. Design, Setting, and Participants Researcher challenge competition (CAMELYON16) to develop automated solutions for detecting lymph node metastases (November 2015-November 2016). A training data set of whole-slide images from 2 centers in the Netherlands with (n = 110) and without (n = 160) nodal metastases verified by immunohistochemical staining were provided to challenge participants to build algorithms. Algorithm performance was evaluated in an independent test set of 129 whole-slide images (49 with and 80 without metastases). The same test set of corresponding glass slides was also evaluated by a panel of 11 pathologists with time constraint (WTC) from the Netherlands to ascertain likelihood of nodal metastases for each slide in a flexible 2-hour session, simulating routine pathology workflow, and by 1 pathologist without time constraint (WOTC). Exposures Deep learning algorithms submitted as part of a challenge competition or pathologist interpretation. Main Outcomes and Measures The presence of specific metastatic foci and the absence vs presence of lymph node metastasis in a slide or image using receiver operating characteristic curve analysis. The 11 pathologists participating in the simulation exercise rated their diagnostic confidence as definitely normal, probably normal, equivocal, probably tumor, or definitely tumor. Results The area under the receiver operating characteristic curve (AUC) for the algorithms ranged from 0.556 to 0.994. The top-performing algorithm achieved a lesion-level, true-positive fraction comparable with that of the pathologist WOTC (72.4\% [95\% CI, 64.3\%-80.4\%]) at a mean of 0.0125 false-positives per normal whole-slide image. For the whole-slide image classification task, the best algorithm (AUC, 0.994 [95\% CI, 0.983-0.999]) performed significantly better than the pathologists WTC in a diagnostic simulation (mean AUC, 0.810 [range, 0.738-0.884]; P {$<$} .001). The top 5 algorithms had a mean AUC that was comparable with the pathologist interpreting the slides in the absence of time constraints (mean AUC, 0.960 [range, 0.923-0.994] for the top 5 algorithms vs 0.966 [95\% CI, 0.927-0.998] for the pathologist WOTC). Conclusions and Relevance In the setting of a challenge competition, some deep learning algorithms achieved better diagnostic performance than a panel of 11 pathologists participating in a simulation exercise designed to mimic routine pathology workflow; algorithm performance was comparable with an expert pathologist interpreting whole-slide images without time constraints. Whether this approach has clinical utility will require evaluation in a clinical setting.},
  number = {22}
}

@article{BELISARIO1962,
  title = {Skin Cancer in Australia.},
  author = {Belisario, J. C.},
  date = {1962},
  journaltitle = {Nippon Hifuka Gakkai zasshi. The Japanese journal of dermatology},
  volume = {72},
  pages = {721--732},
  issn = {0021499X},
  doi = {10.5694/j.1326-5377.1989.tb136585.x},
  abstract = {editorial},
  number = {9}
}

@report{Benaich2019,
  title = {State of {{AI}} 2019},
  author = {Benaich, Nathan and Hogarth, Ian},
  date = {2019},
  pages = {126},
  institution = {{State of AI}},
  url = {https://www.stateof.ai/},
  abstract = {We believe that AI will be a force multiplier on technological progress in our increasingly digital, data-driven world. This is because everything around us today, ranging from culture to consumer products, is a product of intelligence. In this report, we set out to capture a snapshot of the exponential progress in AI with a focus on developments in the past 12 months. Consider this report as a compilation of the most interesting things we've seen that seeks to trigger an informed conversation about the state of AI and its implication for the future. This edition builds on the inaugural State of AI Report 2018, which can be found here. We consider the following key dimensions in our report: Research: Technology breakthroughs and their capabilities. Talent: Supply, demand and concentration of talent working in the field. Industry: Large platforms, financings and areas of application for AI-driven innovation today and tomorrow. China: With two distinct internets, we review AI in China as its own category. Politics: Public opinion of AI, economic implications and the emerging geopolitics of AI.}
}

@article{bera2019artificial,
  title = {Artificial Intelligence in Digital Pathology—New Tools for Diagnosis and Precision Oncology},
  author = {Bera, Kaustav and Schalper, Kurt A and Rimm, David L and Velcheti, Vamsidhar and Madabhushi, Anant},
  date = {2019},
  journaltitle = {Nature reviews Clinical oncology},
  volume = {16},
  pages = {703--715},
  publisher = {{Nature Publishing Group}},
  number = {11}
}

@inproceedings{Berman2018,
  title = {The {{Lovász}}-{{Softmax}} Loss: {{A}} Tractable Surrogate for the Optimization of the Intersection-over-Union Measure in Neural Networks},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Berman, Maxim and Rannen Triki, Amal and Blaschko, Matthew B},
  date = {2018},
  pages = {4413--4421}
}

@article{Bhattacharya2017-yn,
  title = {Precision Diagnosis of Melanoma and Other Skin Lesions from Digital Images},
  author = {Bhattacharya, Abhishek and Young, Albert and Wong, Andrew and Stalling, Simone and Wei, Maria and Hadley, Dexter},
  date = {2017-07},
  journaltitle = {AMIA Jt Summits Transl Sci Proc},
  volume = {2017},
  pages = {220--226},
  publisher = {{ncbi.nlm.nih.gov}},
  abstract = {Melanoma will affect an estimated 73,000 new cases this year and result in 9,000 deaths, yet precise diagnosis remains a serious problem. Without early detection and preventative care, melanoma can quickly spread to become fatal (Stage IV 5-year survival rate is 20-10\%) from a once localized skin lesion (Stage IA 5- year survival rate is 97\%). There is no biomarker for melanoma in clinical use, and the current diagnostic criteria for skin lesions remains subjective and imprecise. Accurate diagnosis of melanoma relies on a histopathologic gold standard; thus, aggressive excision of melanocytic skin lesions has been the mainstay of treatment. It is estimated that 36 biopsies are performed for every melanoma confirmed by pathology among excised lesions. There is significant morbidity in misdiagnosing melanoma such as progression of the disease for a false negative prediction vs the risks of unnecessary surgery for a false positive prediction. Every year, poor diagnostic precision adds an estimated \$673 million in overall cost to manage the disease. Currently, manual dermatoscopic imaging is the standard of care in selecting atypical skin lesions for biopsy, and at best it achieves 90\% sensitivity but only 59\% specificity when performed by an expert dermatologist. Many computer vision (CV) algorithms perform better than dermatologists in classifying skin lesions although not significantly so in clinical practice. Meanwhile, open source deep learning (DL) techniques in CV have been gaining dominance since 2012 for image classification, and today DL can outperform humans in classifying millions of digital images with less than 5\% error rates. Moreover, DL algorithms are readily run on commoditized hardware and have a strong online community of developers supporting their rapid adoption. In this work, we performed a successful pilot study to show proof of concept to DL skin pathology from images. However, DL algorithms must be trained on very large labelled datasets of images to achieve high accuracy. Here, we begin to assemble a large imageset of skin lesions from the UCSF and the San Francisco Veterans Affairs Medical Center (VAMC) dermatology clinics that are well characterized by their underlying pathology, on which to train DL algorithms. If trained on sufficient data, we hypothesize that our approach will significantly outperform general dermatologists in predicting skin lesion pathology. We posit that our work will allow for precision diagnosis of melanoma from widely available digital photography, which may optimize the management of the disease by decreasing unnecessary office visits and the significant morbidity and cost of melanoma misdiagnosis.}
}

@online{Bi2017-yd,
  title = {Automatic Skin Lesion Analysis Using Large-Scale Dermoscopy Images and Deep Residual Networks},
  author = {Bi, Lei and Kim, Jinman and Ahn, Euijoon and Feng, Dagan},
  date = {2017-03},
  abstract = {Malignant melanoma has one of the most rapidly increasing incidences in the world and has a considerable mortality rate. Early diagnosis is particularly important since melanoma can be cured with prompt excision. Dermoscopy images play an important role in the non-invasive early detection of melanoma [1]. However, melanoma detection using human vision alone can be subjective, inaccurate and poorly reproducible even among experienced dermatologists. This is attributed to the challenges in interpreting images with diverse characteristics including lesions of varying sizes and shapes, lesions that may have fuzzy boundaries, different skin colors and the presence of hair [2]. Therefore, the automatic analysis of dermoscopy images is a valuable aid for clinical decision making and for image-based diagnosis to identify diseases such as melanoma [1-4]. Deep residual networks (ResNets) has achieved state-of-the-art results in image classification and detection related problems [5-8]. In this ISIC 2017 skin lesion analysis challenge [9], we propose to exploit the deep ResNets for robust visual features learning and representations.},
  archivePrefix = {arXiv},
  arxivid = {cs.CV/1703.04197},
  eprint = {1703.04197},
  eprinttype = {arxiv},
  primaryClass = {cs.CV}
}

@article{Bini2018,
  title = {Artificial Intelligence, Machine Learning, Deep Learning, and Cognitive Computing: {{What}} Do These Terms Mean and How Will They Impact Health Care?},
  author = {Bini, Stefano A.},
  date = {2018-08},
  journaltitle = {Journal of Arthroplasty},
  volume = {33},
  pages = {2358--2361},
  publisher = {{Churchill Livingstone Inc.}},
  issn = {15328406},
  doi = {10.1016/j.arth.2018.02.067},
  abstract = {This article was presented at the 2017 annual meeting of the American Association of Hip and Knee Surgeons to introduce the members gathered as the audience to the concepts behind artificial intelligence (AI) and the applications that AI can have in the world of health care today. We discuss the origin of AI, progress to machine learning, and then discuss how the limits of machine learning lead data scientists to develop artificial neural networks and deep learning algorithms through biomimicry. We will place all these technologies in the context of practical clinical examples and show how AI can act as a tool to support and amplify human cognitive functions for physicians delivering care to increasingly complex patients. The aim of this article is to provide the reader with a basic understanding of the fundamentals of AI. Its purpose is to demystify this technology for practicing surgeons so they can better understand how and where to apply it.},
  keywords = {artificial intelligence,cognitive computing,deep learning,digital health,digital orthopedics,machine learning},
  number = {8}
}

@article{Blease2019,
  title = {Artificial Intelligence and the Future of Primary Care: Exploratory Qualitative Study of {{UK}} General Practitioners' Views},
  author = {Blease, Charlotte and Kaptchuk, Ted J. and Bernstein, Michael H. and Mandl, Kenneth D. and Halamka, John D. and Desroches, Catherine M.},
  date = {2019-03},
  journaltitle = {Journal of Medical Internet Research},
  volume = {21},
  publisher = {{Journal of Medical Internet Research}},
  issn = {14388871},
  doi = {10.2196/12802},
  abstract = {BACKGROUND: The potential for machine learning to disrupt the medical profession is the subject of ongoing debate within biomedical informatics and related fields. OBJECTIVE: This study aimed to explore general practitioners' (GPs') opinions about the potential impact of future technology on key tasks in primary care. METHODS: In June 2018, we conducted a Web-based survey of 720 UK GPs' opinions about the likelihood of future technology to fully replace GPs in performing 6 key primary care tasks, and, if respondents considered replacement for a particular task likely, to estimate how soon the technological capacity might emerge. This study involved qualitative descriptive analysis of written responses ("comments") to an open-ended question in the survey. RESULTS: Comments were classified into 3 major categories in relation to primary care: (1) limitations of future technology, (2) potential benefits of future technology, and (3) social and ethical concerns. Perceived limitations included the beliefs that communication and empathy are exclusively human competencies; many GPs also considered clinical reasoning and the ability to provide value-based care as necessitating physicians' judgments. Perceived benefits of technology included expectations about improved efficiencies, in particular with respect to the reduction of administrative burdens on physicians. Social and ethical concerns encompassed multiple, divergent themes including the need to train more doctors to overcome workforce shortfalls and misgivings about the acceptability of future technology to patients. However, some GPs believed that the failure to adopt technological innovations could incur harms to both patients and physicians. CONCLUSIONS: This study presents timely information on physicians' views about the scope of artificial intelligence (AI) in primary care. Overwhelmingly, GPs considered the potential of AI to be limited. These views differ from the predictions of biomedical informaticians. More extensive, stand-alone qualitative work would provide a more in-depth understanding of GPs' views.},
  keywords = {Artificial intelligence,Attitudes,Future,General practice,Machine learning,Opinions,Primary care,Qualitative research,Technology},
  number = {3}
}

@book{Bolognia2003-aq,
  title = {Dermatology},
  author = {Bolognia, Jean L and Jorizzo, Joseph L and Rapini, Ronald P},
  date = {2003},
  publisher = {{Gulf Professional Publishing}}
}

@online{Brock2018,
  title = {Large Scale Gan Training for High Fidelity Natural Image Synthesis},
  author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  date = {2018},
  archivePrefix = {arXiv},
  eprint = {1809.11096},
  eprinttype = {arxiv}
}

@article{Brown2018,
  title = {Pros and Cons of Artificial Intelligence in Healthcare},
  author = {Brown, Traci},
  date = {2018},
  journaltitle = {Jama software},
  abstract = {Artificial intelligence is a hot topic in healthcare, sparking ongoing debate about the ethical, clinical, and financial pros and cons of relying on algorithms for patient care. Source: Thinkstock September 17, 2018-In what seems like the blink of an eye, mentions of artificial intelligence have become ubiquitous in the healthcare industry. From deep learning algorithms that can read CT scans faster than humans to natural language processing (NLP) that can comb through unstructured data in electronic health records (EHRs), the applications for AI in healthcare seem endless. But like any technology at the peak of its hype curve, artificial intelligence faces criticism from its skeptics alongside enthusiasm from die-hard evangelists. Despite its potential to unlock new insights and streamline the way providers and patients interact with healthcare data, AI may bring not inconsiderable threats of privacy problems, ethics concerns, and medical errors. Balancing the risks and rewards of AI in healthcare will require collaborative effort from technology developers, regulators, end-users, consumers-and maybe even philosophy majors.}
}

@article{Brunnstrom2017-sy,
  title = {\{\vphantom\}{{PD}}-{{L1}}\vphantom\{\} Immunohistochemistry in Clinical Diagnostics of Lung Cancer: Inter-Pathologist Variability Is Higher than Assay Variability},
  author = {Brunnström, Hans and Johansson, Anna and Westbom-Fremer, Sofia and Backman, Max and Djureinovic, Dijana and Patthey, Annika and Isaksson-Mettävainio, Martin and Gulyas, Miklos and Micke, Patrick},
  date = {2017-10},
  journaltitle = {Modern Pathology},
  shortjournal = {Mod. Pathol.},
  volume = {30},
  pages = {1411--1421},
  publisher = {{nature.com}},
  abstract = {Assessment of programmed cell death ligand 1 (PD-L1) immunohistochemical staining is used for decision on treatment with programmed cell death 1 and PD-L1 checkpoint inhibitors in lung adenocarcinomas and squamous cell carcinomas. This study aimed to compare the staining properties of tumor cells between the antibody clones 28-8, 22C3, SP142, and SP263 and investigate interrater variation between pathologists to see if these stainings can be safely evaluated in the clinical setting. Using consecutive sections from a tissue microarray with tumor tissue from 55 resected lung cancer cases, staining with five PD-L1 assays (28-8 from two different vendors, 22C3, SP142, and SP263) was performed. Seven pathologists individually evaluated the percentage of positive tumor cells, scoring each sample applying cutoff levels used in clinical studies: 50\% positive tumor cells (score 5). Pairwise analysis of antibody clones showed weighted kappa values in the range of 0.45-0.91 with the highest values for comparisons with 22C3 and 28-8 and the lowest involving SP142. Excluding SP142 resulted in kappa 0.75-0.91. Weighted kappa for interobserver variation between pathologists was 0.71-0.96. Up to 20\% of the cases were differently classified as positive or negative by any pathologist compared with consensus score using \$\$1\% positive tumor cells as cutoff. A significantly better agreement between pathologists was seen using \$\$50\% as cutoff (0-5\% of cases). In conclusion, the concordance between the PD-L1 antibodies 22C3, 28-8 and SP263 is relatively good when evaluating lung cancers and suggests that any one of these assays may be sufficient as basis for decision on treatment with nivolumab, pembrolizumab, and durvalumab. The scoring of the pathologist presents an intrinsic source of error that should be considered especially at low PD-L1 scores.},
  number = {10}
}

@article{Buch2018,
  title = {Artificial Intelligence in Medicine: Current Trends and Future Possibilities},
  author = {Buch, Varun H and Ahmed, Irfan and Maruthappu, Mahiben},
  date = {2018-03},
  journaltitle = {British Journal of General Practice},
  volume = {68},
  pages = {143--144},
  publisher = {{Royal College of General Practitioners}},
  issn = {0960-1643},
  doi = {10.3399/bjgp18x695213},
  abstract = {Artificial intelligence (AI) research within medicine is growing rapidly. In 2016, healthcare AI projects attracted more investment than AI projects within any other sector of the global economy.1 However, among the excitement, there is equal scepticism, with some urging caution at inflated expectations.2 This article takes a close look at current trends in medical AI and the future possibilities for general practice. Informing clinical decision making through insights from past data is the essence of evidence-based medicine. Traditionally, statistical methods have approached this task by characterising patterns within data as mathematical equations, for example, linear regression suggests a ‘line of best fit'. Through ‘machine learning' (ML), AI provides techniques that uncover complex associations which cannot easily be reduced to an equation. For example, neural networks represent data through vast numbers of interconnected neurones in a similar fashion to the human brain. This allows ML systems to approach complex problem solving just as a clinician might — by carefully weighing evidence to reach reasoned conclusions. However, unlike a single clinician, these systems can simultaneously observe and rapidly process an almost limitless number of inputs. For example, an AI-driven smartphone app now capably handles the task of triaging 1.2 million people in North London to Accident \& Emergency (A\&E).3 Furthermore, these systems are able to learn from each incremental case and can be exposed, within minutes, to more cases than a clinician could see in many lifetimes. This is why an AI-driven application is able to …},
  number = {668}
}

@article{bulten2020artificial,
  title = {Artificial Intelligence Assistance Significantly Improves {{Gleason}} Grading of Prostate Biopsies by Pathologists},
  author = {Bulten, Wouter and Balkenhol, Maschenka and Belinga, Jean-Joël Awoumou and Brilhante, Américo and Çakır, Aslı and Egevad, Lars and Eklund, Martin and Farré, Xavier and Geronatsiou, Katerina and Molinié, Vincent and others},
  date = {2020},
  journaltitle = {Modern Pathology},
  pages = {1--12},
  publisher = {{Nature Publishing Group}}
}

@article{BusinessWire2019,
  title = {{{FDA}} Grants Breakthrough Designation to {{Paige}}.{{AI}}},
  author = {{Business Wire}},
  date = {2019},
  url = {https://www.businesswire.com/news/home/20190307005205/en/FDA-Grants-Breakthrough-Designation-Paige.AI},
  urldate = {2019-08-23}
}

@article{Bychkov2018,
  title = {Deep Learning Based Tissue Analysis Predicts Outcome in Colorectal Cancer},
  author = {Bychkov, Dmitrii and Linder, Nina and Turkki, Riku and Nordling, Stig and Kovanen, Panu E. and Verrill, Clare and Walliander, Margarita and Lundin, Mikael and Haglund, Caj and Lundin, Johan},
  date = {2018-12},
  journaltitle = {Scientific Reports},
  volume = {8},
  publisher = {{Nature Publishing Group}},
  issn = {20452322},
  doi = {10.1038/s41598-018-21758-3},
  abstract = {Image-based machine learning and deep learning in particular has recently shown expert-level accuracy in medical image classification. In this study, we combine convolutional and recurrent architectures to train a deep network to predict colorectal cancer outcome based on images of tumour tissue samples. The novelty of our approach is that we directly predict patient outcome, without any intermediate tissue classification. We evaluate a set of digitized haematoxylin-eosin-stained tumour tissue microarray (TMA) samples from 420 colorectal cancer patients with clinicopathological and outcome data available. The results show that deep learning-based outcome prediction with only small tissue areas as input outperforms (hazard ratio 2.3; CI 95\% 1.79-3.03; AUC 0.69) visual histological assessment performed by human experts on both TMA spot (HR 1.67; CI 95\% 1.28-2.19; AUC 0.58) and whole-slide level (HR 1.65; CI 95\% 1.30-2.15; AUC 0.57) in the stratification into low- and high-risk patients. Our results suggest that state-of-the-art deep learning techniques can extract more prognostic information from the tissue morphology of colorectal cancer than an experienced human observer.},
  number = {1}
}

@article{Cadwalladerolsker2011,
  title = {What Do We Mean by Mathematical Proof ? {{What}} Do We Mean by Mathematical Proof ?},
  author = {Cadwalladerolsker, Todd and Cadwalladerolsker, Todd},
  date = {2011},
  volume = {1},
  doi = {10.5642/jhummath.201101.04},
  number = {1}
}

@inproceedings{Caicedo2009-bm,
  title = {Histopathology Image Classification Using Bag of Features and Kernel Functions},
  booktitle = {Artificial Intelligence in Medicine},
  author = {Caicedo, Juan C and Cruz, Angel and Gonzalez, Fabio A},
  date = {2009},
  pages = {126--135},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Image representation is an important issue for medical image analysis, classification and retrieval. Recently, the bag of features approach has been proposed to classify natural scenes, using an analogy in which visual features are to images as words are to text documents. This process involves feature detection and description, construction of a visual vocabulary and image representation building through visual-word occurrence analysis. This paper presents an evaluation of different representations obtained from the bag of features approach to classify histopathology images. The obtained image descriptors are processed using appropriate kernel functions for Support Vector Machines classifiers. This evaluation includes extensive experimentation of different strategies, and analyses the impact of each configuration in the classification result.}
}

@online{Campanella2018,
  title = {Terabyte-Scale Deep Multiple Instance Learning for Classification and Localization in Pathology},
  author = {Campanella, Gabriele and Silva, Vitor Werneck Krauss and Fuchs, Thomas J.},
  date = {2018-05},
  url = {http://arxiv.org/abs/1805.06983},
  abstract = {In the field of computational pathology, the use of decision support systems powered by state-of-the-art deep learning solutions has been hampered by the lack of large labeled datasets. Until recently, studies relied on datasets in the order of few hundreds of slides which are not enough to train a model that can work at scale in the clinic. Here, we have gathered a dataset consisting of 12,160 slides, two orders of magnitude larger than previous datasets in pathology and equivalent to 25 times the pixel count of the entire ImageNet dataset. Given the size of our dataset it is possible for us to train a deep learning model under the Multiple Instance Learning (MIL) assumption where only the overall slide diagnosis is necessary for training, avoiding all the expensive pixel-wise annotations that are usually part of supervised learning approaches. We test our framework on a complex task, that of prostate cancer diagnosis on needle biopsies. We performed a thorough evaluation of the performance of our MIL pipeline under several conditions achieving an AUC of 0.98 on a held-out test set of 1,824 slides. These results open the way for training accurate diagnosis prediction models at scale, laying the foundation for decision support system deployment in the clinic.},
  archivePrefix = {arXiv},
  arxivid = {1805.06983},
  eprint = {1805.06983},
  eprinttype = {arxiv}
}

@article{Campanella2018a,
  title = {Towards Machine Learned Quality Control: {{A}} Benchmark for Sharpness Quantification in Digital Pathology},
  author = {Campanella, Gabriele and Rajanna, Arjun R. and Corsale, Lorraine and Schüffler, Peter J. and Yagi, Yukako and Fuchs, Thomas J.},
  date = {2018-04},
  journaltitle = {Computerized Medical Imaging and Graphics},
  volume = {65},
  pages = {142--151},
  publisher = {{Elsevier Ltd}},
  issn = {18790771},
  doi = {10.1016/j.compmedimag.2017.09.001},
  abstract = {Pathology is on the verge of a profound change from an analog and qualitative to a digital and quantitative discipline. This change is mostly driven by the high-throughput scanning of microscope slides in modern pathology departments, reaching tens of thousands of digital slides per month. The resulting vast digital archives form the basis of clinical use in digital pathology and allow large scale machine learning in computational pathology. One of the most crucial bottlenecks of high-throughput scanning is quality control (QC). Currently, digital slides are screened manually to detected out-of-focus regions, to compensate for the limitations of scanner software. We present a solution to this problem by introducing a benchmark dataset for blur detection, an in-depth comparison of state-of-the art sharpness descriptors and their prediction performance within a random forest framework. Furthermore, we show that convolution neural networks, like residual networks, can be used to train blur detectors from scratch. We thoroughly evaluate the accuracy of feature based and deep learning based approaches for sharpness classification (99.74\% accuracy) and regression (MSE 0.004) and additionally compare them to domain experts in a comprehensive human perception study. Our pipeline outputs spacial heatmaps enabling to quantify and localize blurred areas on a slide. Finally, we tested the proposed framework in the clinical setting and demonstrate superior performance over the state-of-the-art QC pipeline comprising commercial software and human expert inspection by reducing the error rate from 17\% to 4.7\%.},
  keywords = {Computational pathology,Deep learning,Digital pathology,Machine learning,Quality control,Quantitative blur detection}
}

@article{Campanella2019,
  title = {Clinical-Grade Computational Pathology Using Weakly Supervised Deep Learning on Whole Slide Images},
  author = {Campanella, Gabriele and Hanna, Matthew G. and Geneslaw, Luke and Miraflor, Allen and Werneck Krauss Silva, Vitor and Busam, Klaus J. and Brogi, Edi and Reuter, Victor E. and Klimstra, David S. and Fuchs, Thomas J.},
  date = {2019},
  journaltitle = {Nature Medicine},
  volume = {25},
  publisher = {{Springer US}},
  issn = {1078-8956},
  doi = {10.1038/s41591-019-0508-1},
  url = {http://dx.doi.org/10.1038/s41591-019-0508-1},
  abstract = {The development of decision support systems for pathology and their deployment in clinical practice have been hindered by the need for large manually annotated datasets. To overcome this problem, we present a multiple instance learning-based deep learning system that uses only the reported diagnoses as labels for training, thereby avoiding expensive and time-consuming pixel-wise manual annotations. We evaluated this framework at scale on a dataset of 44,732 whole slide images from 15,187 patients without any form of data curation. Tests on prostate cancer, basal cell carcinoma and breast cancer metastases to axillary lymph nodes resulted in areas under the curve above 0.98 for all cancer types. Its clinical application would allow pathologists to exclude 65–75\% of slides while retaining 100\% sensitivity. Our results show that this system has the ability to train accurate classification models at unprecedented scale, laying the foundation for the deployment of computational decision support systems in clinical practice.},
  issue = {August}
}

@article{Cancer_Genome_Atlas_Research_Network2013-xh,
  title = {The Cancer Genome Atlas \{\vphantom\}{{Pan}}-{{Cancer}}\vphantom\{\} Analysis Project},
  author = {{Cancer Genome Atlas Research Network} and Weinstein, John N and Collisson, Eric A and Mills, Gordon B and Shaw, Kenna R Mills and Ozenberger, Brad A and Ellrott, Kyle and Shmulevich, Ilya and Sander, Chris and Stuart, Joshua M},
  date = {2013-10},
  journaltitle = {Nature Genetics},
  shortjournal = {Nat. Genet.},
  volume = {45},
  pages = {1113--1120},
  abstract = {The Cancer Genome Atlas (TCGA) Research Network has profiled and analyzed large numbers of human tumors to discover molecular aberrations at the DNA, RNA, protein and epigenetic levels. The resulting rich data provide a major opportunity to develop an integrated picture of commonalities, differences and emergent themes across tumor lineages. The Pan-Cancer initiative compares the first 12 tumor types profiled by TCGA. Analysis of the molecular aberrations and their functional roles across tumor types will teach us how to extend therapies effective in one cancer type to others with a similar genomic profile.},
  number = {10}
}

@article{Carel1978-ov,
  title = {[{{Implementation}} of a Computerized \{\vphantom\}{{ECG}}\vphantom\{\} Interpretation System in an Ambulatory Health Service]},
  author = {Carel, R S},
  date = {1978-03},
  journaltitle = {Harefuah},
  volume = {94},
  pages = {177--180},
  number = {5-6}
}

@article{Carithers2015-tf,
  title = {The \{\vphantom\}{{Genotype}}-{{Tissue}}\vphantom\{\} Expression (\{\vphantom\}{{GTEx}}\vphantom\{\}) Project},
  author = {Carithers, Latarsha J and Moore, Helen M},
  date = {2015-10},
  journaltitle = {Biopreservation and Biobanking},
  shortjournal = {Biopreserv. Biobank.},
  volume = {13},
  pages = {307--308},
  number = {5}
}

@article{Carneiro2017-pz,
  title = {Automated Analysis of Unregistered \{\vphantom\}{{Multi}}-{{View}}\vphantom\{\} Mammograms with Deep Learning},
  author = {Carneiro, Gustavo and Nascimento, Jacinto and Bradley, Andrew P},
  date = {2017-11},
  journaltitle = {IEEE Transactions on Medical Imaging},
  shortjournal = {IEEE Trans. Med. Imaging},
  volume = {36},
  pages = {2355--2365},
  publisher = {{ieeexplore.ieee.org}},
  abstract = {We describe an automated methodology for the analysis of unregistered cranio-caudal (CC) and medio-lateral oblique (MLO) mammography views in order to estimate the patient's risk of developing breast cancer. The main innovation behind this methodology lies in the use of deep learning models for the problem of jointly classifying unregistered mammogram views and respective segmentation maps of breast lesions (i.e., masses and micro-calcifications). This is a holistic methodology that can classify a whole mammographic exam, containing the CC and MLO views and the segmentation maps, as opposed to the classification of individual lesions, which is the dominant approach in the field. We also demonstrate that the proposed system is capable of using the segmentation maps generated by automated mass and micro-calcification detection systems, and still producing accurate results. The semi-automated approach (using manually defined mass and micro-calcification segmentation maps) is tested on two publicly available data sets (INbreast and DDSM), and results show that the volume under ROC surface (VUS) for a 3-class problem (normal tissue, benign, and malignant) is over 0.9, the area under ROC curve (AUC) for the 2-class “benign versus malignant” problem is over 0.9, and for the 2-class breast screening problem (malignancy versus normal/benign) is also over 0.9. For the fully automated approach, the VUS results on INbreast is over 0.7, and the AUC for the 2-class “benign versus malignant” problem is over 0.78, and the AUC for the 2-class breast screening is 0.86.},
  number = {11}
}

@inproceedings{Caron2018,
  title = {Deep Clustering for Unsupervised Learning of Visual Features},
  booktitle = {Proceedings of the European Conference on Computer Vision ({{ECCV}})},
  author = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
  date = {2018},
  pages = {132--149}
}

@article{Carter2019,
  title = {Activation Atlas},
  author = {Carter, Shan and Armstrong, Zan and Schubert, Ludwig and Johnson, Ian and Olah, Chris},
  date = {2019},
  journaltitle = {Distill},
  volume = {4},
  pages = {e15},
  issn = {2476-0757},
  number = {3}
}

@inproceedings{caruana2017intelligible,
  title = {Intelligible Machine Learning for Critical Applications Such as Health Care},
  booktitle = {{{AAAS}} 2017 Annual Meeting},
  author = {Caruana, Rich},
  date = {2017},
  url = {https://aaas.confex.com/aaas/2017/webprogram/Paper19142.html}
}

@online{Chalapathy2016,
  title = {Bidirectional {{LSTM}}-{{CRF}} for Clinical Concept Extraction},
  author = {Chalapathy, Raghavendra and Borzeshi, Ehsan Zare and Piccardi, Massimo},
  date = {2016},
  url = {http://arxiv.org/abs/1611.08373},
  abstract = {Automated extraction of concepts from patient clinical records is an essential facilitator of clinical research. For this reason, the 2010 i2b2/VA Natural Language Processing Challenges for Clinical Records introduced a concept extraction task aimed at identifying and classifying concepts into predefined categories (i.e., treatments, tests and problems). State-of-the-art concept extraction approaches heavily rely on handcrafted features and domain-specific resources which are hard to collect and define. For this reason, this paper proposes an alternative, streamlined approach: a recurrent neural network (the bidirectional LSTM with CRF decoding) initialized with general-purpose, off-the-shelf word embeddings. The experimental results achieved on the 2010 i2b2/VA reference corpora using the proposed framework outperform all recent methods and ranks closely to the best submission from the original 2010 i2b2/VA challenge.},
  archivePrefix = {arXiv},
  arxivid = {1611.08373},
  eprint = {1611.08373},
  eprinttype = {arxiv}
}

@article{Chandrasekaran1983,
  title = {On Evaluating Artificial Intelligence Systems for Medical Diagnosis},
  author = {Chandrasekaran, B},
  date = {1983},
  journaltitle = {AI Magazine},
  volume = {4},
  pages = {34--37},
  issn = {0738-4602},
  doi = {10.1609/aimag.v4i2.397},
  url = {https://new.aaai.org/ojs/index.php/aimagazine/article/view/397},
  abstract = {Among the difficulties in evaluating AI-type medical diagnosis systems are: the intermediate conclusions of the AI system need to be looked at in addition to the "final " answer ; the "superhuman human" fallacy must be guarded against ; and methods for estimating how the approach will scale upwards to larger domains are needed. We propose to measure both the accuracy of diagnosis and the structure of reasoning, the latter with a view to gauging how well the system will scale up.},
  keywords = {artificial intelligence,computer science,data mining,fallacy,medical diagnosis,scale up},
  number = {2}
}

@article{Chang2013-hi,
  title = {Invariant Delineation of Nuclear Architecture in Glioblastoma Multiforme for Clinical and Molecular Association},
  author = {Chang, Hang and Han, Ju and Borowsky, Alexander and Loss, Leandro and Gray, Joe W and Spellman, Paul T and Parvin, Bahram},
  date = {2013-04},
  journaltitle = {IEEE Transactions on Medical Imaging},
  shortjournal = {IEEE Trans. Med. Imaging},
  volume = {32},
  pages = {670--682},
  abstract = {Automated analysis of whole mount tissue sections can provide insights into tumor subtypes and the underlying molecular basis of neoplasm. However, since tumor sections are collected from different laboratories, inherent technical and biological variations impede analysis for very large datasets such as The Cancer Genome Atlas (TCGA). Our objective is to characterize tumor histopathology, through the delineation of the nuclear regions, from hematoxylin and eosin (H\&E) stained tissue sections. Such a representation can then be mined for intrinsic subtypes across a large dataset for prediction and molecular association. Furthermore, nuclear segmentation is formulated within a multi-reference graph framework with geodesic constraints, which enables computation of multidimensional representations, on a cell-by-cell basis, for functional enrichment and bioinformatics analysis. Here, we present a novel method, multi-reference graph cut (MRGC), for nuclear segmentation that overcomes technical variations associated with sample preparation by incorporating prior knowledge from manually annotated reference images and local image features. The proposed approach has been validated on manually annotated samples and then applied to a dataset of 377 Glioblastoma Multiforme (GBM) whole slide images from 146 patients. For the GBM cohort, multidimensional representation of the nuclear features and their organization have identified 1) statistically significant subtypes based on several morphometric indexes, 2) whether each subtype can be predictive or not, and 3) that the molecular correlates of predictive subtypes are consistent with the literature. Data and intermediaries for a number of tumor types (GBM, low grade glial, and kidney renal clear carcinoma) are available at: http://tcga.lbl.gov for correlation with TCGA molecular data. The website also provides an interface for panning and zooming of whole mount tissue sections with/without overlaid segmentation results for quality control.},
  number = {4}
}

@article{Chang2016,
  title = {Big Data in Medicine: {{The}} Upcoming Artificial Intelligence},
  author = {Chang, Anthony C.},
  date = {2016},
  journaltitle = {Progress in Pediatric Cardiology},
  volume = {43},
  pages = {91--94},
  issn = {15581519},
  doi = {10.1016/j.ppedcard.2016.08.021}
}

@article{Chen2014,
  title = {Big Data Deep Learning: Challenges and Perspectives},
  author = {Chen, Xue-Wen and Lin, Xiaotong},
  date = {2014},
  journaltitle = {IEEE access},
  volume = {2},
  pages = {514--525},
  issn = {2169-3536}
}

@inproceedings{Chen2014-gg,
  title = {Deep Learning Based Automatic Immune Cell Detection for Immunohistochemistry Images},
  booktitle = {Machine Learning in Medical Imaging},
  author = {Chen, Ting and Chefd'hotel, Christophe},
  date = {2014-09},
  pages = {17--24},
  publisher = {{Springer, Cham}},
  abstract = {Immunohistochemistry (IHC) staining is a widely used technique in the diagnosis of abnormal cells such as cancer. For instance, it can be used to determine the distribution and localization of the differentially expressed biomarkers of immune cells (such as T-cells or B-cells) in cancerous tissue for an immune response study. Typically, the immunological data of interest includes the type, density and location of the immune cells within the tumor samples; this data is of particular interest to pathologists for accurate patient survival prediction. However, to manually count each subset of immune cells under a bright-field microscope for each piece of IHC stained tissue is usually extremely tedious and time consuming. This makes automatic detection very attractive, but it can be very challenging due to the wide variety of cell appearances resulting from different tissue types, block cuttings, and staining processes. This paper presents a novel method for automatic immune cell counting on digitally scanned images of IHC stained slides. The method first uses a sparse color unmixing technique to separate the IHC image into multiple color channels that correspond to different cell structures. Since the immune cell biomarkers that we are interested in are membrane markers, the detection problem is formulated into a deep learning framework using the membrane image channel. The algorithm is evaluated on a clinical data set containing a large number of IHC slides and demonstrates more effective detection than the existing technique and the result is also in accordance with the human observer's output.},
  series = {Lecture Notes in Computer Science}
}

@inproceedings{Chen2015,
  title = {Automatic Fetal Ultrasound Standard Plane Detection Using Knowledge Transferred Recurrent Neural Networks {{BT}} - Medical Image Computing and Computer-Assisted Intervention – {{MICCAI}} 2015},
  booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
  author = {Chen, Hao and Dou, Qi and Ni, Dong and Cheng, Jie-Zhi and Qin, Jing and Li, Shengli and Heng, Pheng-Ann},
  date = {2015},
  pages = {507--514},
  publisher = {{Springer}},
  abstract = {Accurate acquisition of fetal ultrasound (US) standard planes is one of the most crucial steps in obstetric diagnosis. The conventional way of standard plane acquisition requires a thorough knowledge of fetal anatomy and intensive manual labors. Hence, automatic approaches are highly demanded in clinical practice. However, automatic detection of standard planes containing key anatomical structures from US videos remains a challenging problem due to the high intra-class variations of standard planes. Unlike previous studies that developed specific methods for different anatomical standard planes respectively, we present a general framework to detect standard planes from US videos automatically. Instead of utilizing hand-crafted visual features, our framework explores spatio-temporal feature learning with a novel knowledge transferred recurrent neural network (T-RNN), which incorporates a deep hierarchical visual feature extractor and a temporal sequence learning model. In order to extract visual features effectively, we propose a joint learning framework with knowledge transfer across multi-tasks to address the insufficiency issue of limited training data. Extensive experiments on different US standard planes with hundreds of videos corroborate that our method can achieve promising results, which outperform state-of-the-art methods.},
  isbn = {978-3-319-24553-9}
}

@online{Chen2016,
  title = {Automatic Lymphocyte Detection in {{H}}\&{{E}} Images with Deep Neural Networks},
  author = {Chen, Jianxu and Srinivas, Chukka},
  date = {2016},
  archivePrefix = {arXiv},
  eprint = {1612.03217},
  eprinttype = {arxiv}
}

@article{Chen2016-hu,
  title = {Mitosis Detection in Breast Cancer Histology Images via Deep Cascaded Networks},
  author = {Chen, H and Dou, Q and Wang, X and Qin, J and Heng, P A},
  date = {2016},
  journaltitle = {AAAI},
  publisher = {{aaai.org}},
  abstract = {Abstract The number of mitoses per tissue area gives an important aggressiveness indication of the invasive breast carcinoma. However, automatic mitosis detection in histology images remains a challenging problem. Traditional methods either employ hand- crafted features to discriminate mitoses from other cells or construct a pixel-wise classifier to label every pixel in a sliding window way. While the former suffers from the large shape variation of mitoses and the existence of many mimics with similar appearance, the slow}
}

@article{Chen2016a,
  title = {{{IBM}} Watson: {{How}} Cognitive Computing Can Be Applied to Big Data Challenges in Life Sciences Research},
  author = {Chen, Ying and Argentinis, Elenee and Weber, Griff},
  date = {2016},
  journaltitle = {Clinical Therapeutics},
  volume = {38},
  pages = {688--701},
  issn = {1879114X},
  doi = {10.1016/j.clinthera.2015.12.001},
  abstract = {Life sciences researchers are under pressure to innovate faster than ever. Big data offer the promise of unlocking novel insights and accelerating breakthroughs. Ironically, although more data are available than ever, only a fraction is being integrated, understood, and analyzed. The challenge lies in harnessing volumes of data, integrating the data from hundreds of sources, and understanding their various formats. New technologies such as cognitive computing offer promise for addressing this challenge because cognitive solutions are specifically designed to integrate and analyze big datasets. Cognitive solutions can understand different types of data such as lab values in a structured database or the text of a scientific publication. Cognitive solutions are trained to understand technical, industry-specific content and use advanced reasoning, predictive modeling, and machine learning techniques to advance research faster. Watson, a cognitive computing technology, has been configured to support life sciences research. This version of Watson includes medical literature, patents, genomics, and chemical and pharmacological data that researchers would typically use in their work. Watson has also been developed with specific comprehension of scientific terminology so it can make novel connections in millions of pages of text. Watson has been applied to a few pilot studies in the areas of drug target identification and drug repurposing. The pilot results suggest that Watson can accelerate identification of novel drug candidates and novel drug targets by harnessing the potential of big data.},
  keywords = {big data,cognitive computing,data science,drug discovery,genetics,personalized medicine},
  number = {4}
}

@article{Chen2017,
  title = {Deeplab: {{Semantic}} Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected Crfs},
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
  date = {2017},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {40},
  pages = {834--848},
  publisher = {{IEEE}},
  issn = {0162-8828},
  number = {4}
}

@online{Chen2017a,
  title = {Rethinking Atrous Convolution for Semantic Image Segmentation},
  author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  date = {2017-06},
  url = {http://arxiv.org/abs/1706.05587},
  abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
  archivePrefix = {arXiv},
  arxivid = {1706.05587},
  eprint = {1706.05587},
  eprinttype = {arxiv}
}

@online{Chen2018,
  title = {This Looks like That: Deep Learning for Interpretable Image Recognition},
  author = {Chen, Chaofan and Li, Oscar and Tao, Chaofan and Barnett, Alina Jade and Su, Jonathan and Rudin, Cynthia},
  date = {2018},
  archivePrefix = {arXiv},
  eprint = {1806.10574},
  eprinttype = {arxiv}
}

@inproceedings{Chen2018a,
  title = {Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation},
  booktitle = {Proceedings of the {{European}} Conference on Computer Vision ({{ECCV}})},
  author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  date = {2018},
  pages = {801--818}
}

@article{Cheng2016,
  title = {Computer-Aided Diagnosis with Deep Learning Architecture: Applications to Breast Lesions in {{US}} Images and Pulmonary Nodules in {{CT}} Scans},
  author = {Cheng, Jie-Zhi and Ni, Dong and Chou, Yi-Hong and Qin, Jing and Tiu, Chui-Mei and Chang, Yeun-Chung and Huang, Chiun-Sheng and Shen, Dinggang and Chen, Chung-Ming},
  date = {2016},
  journaltitle = {Scientific reports},
  volume = {6},
  pages = {24454},
  issn = {2045-2322}
}

@incollection{Ching2018,
  title = {Opportunities and Obstacles for Deep Learning in Biology and Medicine : 2019 Update {{Authors}}},
  booktitle = {J. {{R}}. {{Soc}}. {{Interface}}},
  author = {Ching, Travers and Do, Brian T and Himmelstein, Daniel S and Beaulieu-jones, Brett K and Kalinin, Alexandr A and Way, Gregory P and Ferrero, Enrico and Agapow, Paul-michael and Carpenter, Anne E and Zietz, Michael and Hoffman, Michael M and Xie, Wei and Rosen, Gail L and Lanchantin, Jack and Xu, Jinbo and Woloszynek, Stephen and Lu, Zhiyong and Harris, David J and Peng, Yifan and Wiley, Laura K},
  date = {2018},
  volume = {15},
  pages = {20170387.},
  isbn = {0000000305396},
  number = {141}
}

@online{cho2017neural,
  title = {Neural Stain-Style Transfer Learning Using Gan for Histopathological Images},
  author = {Cho, Hyungjoo and Lim, Sungbin and Choi, Gunho and Min, Hyunseok},
  date = {2017},
  archivePrefix = {arXiv},
  eprint = {1710.08543},
  eprinttype = {arxiv}
}

@article{Chollet2015,
  title = {Keras - {{GitHub}} Repository},
  author = {Chollet, Fracnois},
  date = {2015},
  url = {https://github.com/keras-team/keras}
}

@article{Chollet2016-uq,
  title = {Xception: {{Deep}} Learning with Depthwise Separable Convolutions},
  author = {Chollet, François},
  date = {2016},
  journaltitle = {arXiv preprint}
}

@article{Churchill2019,
  title = {Anime Face Dataset},
  author = {Churchill, Spencer},
  date = {2019},
  url = {https://www.kaggle.com/splcher/animefacedataset},
  urldate = {2020-08-25}
}

@article{Ciresan2010,
  title = {Deep, Big, Simple Neural Nets for Handwritten Digit Recognition},
  author = {Ciresan, D C and Meier, U and Gambardella, L M and Schmidhuber, J},
  date = {2010},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Comput},
  volume = {22},
  pages = {3207--3220},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00052},
  number = {12}
}

@article{Ciresan2013-dr,
  title = {Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks},
  author = {Cireşan, Dan C and Giusti, Alessandro and Gambardella, Luca M and Schmidhuber, Jürgen},
  date = {2013},
  journaltitle = {Med. Image Comput. Comput. Assist. Interv.},
  volume = {16},
  pages = {411--418},
  publisher = {{Springer}},
  abstract = {We use deep max-pooling convolutional neural networks to detect mitosis in breast histology images. The networks are trained to classify each pixel in the images, using as context a patch centered on the pixel. Simple postprocessing is then applied to the network output. Our approach won the ICPR 2012 mitosis detection competition, outperforming other contestants by a significant margin.},
  issue = {Pt 2}
}

@inproceedings{Coates2013-og,
  title = {Deep Learning with \{\vphantom\}{{COTS}}\vphantom\{\} \{\vphantom\}{{HPC}}\vphantom\{\} Systems},
  booktitle = {International Conference on Machine Learning},
  author = {Coates, Adam and Huval, Brody and Wang, Tao and Wu, David and Catanzaro, Bryan and Andrew, Ng},
  date = {2013-02},
  pages = {1337--1345},
  abstract = {Scaling up deep learning algorithms has been shown to lead to increased performance in benchmark tasks and to enable discovery of complex high-level features. Recent efforts to train extremely large networks (with over 1 billion parameters) have relied on cloud-like computing infrastructure and thousands of CPU cores. In this paper, we present technical details and results from our own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU servers with Infiniband interconnects and MPI. Our system is able to train 1 billion parameter networks on just 3 machines in a couple of days, and we show that it can scale to networks with over 11 billion parameters using just 16 machines. As this infrastructure is much more easily marshaled by others, the approach enables much wider-spread research with extremely large neural networks.}
}

@article{Coats1988,
  title = {Why Expert Systems Fail},
  author = {Coats, Pamela},
  date = {1988},
  journaltitle = {Financial Management},
  volume = {17},
  pages = {77--86},
  issn = {14769360},
  doi = {10.2307/3666074},
  abstract = {The promises of and expectations for expert systems are seriously overblown; the record of experience with financial expert system development for commercial use has been disappointing. While too many unresolved theoretical issues, mechanical limitations, and people problems exist, the systems are also extremely expensive, too time consuming to develop, and often provide naive or superficial results.},
  eprint = {3666074},
  eprinttype = {jstor},
  keywords = {Artificial intelligence,Computing,Expert systems},
  number = {3}
}

@online{Codella2019,
  title = {Skin Lesion Analysis toward Melanoma Detection 2018: {{A}} Challenge Hosted by the International Skin Imaging Collaboration ({{ISIC}})},
  author = {Codella, Noel and Rotemberg, Veronica and Tschandl, Philipp and Celebi, M. Emre and Dusza, Stephen and Gutman, David and Helba, Brian and Kalloo, Aadi and Liopyris, Konstantinos and Marchetti, Michael and Kittler, Harald and Halpern, Allan},
  date = {2019-02},
  url = {http://arxiv.org/abs/1902.03368},
  abstract = {This work summarizes the results of the largest skin image analysis challenge in the world, hosted by the International Skin Imaging Collaboration (ISIC), a global partnership that has organized the world's largest public repository of dermoscopic images of skin. The challenge was hosted in 2018 at the Medical Image Computing and Computer Assisted Intervention (MICCAI) conference in Granada, Spain. The dataset included over 12,500 images across 3 tasks. 900 users registered for data download, 115 submitted to the lesion segmentation task, 25 submitted to the lesion attribute detection task, and 159 submitted to the disease classification task. Novel evaluation protocols were established, including a new test for segmentation algorithm performance, and a test for algorithm ability to generalize. Results show that top segmentation algorithms still fail on over 10\% of images on average, and algorithms with equal performance on test data can have different abilities to generalize. This is an important consideration for agencies regulating the growing set of machine learning tools in the healthcare domain, and sets a new standard for future public challenges in healthcare.},
  archivePrefix = {arXiv},
  arxivid = {1902.03368},
  eprint = {1902.03368},
  eprinttype = {arxiv}
}

@online{Combalia2019,
  title = {{{BCN20000}}: {{Dermoscopic}} Lesions in the Wild},
  author = {Combalia, Marc and Codella, Noel C. F. and Rotemberg, Veronica and Helba, Brian and Vilaplana, Veronica and Reiter, Ofer and Carrera, Cristina and Barreiro, Alicia and Halpern, Allan C. and Puig, Susana and Malvehy, Josep},
  date = {2019},
  pages = {3--5},
  url = {http://arxiv.org/abs/1908.02288},
  abstract = {This article summarizes the BCN20000 dataset, composed of 19424 dermoscopic images of skin lesions captured from 2010 to 2016 in the facilities of the Hospital Cl\textbackslash 'inic in Barcelona. With this dataset, we aim to study the problem of unconstrained classification of dermoscopic images of skin cancer, including lesions found in hard-to-diagnose locations (nails and mucosa), large lesions which do not fit in the aperture of the dermoscopy device, and hypo-pigmented lesions. The BCN20000 will be provided to the participants of the ISIC Challenge 2019, where they will be asked to train algorithms to classify dermoscopic images of skin cancer automatically.},
  archivePrefix = {arXiv},
  arxivid = {1908.02288},
  eprint = {1908.02288},
  eprinttype = {arxiv}
}

@inproceedings{Conoci2017-bc,
  title = {Advanced Skin Lesion Discrimination Pipeline for Early Melanoma Cancer Diagnosis towards \{\vphantom\}{{PoC}}\vphantom\{\} Devices},
  booktitle = {2017 European Conference on Circuit Theory and Design (\{\vphantom\}{{ECCTD}}\vphantom\{\})},
  author = {Conoci, S and Rundo, F and Petralta, S and Battiato, S},
  date = {2017},
  pages = {1--4},
  abstract = {The proposed work describes an effective pipeline for skin lesion (nevus) analysis with related oncological outcomes. The increasing statistics of skin cancer have recently contributed to the development of new methods for early detection and discrimination of malignant skin lesions in order to drastically reduce the number of biopsies often very invasive for the patients. The main aggressive skin cancer histology is the so-called “melanoma” with related differentiation. Several methods have been proposed in the literature for early melanoma detection but often they lack in sensibility/specificity for a real clinical use. The proposed pipeline is based on an effective approach employing analytic innovative hand-crafted image features combined with a machine learning system. This allows both early detection and discrimination of the skin lesions with good trade-off between sensibility/specificity ratio.},
  keywords = {biomedical optical imaging,cancer,feature extracti}
}

@inproceedings{Cordts2016Cityscapes,
  title = {The Cityscapes Dataset for Semantic Urban Scene Understanding},
  booktitle = {Proc. of the {{IEEE}} Conference on Computer Vision and Pattern Recognition ({{CVPR}})},
  author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  date = {2016}
}

@article{Cornet2013,
  title = {Robot Companions and {{ethiCs}} a {{pragmatiC appRoaCh}} of {{ethiCal dEsign}}},
  author = {Cornet, Gérard},
  date = {2013},
  journaltitle = {Journal International de Bioethique},
  volume = {33},
  pages = {49--58},
  issn = {12877352},
  doi = {10.3917/jib.243.0049},
  url = {https://www.cairn.info/revue-journal-international-de-bioethique-2013-4-page-49.htm},
  abstract = {From his experience as ethical expert for two Robot Companion prototype projects aiming at empowering older MCI persons to remain at home and to support their family carers, Gerard Cornet, Gerontologist, review the ethical rules, principles and pragmatic approaches in different cultures. The ethical process of these two funded projects, one European, Companionable (FP7 e-inclusion call1), the other French, Quo vadis (ANR tecsan) are described from the inclusion of the targeted end users in the process, to the assessment and ranking of their main needs and whishes to design the specifications, test the performance expected. Obstacles to turn round and limits for risks evaluation (directs or implicit), acceptability, utility, respect of intimacy and dignity, and balance with freedom and security and frontiers to artificial intelligence are discussed As quoted in the discussion with the French and Japanese experts attending the Toulouse Robotics and medicine symposium (March 26th 2011), the need of a new ethical approach, going further the present ethical rules is needed for the design and social status of ethical robots, having capacity cas factor of progress and global quality of innovation design in an ageing society.},
  isbn = {9782747220811},
  number = {4}
}

@article{Corredor2019,
  title = {Spatial Architecture and Arrangement of Tumor-Infiltrating Lymphocytes for Predicting Likelihood of Recurrence in Early-Stage Non-Small Cell Lung Cancer.},
  author = {Corredor, German and Wang, Xiangxue and Zhou, Yu and Lu, Cheng and Fu, Pingfu and Syrigos, Konstantinos and Rimm, David L and Yang, Michael and Romero, Eduardo and Schalper, Kurt A and Velcheti, Vamsidhar and Madabhushi, Anant},
  date = {2019-03},
  journaltitle = {Clinical cancer research : an official journal of the American Association for Cancer Research},
  volume = {25},
  pages = {1526--1534},
  issn = {1078-0432 (Print)},
  doi = {10.1158/1078-0432.CCR-18-2013},
  abstract = {PURPOSE: The presence of a high degree of tumor-infiltrating lymphocytes (TIL) has been proven to be associated with outcome in patients with non-small cell lung cancer (NSCLC). However, recent evidence indicates that tissue architecture is also prognostic of disease-specific survival and recurrence. We show a set of descriptors (spatial TIL, SpaTIL) that capture density, and spatial colocalization of TILs and tumor cells across digital images that can predict likelihood of recurrence in early-stage NSCLC. EXPERIMENTAL DESIGN: The association between recurrence in early-stage NSCLC and SpaTIL features was explored on 301 patients across four different cohorts. Cohort D1 (n = 70) was used to identify the most prognostic SpaTIL features and to train a classifier to predict the likelihood of recurrence. The classifier performance was evaluated in cohorts D2 (n = 119), D3 (n = 112), and D4 (n = 112). Two pathologists graded each sample of D1 and D2; intraobserver agreement and association between manual grading and likelihood of recurrence were analyzed. RESULTS: SpaTIL was associated with likelihood of recurrence in all test sets (log-rank P {$<$} 0.02). A multivariate Cox proportional hazards analysis revealed an HR of 3.08 (95\% confidence interval, 2.1-4.5, P = 7.3 x 10(-5)). In contrast, agreement among expert pathologists using tumor grade was moderate (Kappa = 0.5), and the manual TIL grading was only prognostic for one reader in D2 (P = 8.0 x 10(-3)). CONCLUSIONS: A set of features related to density and spatial architecture of TILs was found to be associated with a likelihood of recurrence of early-stage NSCLC. This information could potentially be used for helping in treatment planning and management of early-stage NSCLC.See related commentary by Peled et al., p. 1449.},
  eprint = {30201760},
  eprinttype = {pmid},
  langid = {english},
  number = {5}
}

@article{Coudray2018,
  title = {Classification and Mutation Prediction from Non–Small Cell Lung Cancer Histopathology Images Using Deep Learning},
  author = {Coudray, Nicolas and Ocampo, Paolo Santiago and Sakellaropoulos, Theodore and Narula, Navneet and Snuderl, Matija and Fenyö, David and Moreira, Andre L. and Razavian, Narges and Tsirigos, Aristotelis},
  date = {2018-10},
  journaltitle = {Nature Medicine},
  volume = {24},
  pages = {1559--1567},
  publisher = {{Nature Publishing Group}},
  issn = {1546170X},
  doi = {10.1038/s41591-018-0177-5},
  abstract = {Visual inspection of histopathology slides is one of the main methods used by pathologists to assess the stage, type and subtype of lung tumors. Adenocarcinoma (LUAD) and squamous cell carcinoma (LUSC) are the most prevalent subtypes of lung cancer, and their distinction requires visual inspection by an experienced pathologist. In this study, we trained a deep convolutional neural network (inception v3) on whole-slide images obtained from The Cancer Genome Atlas to accurately and automatically classify them into LUAD, LUSC or normal lung tissue. The performance of our method is comparable to that of pathologists, with an average area under the curve (AUC) of 0.97. Our model was validated on independent datasets of frozen tissues, formalin-fixed paraffin-embedded tissues and biopsies. Furthermore, we trained the network to predict the ten most commonly mutated genes in LUAD. We found that six of them—STK11, EGFR, FAT1, SETBP1, KRAS and TP53—can be predicted from pathology images, with AUCs from 0.733 to 0.856 as measured on a held-out population. These findings suggest that deep-learning models can assist pathologists in the detection of cancer subtype or gene mutations. Our approach can be applied to any cancer type, and the code is available at https://github.com/ncoudray/DeepPATH.},
  number = {10}
}

@article{Coudray2018a,
  title = {Classification and Mutation Prediction from Non–Small Cell Lung Cancer Histopathology Images Using Deep Learning},
  author = {Coudray, Nicolas and Ocampo, Paolo Santiago and Sakellaropoulos, Theodore and Narula, Navneet and Snuderl, Matija and Fenyö, David and Moreira, Andre L. and Razavian, Narges and Tsirigos, Aristotelis},
  date = {2018-10},
  journaltitle = {Nature Medicine},
  volume = {24},
  pages = {1559--1567},
  publisher = {{Nature Publishing Group}},
  issn = {1546170X},
  doi = {10.1038/s41591-018-0177-5},
  abstract = {Visual inspection of histopathology slides is one of the main methods used by pathologists to assess the stage, type and subtype of lung tumors. Adenocarcinoma (LUAD) and squamous cell carcinoma (LUSC) are the most prevalent subtypes of lung cancer, and their distinction requires visual inspection by an experienced pathologist. In this study, we trained a deep convolutional neural network (inception v3) on whole-slide images obtained from The Cancer Genome Atlas to accurately and automatically classify them into LUAD, LUSC or normal lung tissue. The performance of our method is comparable to that of pathologists, with an average area under the curve (AUC) of 0.97. Our model was validated on independent datasets of frozen tissues, formalin-fixed paraffin-embedded tissues and biopsies. Furthermore, we trained the network to predict the ten most commonly mutated genes in LUAD. We found that six of them—STK11, EGFR, FAT1, SETBP1, KRAS and TP53—can be predicted from pathology images, with AUCs from 0.733 to 0.856 as measured on a held-out population. These findings suggest that deep-learning models can assist pathologists in the detection of cancer subtype or gene mutations. Our approach can be applied to any cancer type, and the code is available at https://github.com/ncoudray/DeepPATH.},
  number = {10}
}

@article{Coudray2018b,
  title = {Classification and Mutation Prediction from Non-Small Cell Lung Cancer Histopathology Images Using Deep Learning},
  author = {Coudray, N and Ocampo, P S and Sakellaropoulos, T and Narula, N and Snuderl, M and Fenyo, D and Moreira, A L and Razavian, N and Tsirigos, A},
  date = {2018},
  journaltitle = {Nature Medicine},
  volume = {24},
  pages = {1559--+},
  issn = {1078-8956},
  doi = {10.1038/s41591-018-0177-5},
  number = {10}
}

@inproceedings{Cruz-Roa2011-ka,
  title = {A Framework for Semantic Analysis of Histopathological Images Using Nonnegative Matrix Factorization},
  booktitle = {2011 6th Colombian Computing Congress (\{\vphantom\}{{CCC}}\vphantom\{\})},
  author = {Cruz-Roa, A and D\textbackslash '\textbackslash iaz, G and González, F},
  date = {2011-05},
  pages = {1--7},
  abstract = {This paper presents a novel and general framework for histopathology image analysis using nonnegative matrix factorization. The proposed method uses a collection-based image representation called Bag of Features (BOF) to represents the visual information of a histopathology image collection. Convex Nonnegative Matrix Factorization (CNMF) is applied to a training set of images to find a compact representation in a latent topic space. The latent representation has two important characteristics: first, CNMF is able to find representative clusters of images in the collection, second, clusters are represented by convex linear combinations of images in the training set. This latent representation is exploited in different ways by the proposed framework: concept labels can be assigned to clusters using the labels of the constituting images, representative images and visual words can be identified for each cluster, and new unlabeled images can be labeled by mapping them to the latent space. The proposed annotation model has an interesting property, it is easily interpretable since it is possible to trace those visual words present in the image which contribute the most to a given annotation. This implies that annotations in an image may be explained by identifying the regions that contributed to them. An exploratory experimentation was performed in a histopathology dataset used to diagnose a type of skin cancer called basal cell carcinoma. The preliminary results show that the combination of BOF and NMF is an interesting alternative for biomedical image collection analysis with a high level of interpretability.},
  keywords = {cancer,image representation,m,matrix decomposition}
}

@article{Cruz-Roa2011-ll,
  title = {Visual Pattern Mining in Histology Image Collections Using Bag of Features},
  author = {Cruz-Roa, A and Caicedo, J and González, F},
  date = {2011-06},
  journaltitle = {Artificial Intelligence in Medicine},
  shortjournal = {Artif. Intell. Med.},
  volume = {52},
  pages = {91--106},
  abstract = {OBJECTIVE: The paper addresses the problem of finding visual patterns in histology image collections. In particular, it proposes a method for correlating basic visual patterns with high-level concepts combining an appropriate image collection representation with state-of-the-art machine learning techniques. METHODOLOGY: The proposed method starts by representing the visual content of the collection using a bag-of-features strategy. Then, two main visual mining tasks are performed: finding associations between visual-patterns and high-level concepts, and performing automatic image annotation. Associations are found using minimum-redundancy-maximum-relevance feature selection and co-clustering analysis. Annotation is done by applying a support-vector-machine classifier. Additionally, the proposed method includes an interpretation mechanism that associates concept annotations with corresponding image regions. The method was evaluated in two data sets: one comprising histology images from the different four fundamental tissues, and the other composed of histopathology images used for cancer diagnosis. Different visual-word representations and codebook sizes were tested. The performance in both concept association and image annotation tasks was qualitatively and quantitatively evaluated. RESULTS: The results show that the method is able to find highly discriminative visual features and to associate them to high-level concepts. In the annotation task the method showed a competitive performance: an increase of 21\% in f-measure with respect to the baseline in the histopathology data set, and an increase of 47\% in the histology data set. CONCLUSIONS: The experimental evidence suggests that the bag-of-features representation is a good alternative to represent visual content in histology images. The proposed method exploits this representation to perform visual pattern mining from a wider perspective where the focus is the image collection as a whole, rather than individual images.},
  number = {2}
}

@article{Cruz-Roa2011-pf,
  title = {Automatic Annotation of Histopathological Images Using a Latent Topic Model Based on Non-Negative Matrix Factorization},
  author = {Cruz-Roa, Angel and D\textbackslash '\textbackslash iaz, Gloria and Romero, Eduardo and González, Fabio A},
  date = {2011},
  journaltitle = {J. Pathol. Inform.},
  volume = {2},
  pages = {S4},
  abstract = {Histopathological images are an important resource for clinical diagnosis and biomedical research. From an image understanding point of view, the automatic annotation of these images is a challenging problem. This paper presents a new method for automatic histopathological image annotation based on three complementary strategies, first, a part-based image representation, called the bag of features, which takes advantage of the natural redundancy of histopathological images for capturing the fundamental patterns of biological structures, second, a latent topic model, based on non-negative matrix factorization, which captures the high-level visual patterns hidden in the image, and, third, a probabilistic annotation model that links visual appearance of morphological and architectural features associated to 10 histopathological image annotations. The method was evaluated using 1,604 annotated images of skin tissues, which included normal and pathological architectural and morphological features, obtaining a recall of 74\% and a precision of 50\%, which improved a baseline annotation method based on support vector machines in a 64\% and 24\%, respectively.},
  keywords = {Automatic Annotation,Bag of Features,Basal Cell}
}

@article{Cruz-Roa2013-ny,
  title = {A Deep Learning Architecture for Image Representation, Visual Interpretability and Automated Basal-Cell Carcinoma Cancer Detection},
  author = {Cruz-Roa, Angel Alfonso and Arevalo Ovalle, John Edison and Madabhushi, Anant and González Osorio, Fabio Augusto},
  date = {2013},
  journaltitle = {Med. Image Comput. Comput. Assist. Interv.},
  volume = {16},
  pages = {403--410},
  abstract = {This paper presents and evaluates a deep learning architecture for automated basal cell carcinoma cancer detection that integrates (1) image representation learning, (2) image classification and (3) result interpretability. A novel characteristic of this approach is that it extends the deep learning architecture to also include an interpretable layer that highlights the visual patterns that contribute to discriminate between cancerous and normal tissues patterns, working akin to a digital staining which spotlights image regions important for diagnostic decisions. Experimental evaluation was performed on set of 1,417 images from 308 regions of interest of skin histopathology slides, where the presence of absence of basal cell carcinoma needs to be determined. Different image representation strategies, including bag of features (BOF), canonical (discrete cosine transform (DCT) and Haar-based wavelet transform (Haar)) and proposed learned-from-data representations, were evaluated for comparison. Experimental results show that the representation learned from a large histology image data set has the best overall performance (89.4\% in F-measure and 91.4\% in balanced accuracy), which represents an improvement of around 7\% over canonical representations and 3\% over the best equivalent BOF representation.},
  issue = {Pt 2}
}

@article{Cruz-Roa2014-zb,
  title = {Automatic Detection of Invasive Ductal Carcinoma in Whole Slide Images with Convolutional Neural Networks},
  author = {Cruz-Roa, A and Basavanhally, A and {Others}},
  date = {2014},
  journaltitle = {medical imaging},
  publisher = {{pdfs.semanticscholar.org}},
  abstract = {ABSTRACT This paper presents a deep learning approach for automatic detection and visual analysis of invasive ductal carcinoma (IDC) tissue regions in whole slide images (WSI) of breast cancer (BCa). Deep learning approaches are learn-from-data methods}
}

@article{Cruz-Roa2017-aq,
  title = {Accurate and Reproducible Invasive Breast Cancer Detection in Whole-Slide Images: {{A Deep Learning}} Approach for Quantifying Tumor Extent},
  author = {Cruz-Roa, Angel and Gilmore, Hannah and Basavanhally, Ajay and Feldman, Michael and Ganesan, Shridar and Shih, Natalie N C and Tomaszewski, John and González, Fabio A and Madabhushi, Anant},
  date = {2017-04},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci. Rep.},
  volume = {7},
  pages = {46450},
  abstract = {With the increasing ability to routinely and rapidly digitize whole slide images with slide scanners, there has been interest in developing computerized image analysis algorithms for automated detection of disease extent from digital pathology images. The manual identification of presence and extent of breast cancer by a pathologist is critical for patient management for tumor staging and assessing treatment response. However, this process is tedious and subject to inter- and intra-reader variability. For computerized methods to be useful as decision support tools, they need to be resilient to data acquired from different sources, different staining and cutting protocols and different scanners. The objective of this study was to evaluate the accuracy and robustness of a deep learning-based method to automatically identify the extent of invasive tumor on digitized images. Here, we present a new method that employs a convolutional neural network for detecting presence of invasive tumor on whole slide images. Our approach involves training the classifier on nearly 400 exemplars from multiple different sites, and scanners, and then independently validating on almost 200 cases from The Cancer Genome Atlas. Our approach yielded a Dice coefficient of 75.86\%, a positive predictive value of 71.62\% and a negative predictive value of 96.77\% in terms of pixel-by-pixel evaluation compared to manually annotated regions of invasive ductal carcinoma.}
}

@inproceedings{Csurka2014,
  title = {What Is a Good Evaluation Measure for Semantic Segmentation?},
  author = {Csurka, Gabriela and Larlus, Diane and Perronnin, Florent},
  date = {2014-01},
  pages = {32.1--32.11},
  publisher = {{British Machine Vision Association and Society for Pattern Recognition}},
  doi = {10.5244/c.27.32},
  abstract = {In this work, we consider the evaluation of the semantic segmentation task. We discuss the strengths and limitations of the few existing measures, and propose new ways to evaluate semantic segmentation. First, we argue that a per-image score instead of one computed over the entire dataset brings a lot more insight. Second, we propose to take contours more carefully into account. Based on the conducted experiments, we suggest best practices for the evaluation. Finally, we present a user study we conducted to better understand how the quality of image segmentations is perceived by humans.}
}

@inproceedings{Dabkowski2017,
  title = {Real Time Image Saliency for Black Box Classifiers},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Dabkowski, Piotr and Gal, Yarin},
  date = {2017},
  pages = {6967--6976}
}

@article{DBLP:journals/corr/abs-1906-09453,
  title = {Computer Vision with a Single (Robust) Classifier},
  author = {Santurkar, Shibani and Tsipras, Dimitris and Tran, Brandon and Ilyas, Andrew and Engstrom, Logan and Madry, Aleksander},
  date = {2019},
  journaltitle = {CoRR},
  volume = {abs/1906.0},
  url = {http://arxiv.org/abs/1906.09453},
  archivePrefix = {arXiv},
  arxivid = {1906.09453},
  eprint = {1906.09453},
  eprinttype = {arxiv}
}

@incollection{Dean2012-jo,
  title = {Large Scale Distributed Deep Networks},
  booktitle = {Advances in Neural Information Processing Systems 25},
  author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and aurelio Ranzato, Marc\textbackslash textbackslashtextquotesingle and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc V and Ng, Andrew Y},
  editor = {Pereira, F and Burges, C J C and Bottou, L and Weinberger, K Q},
  date = {2012},
  pages = {1223--1231},
  publisher = {{Curran Associates, Inc.}},
  options = {useprefix=true}
}

@online{DeGeus2018,
  title = {Panoptic Segmentation with a Joint Semantic and Instance Segmentation Network},
  author = {de Geus, Daan and Meletis, Panagiotis and Dubbelman, Gijs},
  date = {2018},
  archivePrefix = {arXiv},
  eprint = {1809.02110},
  eprinttype = {arxiv},
  options = {useprefix=true}
}

@report{DepartmentofParliamentaryServices2015,
  title = {Budget Review 2015},
  author = {{Department of Parliamentary Services}},
  date = {2015},
  pages = {1--173},
  institution = {{Australian Government}},
  issn = {ISSN 2203-5249},
  url = {https://parlinfo.aph.gov.au/parlInfo/download/library/prspub/4036352/upload\{_\}binary/4036352.pdf;fileType=application/pdf},
  abstract = {2015-2016 Budget Review},
  number = {May}
}

@online{Deshpande2020,
  title = {{{SAFRON}}: {{Stitching}} across the Frontier for Generating Colorectal Cancer Histology Images},
  author = {Deshpande, Srijay and Minhas, Fayyaz and Graham, Simon and Rajpoot, Nasir},
  date = {2020},
  pages = {1--10},
  url = {http://arxiv.org/abs/2008.04526},
  abstract = {Synthetic images can be used for the development and evaluation of deep learning algorithms in the context of limited availability of annotations. In the field of computational pathology where histology images are large and visual context is crucial, synthesis of large tissue images via generative modeling is a challenging task due to memory and computing constraints hindering the generation of large images. To address this challenge, we propose a novel framework named as SAFRON to construct realistic large tissue image tiles from ground truth annotations while preserving morphological features and with minimal boundary artifacts at the seams. To this end, we train the proposed SAFRON framework based on conditional generative adversarial networks on large tissue image tiles from the Colorectal Adenocarcinoma Gland (CRAG) and DigestPath datasets. We demonstrate that our model can generate high quality and realistic image tiles of arbitrary large size after training it on relatively small image patches. We also show that training on synthetic data generated by SAFRON can significantly boost the performance of a standard algorithm for gland segmentation of colorectal cancer tissue images. Sample high resolution images generated using SAFRON are available at the URL:https://warwick.ac.uk/TIALab/SAFRON},
  archivePrefix = {arXiv},
  arxivid = {2008.04526},
  eprint = {2008.04526},
  eprinttype = {arxiv}
}

@book{Deutsch2011,
  title = {The Beginning of Infinity: {{Explanations}} That Transform the World},
  author = {Deutsch, David},
  date = {2011},
  publisher = {{Penguin UK}},
  isbn = {0-14-196969-5}
}

@book{DeVita2014-qg,
  title = {\{\vphantom\}{{DeVita}}\vphantom\{\}, Hellman, and Rosenberg's Cancer: {{Principles}} \& Practice of Oncology},
  author = {DeVita, Vincent T and Lawrence, Theodore S and Rosenberg, Steven A},
  date = {2014-12},
  publisher = {{Wolters Kluwer}},
  abstract = {Select references are available in the print book. ALL REFERENCES FORDeVita, Hellman, and Rosenberg's Cancer: Principles \& Practice of Oncology, 10th editionARE AVAILABLE ONLINE AT LWWHealthLibrary.com/oncology.DeVita, Hellman, and Rosenberg's Cancer: Principles \& Practice of Oncology, 10th edition has garnered universal acclaim asthe world's definitive, standard-setting oncology reference. More than 400 respected luminaries explore today's most effective strategies for managing every type of cancer by stage of presentation - discussing the role of all appropriate therapeutic modalities as well as combined-modality treatments. This multidisciplinary approach will help your cancer team collaboratively face the toughest clinical challenges andprovide the best possible care for every cancer patient.Access the complete contents online or on your mobile device, withquarterly updates reflecting late-breaking developments in cancer care, free for the first year on LWW Health Library.Take full advantage of the latest advances with brand-new chapters on Hallmarks of Cancer, Molecular Methods in Cancer, Oncogenic Viruses, Cancer Screening, and new sections on Genetic testing and counseling for cancer, plus comprehensive updates throughout – including coverage of the newest biologic therapies.Make optimal, well-coordinated use of all appropriate therapies with balanced, multidisciplinary advice from a surgeon, a medical oncologist, and a radiation oncologist in each major treatment chapter.Review the latest molecular biology knowledge for each type of cancer and its implications for improved management.Make the best decisions on cancer screening and prevention, palliative care, supportive oncology, and quality-of-life issuesNow packaged with 1 year Free access to Cancer: Principles \& Practice, 10th Edition on theNEW and ENHANCED LWW Health Library. The high-quality, updated content you've come to rely on throughout your career, just got better!Now access Cancer: Principles \& Practice, 10th Edition entire content on the go, and access quarterly updates.Additionally, for the first 6 months, Cancer: Principles \& Practice of Oncology, 10th Edition purchasers will get access to the entire Oncology Collection on LWW Health Library – an incredible value at no additional charge!Select references are available in the print book. ALL REFERENCES FORDeVita, Hellman, and Rosenberg's Cancer: Principles \& Practice of Oncology, 10th editionARE AVAILABLE ONLINE AT LWWHealthLibrary.com/oncology.}
}

@inproceedings{Diaz2010-mm,
  title = {Histopathological Image Classification Using Stain Component Features on a \{\vphantom\}{{pLSA}}\vphantom\{\} Model},
  booktitle = {Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications},
  author = {D\textbackslash '\textbackslash iaz, Gloria and Romero, Eduardo},
  date = {2010},
  pages = {55--62},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Semantic annotation of microscopical field of views is one of the key problems in computer assistance of histopathological images. In this paper a new method for extracting patch descriptors is proposed and evaluated using a probabilistic latent semantic analysis (pLSA) classification model. The proposed approach is based on the analysis of the different dyes used to stain the histological sample. This analysis allows to find local regions that correspond to cells in the image, which are then described by the SIFT descriptors of the stain components. The proposed approach outperforms the conventional sampling and description strategies, proposed in the literature.}
}

@article{Djuric2017,
  title = {Precision Histology: How Deep Learning Is Poised to Revitalize Histomorphology for Personalized Cancer Care},
  author = {Djuric, Ugljesa and Zadeh, Gelareh and Aldape, Kenneth and Diamandis, Phedias},
  date = {2017},
  journaltitle = {npj Precision Oncology},
  volume = {1},
  pages = {22},
  issn = {2397-768X},
  doi = {10.1038/s41698-017-0022-1},
  url = {https://doi.org/10.1038/s41698-017-0022-1},
  number = {1}
}

@article{Dogan2020,
  title = {Semi-Supervised Image Attribute Editing Using Generative Adversarial Networks},
  author = {Dogan, Yahya and Keles, Hacer Yalim},
  date = {2020},
  journaltitle = {Neurocomputing},
  volume = {401},
  pages = {338--352},
  issn = {18728286},
  doi = {10.1016/j.neucom.2020.03.071},
  abstract = {Image attribute editing is a challenging problem that has been recently studied by many researchers using generative networks. The challenge is in the manipulation of selected attributes of images while preserving the other details. The method to achieve this goal is to find an accurate latent vector representation of an image and a direction corresponding to the attribute. Almost all the works in the literature use labeled datasets in a supervised setting for this purpose. In this study, we introduce an architecture called Cyclic Reverse Generator (CRG), which allows learning the inverse function of the generator accurately via an encoder in an unsupervised setting by utilizing cyclic cost minimization. Attribute editing is then performed using the CRG models for finding desired attribute representations in the latent space. In this work, we use two arbitrary reference images, with and without desired attributes, to compute an attribute direction for editing. We show that the proposed approach performs better in terms of image reconstruction compared to the existing end-to-end generative models both quantitatively and qualitatively. We demonstrate state-of-the-art results on both real images and generated images in CelebA dataset.},
  archivePrefix = {arXiv},
  arxivid = {1907.01841},
  eprint = {1907.01841},
  eprinttype = {arxiv},
  keywords = {Convolutional neural networks,Deep learning,Generative adversarial networks,Generative models,Image attribute editing}
}

@article{Doi2007,
  title = {Computer-Aided Diagnosis in Medical Imaging: Historical Review, Current Status and Future Potential},
  author = {Doi, Kunio},
  date = {2007},
  journaltitle = {Computerized medical imaging and graphics},
  volume = {31},
  pages = {198--211},
  issn = {0895-6111},
  number = {4}
}

@inproceedings{dollar2007feature,
  title = {Feature Mining for Image Classification},
  booktitle = {2007 {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Dollár, Piotr and Tu, Zhuowen and Tao, Hai and Belongie, Serge},
  date = {2007},
  pages = {1--8},
  organization = {{IEEE}}
}

@article{Dong2014-jj,
  title = {Computational Pathology to Discriminate Benign from Malignant Intraductal Proliferations of the Breast},
  author = {Dong, Fei and Irshad, Humayun and Oh, Eun-Yeong and Lerwill, Melinda F and Brachtel, Elena F and Jones, Nicholas C and Knoblauch, Nicholas W and Montaser-Kouhsari, Laleh and Johnson, Nicole B and Rao, Luigi K F and Faulkner-Jones, Beverly and Wilbur, David C and Schnitt, Stuart J and Beck, Andrew H},
  date = {2014-12},
  journaltitle = {PLoS One},
  volume = {9},
  pages = {e114885},
  publisher = {{journals.plos.org}},
  abstract = {The categorization of intraductal proliferative lesions of the breast based on routine light microscopic examination of histopathologic sections is in many cases challenging, even for experienced pathologists. The development of computational tools to aid pathologists in the characterization of these lesions would have great diagnostic and clinical value. As a first step to address this issue, we evaluated the ability of computational image analysis to accurately classify DCIS and UDH and to stratify nuclear grade within DCIS. Using 116 breast biopsies diagnosed as DCIS or UDH from the Massachusetts General Hospital (MGH), we developed a computational method to extract 392 features corresponding to the mean and standard deviation in nuclear size and shape, intensity, and texture across 8 color channels. We used L1-regularized logistic regression to build classification models to discriminate DCIS from UDH. The top-performing model contained 22 active features and achieved an AUC of 0.95 in cross-validation on the MGH data-set. We applied this model to an external validation set of 51 breast biopsies diagnosed as DCIS or UDH from the Beth Israel Deaconess Medical Center, and the model achieved an AUC of 0.86. The top-performing model contained active features from all color-spaces and from the three classes of features (morphology, intensity, and texture), suggesting the value of each for prediction. We built models to stratify grade within DCIS and obtained strong performance for stratifying low nuclear grade vs. high nuclear grade DCIS (AUC = 0.98 in cross-validation) with only moderate performance for discriminating low nuclear grade vs. intermediate nuclear grade and intermediate nuclear grade vs. high nuclear grade DCIS (AUC = 0.83 and 0.69, respectively). These data show that computational pathology models can robustly discriminate benign from malignant intraductal proliferative lesions of the breast and may aid pathologists in the diagnosis and classification of these lesions.},
  number = {12}
}

@inproceedings{dosovitskiy2016inverting,
  title = {Inverting Visual Representations with Convolutional Networks},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Dosovitskiy, Alexey and Brox, Thomas},
  date = {2016},
  pages = {4829--4837}
}

@article{Doyle2006-ue,
  title = {A Boosting Cascade for Automated Detection of Prostate Cancer from Digitized Histology},
  author = {Doyle, Scott and Madabhushi, Anant and Feldman, Michael and Tomaszeweski, John},
  date = {2006},
  journaltitle = {Med. Image Comput. Comput. Assist. Interv.},
  volume = {9},
  pages = {504--511},
  abstract = {Current diagnosis of prostatic adenocarcinoma is done by manual analysis of biopsy tissue samples for tumor presence. However, the recent advent of whole slide digital scanners has made histopathological tissue specimens amenable to computer-aided diagnosis (CAD). In this paper, we present a CAD system to assist pathologists by automatically detecting prostate cancer from digitized images of prostate histological specimens. Automated diagnosis on very large high resolution images is done via a multi-resolution scheme similar to the manner in which a pathologist isolates regions of interest on a glass slide. Nearly 600 image texture features are extracted and used to perform pixel-wise Bayesian classification at each image scale to obtain corresponding likelihood scenes. Starting at the lowest scale, we apply the AdaBoost algorithm to combine the most discriminating features, and we analyze only pixels with a high combined probability of malignancy at subsequent higher scales. The system was evaluated on 22 studies by comparing the CAD result to a pathologist's manual segmentation of cancer (which served as ground truth) and found to have an overall accuracy of 88\%. Our results show that (1) CAD detection sensitivity remains consistently high across image scales while CAD specificity increases with higher scales, (2) the method is robust to choice of training samples, and (3) the multi-scale cascaded approach results in significant savings in computational time.},
  issue = {Pt 2}
}

@inproceedings{Drozdzal2016,
  title = {The Importance of Skip Connections in Biomedical Image Segmentation},
  booktitle = {Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  author = {Drozdzal, Michal and Vorontsov, Eugene and Chartrand, Gabriel and Kadoury, Samuel and Pal, Chris},
  date = {2016},
  volume = {10008 LNCS},
  pages = {179--187},
  publisher = {{Springer Verlag}},
  issn = {16113349},
  doi = {10.1007/978-3-319-46976-8_19},
  abstract = {In this paper, we study the influence of both long and short skip connections on Fully Convolutional Networks (FCN) for biomedical image segmentation. In standard FCNs, only long skip connections are used to skip features from the contracting path to the expanding path in order to recover spatial information lost during downsampling. We extend FCNs by adding short skip connections, that are similar to the ones introduced in residual networks, in order to build very deep FCNs (of hundreds of layers). A review of the gradient flow confirms that for a very deep FCN it is beneficial to have both long and short skip connections. Finally, we show that a very deep FCN can achieve near-to-state-of-the-art results on the EM dataset without any further post-processing.},
  isbn = {978-3-319-46975-1},
  keywords = {FCN,ResNet,Semantic segmentation,Skip connections}
}

@article{drucker1992improving,
  title = {Improving Generalization Performance Using Double Backpropagation},
  author = {Drucker, Harris and Le Cun, Yann},
  date = {1992},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {3},
  pages = {991--997},
  number = {6}
}

@article{Dry2009,
  title = {Who Owns Diagnostic Tissue Blocks?},
  author = {Dry, Sarah},
  date = {2009-01},
  journaltitle = {Laboratory Medicine},
  volume = {40},
  pages = {69--73},
  publisher = {{Oxford University Press (OUP)}},
  issn = {0007-5027},
  doi = {10.1309/lm3xp8hbkdsgicjh},
  abstract = {The question of ownership of human tissues has generated debate for centuries. In the last 20 years, the attention of academicians in bioethics and law, as well as the lay press, has focused on ownership of tissues collected specifically for research purposes. Ownership of tissues originally collected for diagnostic purposes, and now residing as formalin-fixed, paraffin-embedded tissue blocks within pathology laboratories throughout this country, has not received as much attention. Who owns these tissue blocks?},
  number = {2}
}

@book{Elder2014,
  title = {Lever's Histopathology of the Skin},
  author = {Elder, David E},
  date = {2014},
  publisher = {{Lippincott Williams \& Wilkins}},
  isbn = {1-4963-0101-3}
}

@article{Elmore2017,
  title = {Pathologists' Diagnosis of Invasive Melanoma and Melanocytic Proliferations: {{Observer}} Accuracy and Reproducibility Study},
  author = {Elmore, Joann G. and Barnhill, Raymond L. and Elder, David E. and Longton, Gary M. and Pepe, Margaret S. and Reisch, Lisa M. and Carney, Patricia A. and Titus, Linda J. and Nelson, Heidi D. and Onega, Tracy and Tosteson, Anna N.A. and Weinstock, Martin A. and Knezevich, Stevan R. and Piepkorn, Michael W.},
  date = {2017-06},
  journaltitle = {BMJ (Online)},
  volume = {357},
  publisher = {{BMJ Publishing Group}},
  issn = {17561833},
  doi = {10.1136/bmj.j2813},
  abstract = {Objective To quantify the accuracy and reproducibility of pathologists' diagnoses of melanocytic skin lesions.Design Observer accuracy and reproducibility study.Setting 10 US states.Participants Skin biopsy cases (n=240), grouped into sets of 36 or 48. Pathologists from 10 US states were randomized to independently interpret the same set on two occasions (phases 1 and 2), at least eight months apart.Main outcome measures Pathologists' interpretations were condensed into five classes: I (eg, nevus or mild atypia); II (eg, moderate atypia); III (eg, severe atypia or melanoma in situ); IV (eg, pathologic stage T1a (pT1a) early invasive melanoma); and V (eg, ≥pT1b invasive melanoma). Reproducibility was assessed by intraobserver and interobserver concordance rates, and accuracy by concordance with three reference diagnoses.Results In phase 1, 187 pathologists completed 8976 independent case interpretations resulting in an average of 10 (SD 4) different diagnostic terms applied to each case. Among pathologists interpreting the same cases in both phases, when pathologists diagnosed a case as class I or class V during phase 1, they gave the same diagnosis in phase 2 for the majority of cases (class I 76.7\%; class V 82.6\%). However, the intraobserver reproducibility was lower for cases interpreted as class II (35.2\%), class III (59.5\%), and class IV (63.2\%). Average interobserver concordance rates were lower, but with similar trends. Accuracy using a consensus diagnosis of experienced pathologists as reference varied by class: I, 92\% (95\% confidence interval 90\% to 94\%); II, 25\% (22\% to 28\%); III, 40\% (37\% to 44\%); IV, 43\% (39\% to 46\%); and V, 72\% (69\% to 75\%). It is estimated that at a population level, 82.8\% (81.0\% to 84.5\%) of melanocytic skin biopsy diagnoses would have their diagnosis verified if reviewed by a consensus reference panel of experienced pathologists, with 8.0\% (6.2\% to 9.9\%) of cases overinterpreted by the initial pathologist and 9.2\% (8.8\% to 9.6\%) underinterpreted.Conclusion Diagnoses spanning moderately dysplastic nevi to early stage invasive melanoma were neither reproducible nor accurate in this large study of pathologists in the USA. Efforts to improve clinical practice should include using a standardized classification system, acknowledging uncertainty in pathology reports, and developing tools such as molecular markers to support pathologists' visual assessments.}
}

@article{Elson2007,
  title = {Asirra: A {{CAPTCHA}} That Exploits Interest-Aligned Manual Image Categorization},
  author = {Elson, Jeremy and Douceur, John J D and Howell, Jon and Saul, Jared},
  date = {2007}
}

@article{Engstrom2019,
  title = {A Discussion of'{{Adversarial}} Examples Are Not Bugs, They Are Features': {{Discussion}} and Author Responses},
  author = {Engstrom, Logan and Ilyas, Andrew and Madry, Aleksander and Santurkar, Shibani and Tran, Brandon and Tsipras, Dimitris},
  date = {2019},
  journaltitle = {Distill},
  volume = {4},
  pages = {e00019--7},
  issn = {2476-0757},
  number = {8}
}

@online{Engstrom2019a,
  title = {Learning Perceptually-Aligned Representations via Adversarial Robustness},
  author = {Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Tran, Brandon and Madry, Aleksander},
  date = {2019},
  archivePrefix = {arXiv},
  eprint = {1906.00945},
  eprinttype = {arxiv}
}

@online{engstrom2019adversarial,
  title = {Adversarial Robustness as a Prior for Learned Representations},
  author = {Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Tran, Brandon and Madry, Aleksander},
  date = {2019},
  archivePrefix = {arXiv},
  eprint = {1906.00945},
  eprinttype = {arxiv}
}

@article{Erhan2009,
  title = {Visualizing Higher-Layer Features of a Deep Network},
  author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  date = {2009},
  journaltitle = {University of Montreal},
  volume = {1341},
  pages = {1},
  number = {3}
}

@article{Esteva2017,
  title = {Dermatologist-Level Classification of Skin Cancer with Deep Neural Networks},
  author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko, Justin and Swetter, Susan M and Blau, Helen M and Thrun, Sebastian},
  date = {2017},
  journaltitle = {Nature},
  volume = {542},
  pages = {115--118},
  issn = {0028-0836},
  number = {7639}
}

@article{esteva2017dermatologist,
  title = {Dermatologist-Level Classification of Skin Cancer with Deep Neural Networks},
  author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko, Justin and Swetter, Susan M and Blau, Helen M and Thrun, Sebastian},
  date = {2017},
  journaltitle = {nature},
  volume = {542},
  pages = {115--118},
  publisher = {{Nature Publishing Group}},
  number = {7639}
}

@inproceedings{Estienne2019,
  title = {U-{{ReSNet}}: {{Ultimate}} Coupling of Registration and Segmentation with Deep Nets},
  booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
  author = {Estienne, Théo and Vakalopoulou, Maria and Christodoulidis, Stergios and Battistela, Enzo and Lerousseau, Marvin and Carre, Alexandre and Klausner, Guillaume and Sun, Roger and Robert, Charlotte and Mougiakakou, Stavroula},
  date = {2019},
  pages = {310--319},
  publisher = {{Springer}}
}

@article{Everingham:2010:PVO:1747084.1747104,
  title = {The Pascal Visual Object Classes ({{VOC}}) Challenge},
  author = {Everingham, Mark and Gool, Luc and Williams, Christopher K and Winn, John and Zisserman, Andrew},
  date = {2010-06},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int. J. Comput. Vision},
  volume = {88},
  pages = {303--338},
  publisher = {{Kluwer Academic Publishers}},
  address = {Hingham, MA, USA},
  issn = {0920-5691},
  doi = {10.1007/s11263-009-0275-4},
  url = {http://dx.doi.org/10.1007/s11263-009-0275-4},
  keywords = {Benchmark,Database,Object detection,Object recognition},
  number = {2}
}

@article{Faust2019,
  title = {Intelligent Feature Engineering and Ontological Mapping of Brain Tumour Histomorphologies by Deep Learning},
  author = {Faust, Kevin and Bala, Sudarshan and van Ommeren, Randy and Portante, Alessia and Al Qawahmed, Raniah and Djuric, Ugljesa and Diamandis, Phedias},
  date = {2019},
  journaltitle = {Nature Machine Intelligence},
  volume = {1},
  pages = {316},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5839},
  number = {7},
  options = {useprefix=true}
}

@article{Ferreira1997,
  title = {The Virtual Microscope.},
  author = {Ferreira, R and Moon, B and Humphries, J and Sussman, A and Saltz, J and Miller, R and Demarzo, A},
  date = {1997},
  journaltitle = {Proceedings : a conference of the American Medical Informatics Association. AMIA Fall Symposium},
  pages = {449--453},
  issn = {1091-8280 (Print)},
  abstract = {We present the design of the Virtual Microscope, a software system employing a client/server architecture to provide a realistic emulation of a high power light microscope. We discuss several technical challenges related to providing the performance necessary to achieve rapid response time, mainly in dealing with the enormous amounts of data (tens to hundreds of gigabytes per slide) that must be retrieved from secondary storage and processed. To effectively implement the data server, the system design relies on the computational power and high I/O throughput available from an appropriately configured parallel computer.},
  eprint = {9357666},
  eprinttype = {pmid},
  keywords = {Computer-Assisted,Computers,Image Processing,instrumentation,Microscopy,Software,Software Design,User-Computer Interface},
  langid = {english}
}

@article{Filipczuk2013-hm,
  title = {\{\vphantom\}{{Computer}}-{{Aided}}\vphantom\{\} Breast Cancer Diagnosis Based on the Analysis of Cytological Images of Fine Needle Biopsies},
  author = {Filipczuk, Pawel and Fevens, Thomas and Krzyzak, Adam and Monczak, Roman},
  date = {2013-12},
  journaltitle = {IEEE Transactions on Medical Imaging},
  shortjournal = {IEEE Trans. Med. Imaging},
  volume = {32},
  pages = {2169--2178},
  abstract = {The effectiveness of the treatment of breast cancer depends on its timely detection. An early step in the diagnosis is the cytological examination of breast material obtained directly from the tumor. This work reports on advances in computer-aided breast cancer diagnosis based on the analysis of cytological images of fine needle biopsies to characterize these biopsies as either benign or malignant. Instead of relying on the accurate segmentation of cell nuclei, the nuclei are estimated by circles using the circular Hough transform. The resulting circles are then filtered to keep only high-quality estimations for further analysis by a support vector machine which classifies detected circles as correct or incorrect on the basis of texture features and the percentage of nuclei pixels according to a nuclei mask obtained using Otsu's thresholding method. A set of 25 features of the nuclei is used in the classification of the biopsies by four different classifiers. The complete diagnostic procedure was tested on 737 microscopic images of fine needle biopsies obtained from patients and achieved 98.51\% effectiveness. The results presented in this paper demonstrate that a computerized medical diagnosis system based on our method would be effective, providing valuable, accurate diagnostic information.},
  number = {12}
}

@article{Firmino2016,
  title = {Computer-Aided Detection ({{CADe}}) and Diagnosis ({{CADx}}) System for Lung Cancer with Likelihood of Malignancy},
  author = {Firmino, Macedo and Angelo, Giovani and Morais, Higor and Dantas, Marcel R and Valentim, Ricardo},
  date = {2016},
  journaltitle = {BioMedical Engineering OnLine},
  volume = {15},
  pages = {2},
  issn = {1475-925X},
  doi = {10.1186/s12938-015-0120-7},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5002110/},
  number = {1}
}

@article{Fischer2008-ob,
  title = {Hematoxylin and Eosin Staining of Tissue and Cell Sections},
  author = {Fischer, Andrew H and Jacobson, Kenneth A and Rose, Jack and Zeller, Rolf},
  date = {2008-05},
  journaltitle = {CSH Protoc.},
  volume = {2008},
  pages = {db.prot4986},
  abstract = {INTRODUCTIONHematoxylin and eosin (H\&E) stains have been used for at least a century and are still essential for recognizing various tissue types and the morphologic changes that form the basis of contemporary cancer diagnosis. The stain has been unchanged for many years because it works well with a variety of fixatives and displays a broad range of cytoplasmic, nuclear, and extracellular matrix features. Hematoxylin has a deep blue-purple color and stains nucleic acids by a complex, incompletely understood reaction. Eosin is pink and stains proteins nonspecifically. In a typical tissue, nuclei are stained blue, whereas the cytoplasm and extracellular matrix have varying degrees of pink staining. Well-fixed cells show considerable intranuclear detail. Nuclei show varying cell-type- and cancer-type-specific patterns of condensation of heterochromatin (hematoxylin staining) that are diagnostically very important. Nucleoli stain with eosin. If abundant polyribosomes are present, the cytoplasm will have a distinct blue cast. The Golgi zone can be tentatively identified by the absence of staining in a region next to the nucleus. Thus, the stain discloses abundant structural information, with specific functional implications. A limitation of hematoxylin staining is that it is incompatible with immunofluorescence. It is useful, however, to stain one serial paraffin section from a tissue in which immunofluorescence will be performed. Hematoxylin, generally without eosin, is useful as a counterstain for many immunohistochemical or hybridization procedures that use colorimetric substrates (such as alkaline phosphatase or peroxidase). This protocol describes H\&E staining of tissue and cell sections.}
}

@article{Fleming2018,
  title = {Computer-Calculated Compounds},
  author = {Fleming, Nic},
  date = {2018},
  journaltitle = {Nature},
  pages = {5--7}
}

@article{Fogel2018,
  title = {Artificial Intelligence Powers Digital Medicine},
  author = {Fogel, Alexander L. and Kvedar, Joseph C.},
  date = {2018-12},
  journaltitle = {npj Digital Medicine},
  volume = {1},
  publisher = {{Springer Nature}},
  doi = {10.1038/s41746-017-0012-2},
  abstract = {Artificial intelligence (AI) has recently surpassed human performance in several domains, and there is great hope that in healthcare, AI may allow for better prevention, detection, diagnosis, and treatment of disease. While many fear that AI will disrupt jobs and the physician–patient relationship, we believe that AI can eliminate many repetitive tasks to clear the way for human-to-human bonding and the application of emotional intelligence and judgment. We review several recent studies of AI applications in healthcare that provide a view of a future where healthcare delivery is a more unified, human experience.},
  number = {1}
}

@article{Fogel2018a,
  title = {Artificial Intelligence Powers Digital Medicine},
  author = {Fogel, A L and Kvedar, J C},
  date = {2018},
  journaltitle = {Npj Digital Medicine},
  volume = {1},
  issn = {2398-6352},
  doi = {10.1038/s41746-017-0012-2},
  url = {\{%\}3CGo to}
}

@book{ForschungundTechnologiedesLandesNordrhein-Westfalen2015,
  title = {Ziel- Und Leistungsvereinbarungen 2007 – 2010. {{Hochschulen}} in Nordrhein-Westfalen},
  author = {Forschung und Technologie des Landes Nordrhein-Westfalen, Ministerium für Innovation Wissenschaft},
  date = {2015},
  pages = {91--99},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2016.2577031},
  url = {https://github.com/ http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and ob-jectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2\% mAP) and 2012 (70.4\% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster\_rcnn.},
  archivePrefix = {arXiv},
  arxivid = {arXiv:1506.01497v1},
  eprint = {27295650},
  eprinttype = {pmid},
  isbn = {0162-8828 VO - PP}
}

@article{Frey2017-vz,
  title = {Detection of Malignant Melanoma in Histological Sample Using Deep Neural Networks},
  author = {Frey, Adam},
  date = {2017},
  publisher = {{Univerzita Karlova, Matematicko-fyzikáln\{\textbackslash '\textbackslash i\} fakulta}}
}

@article{Fujisawa2019,
  title = {Deep-Learning-Based, Computer-Aided Classifier Developed with a Small Dataset of Clinical Images Surpasses Board-Certified Dermatologists in Skin Tumour Diagnosis},
  author = {Fujisawa, Y. and Otomo, Y. and Ogata, Y. and Nakamura, Y. and Fujita, R. and Ishitsuka, Y. and Watanabe, R. and Okiyama, N. and Ohara, K. and Fujimoto, M.},
  date = {2019-02},
  journaltitle = {British Journal of Dermatology},
  volume = {180},
  pages = {373--381},
  publisher = {{Blackwell Publishing Ltd}},
  issn = {13652133},
  doi = {10.1111/bjd.16924},
  abstract = {Background Application of deep-learning technology to skin cancer classification can potentially improve the sensitivity and specificity of skin cancer screening, but the number of training images required for such a system is thought to be extremely large. Objectives To determine whether deep-learning technology could be used to develop an efficient skin cancer classification system with a relatively small data-set of clinical images. Methods A deep convolutional neural network (DCNN) was trained using a dataset of 4867 clinical images obtained from 1842 patients diagnosed with skin tumours at the University of Tsukuba Hospital from 2003 to 2016. The images consisted of 14 diagnoses, including both malignant and benign conditions. Its performance was tested against 13 board-certified dermatologists and nine dermatology trainees. Results The overall classification accuracy of the trained DCNN was 76Á5\%. The DCNN achieved 96Á3\% sensitivity (correctly classified malignant as malignant) and 89Á5\% specificity (correctly classified benign as benign). Although the accuracy of malignant or benign classification by the board-certified dermatologists was statistically higher than that of the dermatology trainees (85Á3\% AE 3Á7\% and 74Á4\% AE 6Á8\%, P {$<$} 0Á01), the DCNN achieved even greater accuracy, as high as 92Á4\% AE 2Á1\% (P {$<$} 0Á001). Conclusions We have developed an efficient skin tumour classifier using a DCNN trained on a relatively small dataset. The DCNN classified images of skin tumours more accurately than board-certified dermatologists. Collectively, the current system may have capabilities for screening purposes in general medical practice, particularly because it requires only a single clinical image for classification.},
  number = {2}
}

@online{gabbay2019style,
  title = {Style Generator Inversion for Image Enhancement and Animation},
  author = {Gabbay, Aviv and Hoshen, Yedid},
  date = {2019},
  archivePrefix = {arXiv},
  eprint = {1906.11880},
  eprinttype = {arxiv}
}

@report{Gal2016,
  title = {Uncertainty in Deep Learning},
  author = {Gal, Yarin},
  date = {2016}
}

@online{Gale2017-uw,
  title = {Detecting Hip Fractures with Radiologist-Level Performance Using Deep Neural Networks},
  author = {Gale, William and Oakden-Rayner, Luke and Carneiro, Gustavo and Bradley, Andrew P and Palmer, Lyle J},
  date = {2017-11},
  abstract = {We developed an automated deep learning system to detect hip fractures from frontal pelvic x-rays, an important and common radiological task. Our system was trained on a decade of clinical x-rays ( ̃53,000 studies) and can be applied to clinical data, automatically excluding inappropriate and technically unsatisfactory studies. We demonstrate diagnostic performance equivalent to a human radiologist and an area under the ROC curve of 0.994. Translated to clinical practice, such a system has the potential to increase the efficiency of diagnosis, reduce the need for expensive additional testing, expand access to expert level medical image interpretation, and improve overall patient outcomes.},
  archivePrefix = {arXiv},
  arxivid = {cs.CV/1711.06504},
  eprint = {1711.06504},
  eprinttype = {arxiv},
  primaryClass = {cs.CV}
}

@article{Gardner2019,
  title = {Keep Calm and Tweet on: {{Legal}} and Ethical Considerations for Pathologists Using Social Media},
  author = {Gardner, Jerad M. and Allen, Timothy C.},
  date = {2019-01},
  journaltitle = {Archives of Pathology and Laboratory Medicine},
  volume = {143},
  pages = {75--80},
  publisher = {{College of American Pathologists}},
  issn = {15432165},
  doi = {10.5858/arpa.2018-0313-SA},
  abstract = {* Recent privacy breaches by a major social media company have again raised questions from some pathologists regarding the legality and ethics of sharing pathology images on social media. The authors examined ethical principles as well as historic and legal precedents relevant to pathology medical photography. Taking and sharing photographs of pathology specimens is embedded into the culture of the specialty of pathology and has been for more than a century. In general, the pathologist who takes the photograph of a gross or microscopic specimen owns the copyright to that photograph. Patient consent is not legally or ethically required to take or use deidentified photographs of pathology specimens. Current US privacy laws (Health Insurance Portability and Accountability Act [HIPAA] of 1996) permit public sharing of deidentified pathology photographs without specific patient consent, even on social media. There is no case law of action taken against pathologists for sharing deidentified pathology images on social media or elsewhere. If there is any legal risk for pathologists or risk of patient harm in sharing pathology photographs, it is very small. The benefits of professional social media use for pathologists, patients, and society are numerous and well documented in the literature.},
  number = {1}
}

@article{GegundezFernandez2018,
  title = {Technification versus Humanisation. {{Artificial}} Intelligence for Medical Diagnosis},
  author = {Gegúndez Fernández, J. A.},
  date = {2018},
  journaltitle = {Archivos de la Sociedad Espanola de Oftalmologia},
  volume = {93},
  pages = {e17--e19},
  publisher = {{Sociedad Espa\&ntilde;ola de Oftalmolog\&iacute;a}},
  issn = {03656691},
  doi = {10.1016/j.oftal.2017.11.004},
  url = {http://dx.doi.org/10.1016/j.oftale.2017.11.012},
  number = {3}
}

@inproceedings{Gelasca2008-hu,
  title = {Evaluation and Benchmark for Biological Image Segmentation},
  booktitle = {2008 15th \{\vphantom\}{{IEEE}}\vphantom\{\} International Conference on Image Processing},
  author = {Gelasca, E Drelie and Byun, J and Obara, B and Manjunath, B S},
  date = {2008-10},
  pages = {1816--1819},
  abstract = {This paper describes ongoing work on creating a benchmarking and validation dataset for biological image segmentation. While the primary target is biological images, we believe that the dataset would be of help to researchers working in image segmentation and tracking in general. The motivation for creating this resource comes from the observation that while there are a large number of effective segmentation methods available in the research literature, it is difficult for the application scientists to make an informed choice as to what methods would work for her particular problem. No one single tool exists that is effective on a diverse set of application contexts and different methods have their own strengths and limitations. We describe below three different classes of data, ranging in scale from subcellular to cellular to tissue level images, each of which pose their own set of challenges to image analysis. Of particular value to the image processing researchers is that the data comes with associated ground truth information that can be used to evaluate the effectiveness of different methods. The analysis and evaluation are also integrated into a database framework that is available online at http://dough.ece.ucsb.edu.},
  keywords = {biological tissues,cellular biophysics,image segme}
}

@article{Geller2003-vq,
  title = {Epidemiology of Melanoma and Nonmelanoma Skin Cancer},
  author = {Geller, Alan C and Annas, George D},
  date = {2003-02},
  journaltitle = {Seminars in Oncology Nursing},
  shortjournal = {Semin. Oncol. Nurs.},
  volume = {19},
  pages = {2--11},
  abstract = {OBJECTIVES: To describe the epidemiology of melanoma and nonmelanoma skin cancers. DATA SOURCES: Review and research articles, book chapters, and Surveillance, Epidemiology, and End Results (SEER) data. CONCLUSIONS: In 2002, an estimated 1.3 million Americans were diagnosed with skin cancer. Of these, 53,000 individuals were diagnosed with melanoma, the most common fatal form of skin cancer, and more than 7,000 Americans died of melanoma. Nonmelanoma skin cancer has the highest incidence of all cancers and the rise in the rate of cutaneous melanoma exceeds all other preventable cancers. IMPLICATIONS FOR NURSING PRACTICE: Nurses can act as case-finders and as advocates and educators for prevention of overexposure to ultraviolet radiation. Nurses should ascertain possible inherited risk and monitor patients for additional primary skin cancers.},
  number = {1}
}

@article{Gertych2015-ya,
  title = {Machine Learning Approaches to Analyze Histological Images of Tissues from Radical Prostatectomies},
  author = {Gertych, Arkadiusz and Ing, Nathan and Ma, Zhaoxuan and Fuchs, Thomas J and Salman, Sadri and Mohanty, Sambit and Bhele, Sanica and Velásquez-Vacca, Adriana and Amin, Mahul B and Knudsen, Beatrice S},
  date = {2015-12},
  journaltitle = {Computerized Medical Imaging and Graphics},
  shortjournal = {Comput. Med. Imaging Graph.},
  volume = {46 Pt 2},
  pages = {197--208},
  publisher = {{Elsevier}},
  abstract = {Computerized evaluation of histological preparations of prostate tissues involves identification of tissue components such as stroma (ST), benign/normal epithelium (BN) and prostate cancer (PCa). Image classification approaches have been developed to identify and classify glandular regions in digital images of prostate tissues; however their success has been limited by difficulties in cellular segmentation and tissue heterogeneity. We hypothesized that utilizing image pixels to generate intensity histograms of hematoxylin (H) and eosin (E) stains deconvoluted from H\&E images numerically captures the architectural difference between glands and stroma. In addition, we postulated that joint histograms of local binary patterns and local variance (LBPxVAR) can be used as sensitive textural features to differentiate benign/normal tissue from cancer. Here we utilized a machine learning approach comprising of a support vector machine (SVM) followed by a random forest (RF) classifier to digitally stratify prostate tissue into ST, BN and PCa areas. Two pathologists manually annotated 210 images of low- and high-grade tumors from slides that were selected from 20 radical prostatectomies and digitized at high-resolution. The 210 images were split into the training (n=19) and test (n=191) sets. Local intensity histograms of H and E were used to train a SVM classifier to separate ST from epithelium (BN+PCa). The performance of SVM prediction was evaluated by measuring the accuracy of delineating epithelial areas. The Jaccard J=59.5 \$\$ 14.6 and Rand Ri=62.0 \$\$ 7.5 indices reported a significantly better prediction when compared to a reference method (Chen et al., Clinical Proteomics 2013, 10:18) based on the averaged values from the test set. To distinguish BN from PCa we trained a RF classifier with LBPxVAR and local intensity histograms and obtained separate performance values for BN and PCa: JBN=35.2 \$\$ 24.9, OBN=49.6 \$\$ 32, JPCa=49.5 \$\$ 18.5, OPCa=72.7 \$\$ 14.8 and Ri=60.6 \$\$ 7.6 in the test set. Our pixel-based classification does not rely on the detection of lumens, which is prone to errors and has limitations in high-grade cancers and has the potential to aid in clinical studies in which the quantification of tumor content is necessary to prognosticate the course of the disease. The image data set with ground truth annotation is available for public use to stimulate further research in this area.},
  keywords = {Image analysis,Machine learning,Prostate cancer}
}

@inproceedings{Ghorbani2019,
  title = {Interpretation of Neural Networks Is Fragile},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Ghorbani, Amirata and Abid, Abubakar and Zou, James},
  date = {2019},
  volume = {33},
  pages = {3681--3688},
  isbn = {2374-3468}
}

@article{Gilchrist1985,
  title = {Interobserver Reproducibility of Histopathological Features in Stage {{II}} Breast Cancer},
  author = {Gilchrist, Kennedy W and Kalish, Leslie and Gould, Victor E and Hirschl, Simon and Imbriglia, Joseph E and Levy, Walter M and Patchefsky, Arthur S and Penner, Donald W and Pickren, John and Roth, Joel A},
  date = {1985},
  journaltitle = {Breast cancer research and treatment},
  volume = {5},
  pages = {3--10},
  publisher = {{Springer}},
  issn = {0167-6806},
  number = {1}
}

@article{Gilles2008-ex,
  title = {Pathologist Interobserver Variability of Histologic Features in Childhood Brain Tumors: Results from the \{\vphantom\}{{CCG}}-945\vphantom\{\} Study},
  author = {Gilles, Floyd H and Tavaré, C Jane and Becker, Laurence E and Burger, Peter C and Yates, Allan J and Pollack, Ian F and Finlay, Jonathan L},
  date = {2008-03},
  journaltitle = {Pediatric and Developmental Pathology},
  shortjournal = {Pediatr. Dev. Pathol.},
  volume = {11},
  pages = {108--117},
  publisher = {{journals.sagepub.com}},
  abstract = {In the Children's Cancer Group-945 trial, study design allowed estimation of overall interpathologist observational agreement for 6 histologic features frequently used in brain tumor diagnoses. We evaluated agreement between pairs of 5 experienced neuropathologists, who had knowledge of the general diagnoses prior to slide readings. We performed this study in an attempt to further improve pathologist interinstitutional agreement. The features mitosis, necrosis, and giant cells had “fair” overall kappa estimates of reproducibility of around 0.5, while endothelial proliferation had only a “poor” overall kappa of 0.35. The Rogot reproducibility index averaged 0.5 for pleomorphism and hyperchromia. The upper bounds for the 10 pair summary agreement estimates were at best 0.65 (“good”) for all 6 features. These relatively low-reproducibility estimates for the very small number of histologic features being assessed in tumors institutionally diagnosed as high-grade gliomas indicate that neuropathologists either used different operational definitions or interpreted them differently. We found that we could rank the histologic features from best to worst agreement among study pathologists as necrosis, giant cells, mitosis, endothelial proliferation, hyperchromic nuclei, and pleomorphic cells. We suggest that neuropathologists involved in multi-institutional studies of putative therapies not discard these traditional histologic features, but rather develop standardized operational definitions and measure their variability before beginning the studies. Only after such histologic feature variability studies are conducted will we have the data to identify specific histologic features of value to clinicians and researchers. Agreement and strict adherence to improved nonsubjective diagnostic criteria would improve histologic feature reliability and, consequently, their usefulness in studies.},
  number = {2}
}

@article{Gillies2015,
  title = {Radiomics: Images Are More than Pictures, They Are Data},
  author = {Gillies, Robert J and Kinahan, Paul E and Hricak, Hedvig},
  date = {2015},
  journaltitle = {Radiology},
  volume = {278},
  pages = {563--577},
  issn = {0033-8419},
  number = {2}
}

@article{Giulini2019,
  title = {A Deep Learning Approach to the Structural Analysis of Proteins},
  author = {Giulini, Marco and Potestio, Raffaello},
  date = {2019},
  journaltitle = {Interface Focus},
  volume = {9},
  issn = {20428901},
  doi = {10.1098/rsfs.2019.0003},
  abstract = {Deep Learning (DL) algorithms hold great promise for applications in the field of computational biophysics. In fact, the vast amount of available molecular structures, as well as their notable complexity, constitutes an ideal context in which DL-based approaches can be profitably employed. To express the full potential of these techniques, though, it is a prerequisite to express the information contained in the molecule's atomic positions and distances in a set of input quantities that the network can process. Many of the molecular descriptors devised insofar are effective and manageable for relatively small structures, but become complex and cumbersome for larger ones. Furthermore, most of them are defined locally, a feature that could represent a limit for those applications where global properties are of interest. Here, we build a deep learning architecture capable of predicting non-trivial and intrinsically global quantities, that is, the eigenvalues of a protein's lowest-energy fluctuation modes. This application represents a first, relatively simple test bed for the development of a neural network approach to the quantitative analysis of protein structures, and demonstrates unexpected use in the identification of mechanically relevant regions of the molecule.},
  keywords = {Deep neural networks,Elastic network models,Protein structure},
  number = {3}
}

@inproceedings{Glorot,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  author = {Glorot, Xavier and Bengio, Yoshua},
  pages = {249--256}
}

@inproceedings{glorot2011deep,
  title = {Deep Sparse Rectifier Neural Networks},
  booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  date = {2011},
  pages = {315--323}
}

@inproceedings{Goodfellow2014,
  title = {Generative Adversarial Nets},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014},
  pages = {2672--2680}
}

@online{Goodfellow2014a,
  title = {Explaining and Harnessing Adversarial Examples},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  date = {2014-12},
  url = {http://arxiv.org/abs/1412.6572},
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  archivePrefix = {arXiv},
  arxivid = {1412.6572},
  eprint = {1412.6572},
  eprinttype = {arxiv}
}

@online{Goodfellow2016,
  title = {{{NIPS}} 2016 Tutorial: {{Generative}} Adversarial Networks},
  author = {Goodfellow, Ian},
  date = {2016-12},
  url = {http://arxiv.org/abs/1701.00160},
  abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
  archivePrefix = {arXiv},
  arxivid = {1701.00160},
  eprint = {1701.00160},
  eprinttype = {arxiv}
}

@article{goodman2017european,
  title = {European {{Union}} Regulations on Algorithmic Decision-Making and a “Right to Explanation”},
  author = {Goodman, Bryce and Flaxman, Seth},
  date = {2017},
  journaltitle = {AI Magazine},
  volume = {38},
  pages = {50--57},
  number = {3}
}

@article{Gould2018,
  title = {Superpowered Skin},
  author = {Gould, Julie},
  date = {2018},
  journaltitle = {Nature},
  volume = {563},
  pages = {S84--S85},
  issn = {14764687},
  doi = {10.1038/d41586-018-07429-3},
  url = {https://training.seer.cancer.gov/melanoma/anatomy/layers.html},
  abstract = {The skin is the body's largest organ and has several, diverse functions. As well as being a physical barrier, it has immune and sensory properties. Under the surface Skin's most important role is to protect the body from the environment. It comprises three main layers: the epidermis, the dermis and subcutaneous fat. Most of the body is covered in hairy skin but the palms of the hands and the soles of the feet are covered in hair-free (glabrous) skin. Credit: Lucy Reading-Ikkanda Epidermis The outermost layer of skin acts as a mechanical and antimicrobial barrier, and consists of several layers. Its top part, the stratum corneum, prevents water from leaving the body and toxic substances from entering. Dermis Nerve endings in skin's middle layer help people to feel sensations such as itching, pain, pleasure and heat. The dermis produces sweat and oils, and contains hair follicles. It also hosts a variety of immune cells. Subcutaneous fat Skin's deepest layer is sandwiched between the dermis and skeletal muscles. Its roles include fat storage, connecting the dermis to muscle and bone, and controlling body temperature. Hairy skin More than 90\% of the body is covered by hairy skin1. It is involved in perceiving a variety of tacile sensations, including those that form part of social exchanges, and the ability to detect the presence of foreign objects. In hairy skin, the epidermis is less than 0.1 millimetres thick and the dermis is 1–2 millimetres deep. Glabrous skin Hair-free skin is found mainly on the palms and soles. It is innervated by specialized nerves that help us to understand subtle tactile details. Such skin is thicker than hairy skin; the epidermis is about 1.5 millimetres thick and the dermis is about 3 millimetres deep. Sensational sensitivity Skin's somatosensory system comprises more than a dozen subtypes of sensory neuron, but only those involved in tactile sensation are well understood. Such neurons enable skin to react to and interpret myriad stimuli, including temperature gradients, pressure and physical damage. Credit: Lucy Reading-Ikkanda C fibre These unmyelinated nerve fibres are found only in hairy skin. Although sensitive to indentation, they are most active when a stimulus moves slowly across the skin's surface. Ruffini ending Found in the dermis of both hairy and glabrous skin, these sensory receptors respond optimally to stretching of skin. Pacinian corpuscle Located deep in the dermis of both types of skin, Pacinian corpuscles respond to high-frequency vibration. Meissner corpuscle These nerve receptors lie just beneath the epidermis of glabrous skin, where they detect movement across the skin and fluttering touch. Merkel cell Part of the stratum basale of the epidermis, these cells help to relay information about the texture, curvature and shape of objects. Merkel cells are most dense in glabrous skin. Protective layers Skin's epidermis and dermis help to protect the body from microbes, pollutants, ultraviolet radiation and excessive loss or absorption of water. Credit: Lucy Reading-Ikkanda Barrier breakdown Despite its many superpowers, the skin is not infallible. Because of its visibility, diseases that affect the skin can have psychological as well as physical effects. Burdensome boundary Skin disease's worldwide burden can be quantified in terms of the disability-adjusted life year (DALY), which reflects a lost year of healthy life. The main burden falls on people aged 15–19, mostly owing to acne vulgaris. From the age of 50, there is a slow increase in burden as skin loses function and the incidence of skin cancer rises. Dermatitis, including eczema, persists throughout life; people tend not to outgrow the condition but learn to better manage it. Source: Inst. Health Metrics/Univ. Washington Cancer comparison Melanoma kills more people worldwide than does non-melanoma skin cancer, even though non-melanoma is much more common. However, deaths from melanoma are dwarfed by those from other cancers. Skin cancer was responsible for about 60\% of US skin-related deaths in 2013.},
  keywords = {Cancer,Diseases,Medical research,Physiology},
  number = {7732}
}

@book{Govindan2009-wu,
  title = {\{\vphantom\}{{DeVita}}\vphantom\{\}, Hellman, and Rosenberg's Cancer: {{Principles}} \& Practice of Oncology Review},
  author = {Govindan, Ramaswamy and DeVita, Vincent T},
  date = {2009},
  publisher = {{Lippincott Williams \& Wilkins}},
  abstract = {Based on DeVita, Lawrence, and Rosenberg's Cancer: Principles \& Practice of Oncology, Eighth Edition, this comprehensive question-and-answer review book covers the entire specialty of oncology and provides thorough preparation for oncology boards. The book contains hundreds of multiple-choice and case-based questions covering the principles of surgical oncology, radiation oncology, medical oncology, and hematology/oncology and the biology, diagnosis, staging, and multimodality treatment of cancers at every anatomic site. Included are state-of-the-art chapters on molecular techniques, targeted therapies, and current approaches to cancer prevention. Questions are followed by answers and detailed explanations. A companion Website will offer an interactive question bank for individual self-testing.}
}

@article{Greenspan2016-jt,
  title = {Guest Editorial Deep Learning in Medical Imaging: {{Overview}} and Future Promise of an Exciting New Technique},
  author = {Greenspan, H and van Ginneken, B and Summers, R M},
  date = {2016-05},
  journaltitle = {IEEE Transactions on Medical Imaging},
  shortjournal = {IEEE Trans. Med. Imaging},
  volume = {35},
  pages = {1153--1159},
  abstract = {The papers in this special section focus on the technology and applications supported by deep learning. Deep learning is a growing trend in general data analysis and has been termed one of the 10 breakthrough technologies of 2013. Deep learning is an improvement of artificial neural networks, consisting of more layers that permit higher levels of abstraction and improved predictions from data. To date, it is emerging as the leading machine-learning tool in the general imaging and computer vision domains. In particular, convolutional neural networks (CNNs) have proven to be powerful tools for a broad range of computer vision tasks. Deep CNNs automatically learn mid-level and high-level abstractions obtained from raw data (e.g., images). Recent results indicate that the generic descriptors extracted from CNNs are extremely effective in object recognition and localization in natural images. Medical image analysis groups across the world are quickly entering the field and applying CNNs and other deep learning methodologies to a wide variety of applications.},
  keywords = {Artificial neural networks,Biomedical image proces},
  number = {5},
  options = {useprefix=true}
}

@online{Gregor2015,
  title = {{{DRAW}}: {{A}} Recurrent Neural Network for Image Generation},
  author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
  date = {2015-02},
  url = {http://arxiv.org/abs/1502.04623},
  abstract = {This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.},
  archivePrefix = {arXiv},
  arxivid = {1502.04623},
  eprint = {1502.04623},
  eprinttype = {arxiv}
}

@article{GTEx_Consortium2013-dx,
  title = {The \{\vphantom\}{{Genotype}}-{{Tissue}}\vphantom\{\} Expression (\{\vphantom\}{{GTEx}}\vphantom\{\}) Project},
  author = {{GTEx Consortium}},
  date = {2013-06},
  journaltitle = {Nature Genetics},
  shortjournal = {Nat. Genet.},
  volume = {45},
  pages = {580--585},
  abstract = {Genome-wide association studies have identified thousands of loci for common diseases, but, for the majority of these, the mechanisms underlying disease susceptibility remain unknown. Most associated variants are not correlated with protein-coding changes, suggesting that polymorphisms in regulatory regions probably contribute to many disease phenotypes. Here we describe the Genotype-Tissue Expression (GTEx) project, which will establish a resource database and associated tissue bank for the scientific community to study the relationship between genetic variation and gene expression in human tissues.},
  number = {6}
}

@article{Gulshan2016-xh,
  title = {Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs},
  author = {Gulshan, Varun and Peng, Lily and Coram, Marc and Stumpe, Martin C and Wu, Derek and Narayanaswamy, Arunachalam and Venugopalan, Subhashini and Widner, Kasumi and Madams, Tom and Cuadros, Jorge and Kim, Ramasamy and Raman, Rajiv and Nelson, Philip C and Mega, Jessica L and Webster, Dale R},
  date = {2016-12},
  journaltitle = {JAMA},
  volume = {316},
  pages = {2402--2410},
  publisher = {{American Medical Association}},
  abstract = {ImportanceDeep learning is a family of computational methods that allow an algorithm to program itself by learning from a large set of examples that demonstrate the desired behavior, removing the need to specify rules explicitly. Application of these methods to medical imaging requires further assessment and validation.ObjectiveTo apply deep learning to create an algorithm for automated detection of diabetic retinopathy and diabetic macular edema in retinal fundus photographs.Design and SettingA specific type of neural network optimized for image classification called a deep convolutional neural network was trained using a retrospective development data set of 128 175 retinal images, which were graded 3 to 7 times for diabetic retinopathy, diabetic macular edema, and image gradability by a panel of 54 US licensed ophthalmologists and ophthalmology senior residents between May and December 2015. The resultant algorithm was validated in January and February 2016 using 2 separate data sets, both graded by at least 7 US board-certified ophthalmologists with high intragrader consistency.ExposureDeep learning–trained algorithm.Main Outcomes and MeasuresThe sensitivity and specificity of the algorithm for detecting referable diabetic retinopathy (RDR), defined as moderate and worse diabetic retinopathy, referable diabetic macular edema, or both, were generated based on the reference standard of the majority decision of the ophthalmologist panel. The algorithm was evaluated at 2 operating points selected from the development set, one selected for high specificity and another for high sensitivity.ResultsThe EyePACS-1 data set consisted of 9963 images from 4997 patients (mean age, 54.4 years; 62.2\% women; prevalence of RDR, 683/8878 fully gradable images [7.8\%]); the Messidor-2 data set had 1748 images from 874 patients (mean age, 57.6 years; 42.6\% women; prevalence of RDR, 254/1745 fully gradable images [14.6\%]). For detecting RDR, the algorithm had an area under the receiver operating curve of 0.991 (95\% CI, 0.988-0.993) for EyePACS-1 and 0.990 (95\% CI, 0.986-0.995) for Messidor-2. Using the first operating cut point with high specificity, for EyePACS-1, the sensitivity was 90.3\% (95\% CI, 87.5\%-92.7\%) and the specificity was 98.1\% (95\% CI, 97.8\%-98.5\%). For Messidor-2, the sensitivity was 87.0\% (95\% CI, 81.1\%-91.0\%) and the specificity was 98.5\% (95\% CI, 97.7\%-99.1\%). Using a second operating point with high sensitivity in the development set, for EyePACS-1 the sensitivity was 97.5\% and specificity was 93.4\% and for Messidor-2 the sensitivity was 96.1\% and specificity was 93.9\%.Conclusions and RelevanceIn this evaluation of retinal fundus photographs from adults with diabetes, an algorithm based on deep machine learning had high sensitivity and specificity for detecting referable diabetic retinopathy. Further research is necessary to determine the feasibility of applying this algorithm in the clinical setting and to determine whether use of the algorithm could lead to improved care and outcomes compared with current ophthalmologic assessment.},
  keywords = {diabetic,diabetic retinopathy,neural networks (computer)},
  number = {22}
}

@online{Guo2017,
  title = {On Calibration of Modern Neural Networks},
  author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  date = {2017-06},
  url = {http://arxiv.org/abs/1706.04599},
  abstract = {Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a single-parameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.},
  archivePrefix = {arXiv},
  arxivid = {1706.04599},
  eprint = {1706.04599},
  eprinttype = {arxiv}
}

@inproceedings{Gupta2019,
  title = {{{GAN}}-{{Based}} Image Enrichment in Digital Pathology Boosts Segmentation Accuracy {{BT}} - Medical Image Computing and Computer Assisted Intervention – {{MICCAI}} 2019},
  author = {Gupta, Laxmi and Klinkhammer, Barbara M and Boor, Peter and Merhof, Dorit and Gadermayr, Michael},
  editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M and Staib, Lawrence H and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
  date = {2019},
  pages = {631--639},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  abstract = {We introduce the idea of ‘image enrichment' whereby the information content of images is increased in order to enhance segmentation accuracy. Unlike in data augmentation, the focus is not on increasing the number of training samples (by adding new virtual samples), but on increasing the information for each sample. For this purpose, we use a GAN-based image-to-image translation approach to generate corresponding virtual samples from a given (original) image. The virtual samples are then merged with the original sample to create a multi-channel image, which serves as the enriched image. We train and test a segmentation network on enriched images showing kidney pathology and obtain segmentation scores exhibiting an improvement compared to conventional processing of the original images only. We perform an extensive evaluation and discuss the reasons for the improvement.},
  isbn = {978-3-030-32239-7}
}

@article{Gutierrez2011-cy,
  title = {A Supervised Visual Model for Finding Regions of Interest in Basal Cell Carcinoma Images},
  author = {Gutiérrez, Ricardo and Gómez, Francisco and Roa-Peña, Luc\textbackslash '\textbackslash ia and Romero, Eduardo},
  date = {2011-03},
  journaltitle = {Diagnostic Pathology},
  shortjournal = {Diagn. Pathol.},
  volume = {6},
  pages = {26},
  abstract = {This paper introduces a supervised learning method for finding diagnostic regions of interest in histopathological images. The method is based on the cognitive process of visual selection of relevant regions that arises during a pathologist's image examination. The proposed strategy emulates the interaction of the visual cortex areas V1, V2 and V4, being the V1 cortex responsible for assigning local levels of relevance to visual inputs while the V2 cortex gathers together these small regions according to some weights modulated by the V4 cortex, which stores some learned rules. This novel strategy can be considered as a complex mix of “bottom-up” and “top-down” mechanisms, integrated by calculating a unique index inside each region. The method was evaluated on a set of 338 images in which an expert pathologist had drawn the Regions of Interest. The proposed method outperforms two state-of-the-art methods devised to determine Regions of Interest (RoIs) in natural images. The quality gain with respect to an adaptated Itti's model which found RoIs was 3.6 dB in average, while with respect to the Achanta's proposal was 4.9 dB.}
}

@article{hajar2017physician,
  title = {The Physician's Oath: {{Historical}} Perspectives},
  author = {Hajar, Rachel},
  date = {2017},
  journaltitle = {Heart views: the official journal of the Gulf Heart Association},
  volume = {18},
  pages = {154},
  publisher = {{Wolters Kluwer–Medknow Publications}},
  number = {4}
}

@article{Halicek2017,
  title = {Deep Convolutional Neural Networks for Classifying Head and Neck Cancer Using Hyperspectral Imaging},
  author = {Halicek, Martin and Lu, Guolan and Little, James V and Wang, Xu and Patel, Mihir and Griffith, Christopher C and El-Deiry, Mark W and Chen, Amy Y and Fei, Baowei},
  date = {2017},
  journaltitle = {Journal of biomedical optics},
  volume = {22},
  pages = {60503},
  issn = {1083-3668},
  number = {6}
}

@inproceedings{Halicek2019,
  title = {Detection of Squamous Cell Carcinoma in Digitized Histological Images from the Head and Neck Using Convolutional Neural Networks},
  booktitle = {Medical Imaging 2019: {{Digital}} Pathology},
  author = {Halicek, Martin and Shahedi, Maysam and Little, James V and Chen, Amy Y and Myers, Larry L and Sumer, Baran D and Fei, Baowei},
  date = {2019},
  volume = {10956},
  pages = {109560K},
  publisher = {{International Society for Optics and Photonics}}
}

@article{Hambleton2019,
  title = {Australia's Digital Health Journey},
  author = {Hambleton, Steven J. and Aloizos AM, John},
  date = {2019},
  journaltitle = {Medical Journal of Australia},
  volume = {210},
  pages = {S5--S6},
  issn = {13265377},
  doi = {10.5694/mja2.50039},
  keywords = {eHealth,Health policy,Information management,Information storage and retrieval,Patient safety,Public health,Technology},
  number = {S6}
}

@article{Hamet2017,
  title = {Artificial Intelligence in Medicine},
  author = {Hamet, Pavel and Tremblay, Johanne},
  date = {2017},
  journaltitle = {Metabolism: Clinical and Experimental},
  volume = {69},
  pages = {S36--S40},
  publisher = {{Elsevier Inc.}},
  issn = {15328600},
  doi = {10.1016/j.metabol.2017.01.011},
  url = {http://dx.doi.org/10.1016/j.metabol.2017.01.011},
  abstract = {Artificial Intelligence (AI) is a general term that implies the use of a computer to model intelligent behavior with minimal human intervention. AI is generally accepted as having started with the invention of robots. The term derives from the Czech word robota, meaning biosynthetic machines used as forced labor. In this field, Leonardo Da Vinci's lasting heritage is today's burgeoning use of robotic-assisted surgery, named after him, for complex urologic and gynecologic procedures. Da Vinci's sketchbooks of robots helped set the stage for this innovation. AI, described as the science and engineering of making intelligent machines, was officially born in 1956. The term is applicable to a broad range of items in medicine such as robotics, medical diagnosis, medical statistics, and human biology—up to and including today's “omics”. AI in medicine, which is the focus of this review, has two main branches: virtual and physical. The virtual branch includes informatics approaches from deep learning information management to control of health management systems, including electronic health records, and active guidance of physicians in their treatment decisions. The physical branch is best represented by robots used to assist the elderly patient or the attending surgeon. Also embodied in this branch are targeted nanorobots, a unique new drug delivery system. The societal and ethical complexities of these applications require further reflection, proof of their medical utility, economic value, and development of interdisciplinary strategies for their wider application.},
  keywords = {Artificial intelligence,Avatars,Future of medicine,Robots}
}

@article{Hamet2017a,
  title = {Artificial Intelligence in Medicine},
  author = {Hamet, Pavel and Tremblay, Johanne},
  date = {2017-04},
  journaltitle = {Metabolism: Clinical and Experimental},
  volume = {69},
  pages = {S36--S40},
  publisher = {{W.B. Saunders}},
  issn = {15328600},
  doi = {10.1016/j.metabol.2017.01.011},
  abstract = {Artificial Intelligence (AI) is a general term that implies the use of a computer to model intelligent behavior with minimal human intervention. AI is generally accepted as having started with the invention of robots. The term derives from the Czech word robota, meaning biosynthetic machines used as forced labor. In this field, Leonardo Da Vinci's lasting heritage is today's burgeoning use of robotic-assisted surgery, named after him, for complex urologic and gynecologic procedures. Da Vinci's sketchbooks of robots helped set the stage for this innovation. AI, described as the science and engineering of making intelligent machines, was officially born in 1956. The term is applicable to a broad range of items in medicine such as robotics, medical diagnosis, medical statistics, and human biology—up to and including today's “omics”. AI in medicine, which is the focus of this review, has two main branches: virtual and physical. The virtual branch includes informatics approaches from deep learning information management to control of health management systems, including electronic health records, and active guidance of physicians in their treatment decisions. The physical branch is best represented by robots used to assist the elderly patient or the attending surgeon. Also embodied in this branch are targeted nanorobots, a unique new drug delivery system. The societal and ethical complexities of these applications require further reflection, proof of their medical utility, economic value, and development of interdisciplinary strategies for their wider application.},
  keywords = {Artificial intelligence,Avatars,Future of medicine,Robots}
}

@online{Han2016,
  title = {{{DSD}}: {{Dense}}-Sparse-Dense Training for Deep Neural Networks},
  author = {Han, Song and Pool, Jeff and Narang, Sharan and Mao, Huizi and Gong, Enhao and Tang, Shijian and Elsen, Erich and Vajda, Peter and Paluri, Manohar and Tran, John and Catanzaro, Bryan and Dally, William J.},
  date = {2016-07},
  url = {http://arxiv.org/abs/1607.04381},
  abstract = {Modern deep neural networks have a large number of parameters, making them very hard to train. We propose DSD, a dense-sparse-dense training flow, for regularizing deep neural networks and achieving better optimization performance. In the first D (Dense) step, we train a dense network to learn connection weights and importance. In the S (Sparse) step, we regularize the network by pruning the unimportant connections with small weights and retraining the network given the sparsity constraint. In the final D (re-Dense) step, we increase the model capacity by removing the sparsity constraint, re-initialize the pruned parameters from zero and retrain the whole dense network. Experiments show that DSD training can improve the performance for a wide range of CNNs, RNNs and LSTMs on the tasks of image classification, caption generation and speech recognition. On ImageNet, DSD improved the Top1 accuracy of GoogLeNet by 1.1\%, VGG-16 by 4.3\%, ResNet-18 by 1.2\% and ResNet-50 by 1.1\%, respectively. On the WSJ'93 dataset, DSD improved DeepSpeech and DeepSpeech2 WER by 2.0\% and 1.1\%. On the Flickr-8K dataset, DSD improved the NeuralTalk BLEU score by over 1.7. DSD is easy to use in practice: at training time, DSD incurs only one extra hyper-parameter: the sparsity ratio in the S step. At testing time, DSD doesn't change the network architecture or incur any inference overhead. The consistent and significant performance gain of DSD experiments shows the inadequacy of the current training methods for finding the best local optimum, while DSD effectively achieves superior optimization performance for finding a better solution. DSD models are available to download at https://songhan.github.io/DSD.},
  archivePrefix = {arXiv},
  arxivid = {1607.04381},
  eprint = {1607.04381},
  eprinttype = {arxiv}
}

@article{Han2017-no,
  title = {Breast Cancer Multi-Classification from Histopathological Images with Structured Deep Learning Model},
  author = {Han, Zhongyi and Wei, Benzheng and Zheng, Yuanjie and Yin, Yilong and Li, Kejian and Li, Shuo},
  date = {2017-06},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci. Rep.},
  volume = {7},
  pages = {4172},
  abstract = {Automated breast cancer multi-classification from histopathological images plays a key role in computer-aided breast cancer diagnosis or prognosis. Breast cancer multi-classification is to identify subordinate classes of breast cancer (Ductal carcinoma, Fibroadenoma, Lobular carcinoma, etc.). However, breast cancer multi-classification from histopathological images faces two main challenges from: (1) the great difficulties in breast cancer multi-classification methods contrasting with the classification of binary classes (benign and malignant), and (2) the subtle differences in multiple classes due to the broad variability of high-resolution image appearances, high coherency of cancerous cells, and extensive inhomogeneity of color distribution. Therefore, automated breast cancer multi-classification from histopathological images is of great clinical significance yet has never been explored. Existing works in literature only focus on the binary classification but do not support further breast cancer quantitative assessment. In this study, we propose a breast cancer multi-classification method using a newly proposed deep learning model. The structured deep learning model has achieved remarkable performance (average 93.2\% accuracy) on a large-scale dataset, which demonstrates the strength of our method in providing an efficient tool for breast cancer multi-classification in clinical settings.},
  number = {1}
}

@article{Han2018,
  title = {Classification of the Clinical Images for Benign and Malignant Cutaneous Tumors Using a Deep Learning Algorithm},
  author = {Han, Seung Seog and Kim, Myoung Shin and Lim, Woohyung and Park, Gyeong Hun and Park, Ilwoo and Chang, Sung Eun},
  date = {2018-07},
  journaltitle = {Journal of Investigative Dermatology},
  volume = {138},
  pages = {1529--1538},
  publisher = {{Elsevier B.V.}},
  issn = {15231747},
  doi = {10.1016/j.jid.2018.01.028},
  abstract = {We tested the use of a deep learning algorithm to classify the clinical images of 12 skin diseases—basal cell carcinoma, squamous cell carcinoma, intraepithelial carcinoma, actinic keratosis, seborrheic keratosis, malignant melanoma, melanocytic nevus, lentigo, pyogenic granuloma, hemangioma, dermatofibroma, and wart. The convolutional neural network (Microsoft ResNet-152 model; Microsoft Research Asia, Beijing, China) was fine-tuned with images from the training portion of the Asan dataset, MED-NODE dataset, and atlas site images (19,398 images in total). The trained model was validated with the testing portion of the Asan, Hallym and Edinburgh datasets. With the Asan dataset, the area under the curve for the diagnosis of basal cell carcinoma, squamous cell carcinoma, intraepithelial carcinoma, and melanoma was 0.96 ± 0.01, 0.83 ± 0.01, 0.82 ± 0.02, and 0.96 ± 0.00, respectively. With the Edinburgh dataset, the area under the curve for the corresponding diseases was 0.90 ± 0.01, 0.91 ± 0.01, 0.83 ± 0.01, and 0.88 ± 0.01, respectively. With the Hallym dataset, the sensitivity for basal cell carcinoma diagnosis was 87.1\% ± 6.0\%. The tested algorithm performance with 480 Asan and Edinburgh images was comparable to that of 16 dermatologists. To improve the performance of convolutional neural network, additional images with a broader range of ages and ethnicities should be collected.},
  number = {7}
}

@article{Hanna2019,
  title = {Whole Slide Imaging Equivalency and Efficiency Study: Experience at a Large Academic Center},
  author = {Hanna, Matthew G. and Reuter, Victor E. and Hameed, Meera R. and Tan, Lee K. and Chiang, Sarah and Sigel, Carlie and Hollmann, Travis and Giri, Dilip and Samboy, Jennifer and Moradel, Carlos and Rosado, Andrea and Otilano, John R. and England, Christine and Corsale, Lorraine and Stamelos, Evangelos and Yagi, Yukako and Schüffler, Peter J. and Fuchs, Thomas and Klimstra, David S. and Sirintrapun, S. Joseph},
  date = {2019-07},
  journaltitle = {Modern Pathology},
  publisher = {{Nature Publishing Group}},
  issn = {15300285},
  doi = {10.1038/s41379-019-0205-0},
  abstract = {Whole slide imaging is Food and Drug Administration-approved for primary diagnosis in the United States of America; however, relatively few pathology departments in the country have fully implemented an enterprise wide digital pathology system enabled for primary diagnosis. Digital pathology has significant potential to transform pathology practice with several published studies documenting some level of diagnostic equivalence between digital and conventional systems. However, whole slide imaging also has significant potential to disrupt pathology practice, due to the differences in efficiency of manipulating digital images vis-à-vis glass slides, and studies on the efficiency of actual digital pathology workload are lacking. Our randomized, equivalency and efficiency study aimed to replicate clinical workflow, comparing conventional microscopy to a complete digital pathology signout using whole slide images, evaluating the equivalency and efficiency of glass slide to whole slide image reporting, reflective of true pathology practice workloads in the clinical setting. All glass slides representing an entire day's routine clinical signout workload for six different anatomic pathology subspecialties at Memorial Sloan Kettering Cancer Center were scanned on Leica Aperio AT2 at ×40 (0.25 µm/pixel). Integration of whole slide images for each accessioned case is through an interface between the Leica eSlide manager database and the laboratory information system, Cerner CoPathPlus. Pathologists utilized a standard institution computer workstation and viewed whole slide images through an internally developed, vendor agnostic whole slide image viewer, named the “MSK Slide Viewer”. Subspecialized pathologists first reported on glass slides from surgical pathology cases using routine clinical workflow. Glass slides were de-identified, scanned, and re-accessioned in the laboratory information system test environment. After a washout period of 13 weeks, pathologists reported the same clinical workload using whole slide image integrated within the laboratory information system. Intraobserver equivalency metrics included top-line diagnosis, margin status, lymphovascular and/or perineural invasion, pathology stage, and the need to order ancillary testing (i.e., recuts, immunohistochemistry). Turnaround time (efficiency) evaluation was defined by the start of each case when opened in the laboratory information system and when the case was completed for that day (i.e., case sent to signout queue or pending ancillary studies). Eight pathologists participated from the following subspecialties: bone and soft tissue, genitourinary, gastrointestinal, breast, gynecologic, and dermatopathology. Glass slides signouts comprised of 204 cases, encompassing 2091 glass slides; and digital signouts comprised of 199 cases, encompassing 2073 whole slide images. The median whole slide image file size was 1.54 GB; scan time/slide, 6 min 24 s; and scan area 32.1 × 18.52 mm. Overall diagnostic equivalency (e.g., top-line diagnosis) was 99.3\% between digital and glass slide signout; however, signout using whole slide images showed a median overall 19\% decrease in efficiency per case. No significant difference by reader, subspecialty, or specimen type was identified. Our experience is the most comprehensive study to date and shows high intraobserver whole slide image to glass slide equivalence in reporting of true clinical workflows and workloads. Efficiency needs to improve for digital pathology to gain more traction among pathologists.}
}

@article{harari2017reboot,
  title = {Reboot for the {{AI}} Revolution},
  author = {Harari, Yuval Noah},
  date = {2017},
  journaltitle = {Nature News},
  volume = {550},
  pages = {324},
  number = {7676}
}

@article{Haratake1987-nu,
  title = {\{\vphantom\}{{INTER}}‐and\vphantom\{\} \{\vphantom\}{{INTRA}}‐{{PATHOLOGIST}}\vphantom\{\} \{\vphantom\}{{VARIABILITY}}\vphantom\{\} \{\vphantom\}{{IN}}\vphantom\{\} \{\vphantom\}{{HISTOLOGIC}}\vphantom\{\} \{\vphantom\}{{DIAGNOSES}}\vphantom\{\} \{\vphantom\}{{OF}}\vphantom\{\} \{\vphantom\}{{LUNG}}\vphantom\{\} \{\vphantom\}{{CANCER}}\vphantom\{\}},
  author = {Haratake, J and Home, A and Tokudome, S and Era, S and {Others}},
  date = {1987},
  journaltitle = {Pathology},
  publisher = {{Wiley Online Library}},
  abstract = {Abstract To estimate variability and reliability in histologic diagnosis (Dx) of lung cancers, lung cancer preparations were divided into eight equal sets and diagnosed independently by an eight-man pathology panel. Majority Dx (Dx affirmed by more than 4 panelists) was regarded as the consensus Dx of each cancer. The consensus rate of each panelist ranged from 78.8\% to 96.1\% with an average of 89.4\%. The consensus rates were not significantly different among the panelists. Relatively high inter-pathologist agreement was observed in}
}

@article{Hart2019,
  title = {Classification of Melanocytic Lesions in Selected and Whole-Slide Images via Convolutional Neural Networks},
  author = {Hart, StevenN and Flotte, William and Norgan, AndrewP and Shah, KabeerK and Buchan, ZacharyR and Mounajjed, Taofic and Flotte, ThomasJ},
  date = {2019-02},
  journaltitle = {Journal of Pathology Informatics},
  volume = {10},
  pages = {5},
  publisher = {{Medknow}},
  issn = {2153-3539},
  doi = {10.4103/jpi.jpi_32_18},
  abstract = {Whole-slide images (WSIs) are a rich new source of biomedical imaging data. The use of automated systems to classify and segment WSIs has recently come to forefront of the pathology research community. While digital slides have obvious educational and clinical uses, their most exciting potential lies in the application of quantitative computational tools to automate search tasks, assist in classic diagnostic classification tasks, and improve prognosis and theranostics. An essential step in enabling these advancements is to apply advances in machine learning and artificial intelligence from other fields to previously inaccessible pathology datasets, thereby enabling the application of new technologies to solve persistent diagnostic challenges in pathology. Here, we applied convolutional neural networks to differentiate between two forms of melanocytic lesions (Spitz and conventional). Classification accuracy at the patch level was 99.0\%-2\% when applied to WSI. Importantly, when the model was trained without careful image curation by a pathologist, the training took significantly longer and had lower overall performance. These results highlight the utility of augmented human intelligence in digital pathology applications, and the critical role pathologists will play in the evolution of computational pathology algorithms. © 2019 Journal of Pathology Informatics | Published by Wolters Kluwer - Medknow.},
  number = {1}
}

@article{Harting2015,
  title = {Medical Photography: {{Current}} Technology, Evolving Issues and Legal Perspectives},
  author = {Harting, M. T. and Dewees, J. M. and Vela, K. M. and Khirallah, R. T.},
  date = {2015-04},
  journaltitle = {International journal of clinical practice},
  volume = {69},
  pages = {401--409},
  publisher = {{Blackwell Publishing Ltd}},
  issn = {17421241},
  doi = {10.1111/ijcp.12627},
  abstract = {© 2015 John Wiley \& Sons Ltd.Medical photographic image capture and data management has undergone a rapid and compelling change in complexity over the last 20 years. This is because of multiple factors, including significant advances in ease of photograph capture, alongside an evolution of mechanisms of data portability/dissemination, combined with governmental focus on health information privacy. Literature to guide medical, legal, governmental and business professionals when dealing with issues related to medical photography is virtually nonexistent. Herein, we will address the breadth of uses of medical photography, device properties/specific devices utilised for image capture, methods of data transfer and dissemination and patient perceptions and attitudes regarding photography in a medical setting. In addition, we will address the legal implications, including legal precedent, and privacy law, informed consent, protected health information and the Health Insurance Portability and Accountability Act (HIPAA), as they pertain to medical photography.},
  number = {4}
}

@article{Hatipoglu2017-rn,
  title = {Cell Segmentation in Histopathological Images with Deep Learning Algorithms by Utilizing Spatial Relationships},
  author = {Hatipoglu, Nuh and Bilgin, Gokhan},
  date = {2017-10},
  journaltitle = {Medical \& Biological Engineering \& Computing},
  shortjournal = {Med. Biol. Eng. Comput.},
  volume = {55},
  pages = {1829--1848},
  publisher = {{Springer}},
  abstract = {In many computerized methods for cell detection, segmentation, and classification in digital histopathology that have recently emerged, the task of cell segmentation remains a chief problem for image processing in designing computer-aided diagnosis (CAD) systems. In research and diagnostic studies on cancer, pathologists can use CAD systems as second readers to analyze high-resolution histopathological images. Since cell detection and segmentation are critical for cancer grade assessments, cellular and extracellular structures should primarily be extracted from histopathological images. In response, we sought to identify a useful cell segmentation approach with histopathological images that uses not only prominent deep learning algorithms (i.e., convolutional neural networks, stacked autoencoders, and deep belief networks), but also spatial relationships, information of which is critical for achieving better cell segmentation results. To that end, we collected cellular and extracellular samples from histopathological images by windowing in small patches with various sizes. In experiments, the segmentation accuracies of the methods used improved as the window sizes increased due to the addition of local spatial and contextual information. Once we compared the effects of training sample size and influence of window size, results revealed that the deep learning algorithms, especially convolutional neural networks and partly stacked autoencoders, performed better than conventional methods in cell segmentation.},
  keywords = {Computer-aided diagnosis systems,Deep learning al},
  number = {10}
}

@article{Havaei2017,
  title = {Brain Tumor Segmentation with Deep Neural Networks},
  author = {Havaei, Mohammad and Davy, Axel and Warde-Farley, David and Biard, Antoine and Courville, Aaron and Bengio, Yoshua and Pal, Chris and Jodoin, Pierre Marc and Larochelle, Hugo},
  date = {2017},
  journaltitle = {Medical Image Analysis},
  volume = {35},
  pages = {18--31},
  publisher = {{Elsevier B.V.}},
  issn = {13618423},
  doi = {10.1016/j.media.2016.05.004},
  url = {http://dx.doi.org/10.1016/j.media.2016.05.004},
  abstract = {In this paper, we present a fully automatic brain tumor segmentation method based on Deep Neural Networks (DNNs). The proposed networks are tailored to glioblastomas (both low and high grade) pictured in MR images. By their very nature, these tumors can appear anywhere in the brain and have almost any kind of shape, size, and contrast. These reasons motivate our exploration of a machine learning solution that exploits a flexible, high capacity DNN while being extremely efficient. Here, we give a description of different model choices that we've found to be necessary for obtaining competitive performance. We explore in particular different architectures based on Convolutional Neural Networks (CNN), i.e. DNNs specifically adapted to image data.We present a novel CNN architecture which differs from those traditionally used in computer vision. Our CNN exploits both local features as well as more global contextual features simultaneously. Also, different from most traditional uses of CNNs, our networks use a final layer that is a convolutional implementation of a fully connected layer which allows a 40 fold speed up. We also describe a 2-phase training procedure that allows us to tackle difficulties related to the imbalance of tumor labels. Finally, we explore a cascade architecture in which the output of a basic CNN is treated as an additional source of information for a subsequent CNN. Results reported on the 2013 BRATS test data-set reveal that our architecture improves over the currently published state-of-the-art while being over 30 times faster.},
  keywords = {Brain tumor segmentation,Cascaded convolutional neural networks,Convolutional neural networks,Deep neural networks}
}

@inproceedings{He,
  title = {Delving Deep into Rectifiers: {{Surpassing}} Human-Level Performance on Imagenet Classification},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  pages = {1026--1034}
}

@inproceedings{He2016,
  title = {Identity Mappings in Deep Residual Networks},
  booktitle = {European Conference on Computer Vision},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016},
  pages = {630--645},
  publisher = {{Springer}}
}

@inproceedings{He2016a,
  title = {Deep Residual Learning for Image Recognition},
  booktitle = {Proceedings of the {{IEEE}} Computer Society Conference on Computer Vision and Pattern Recognition},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016},
  volume = {2016-Decem},
  pages = {770--778},
  issn = {10636919},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archivePrefix = {arXiv},
  arxivid = {1512.03385},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  isbn = {978-1-4673-8850-4}
}

@article{Hegde2019,
  title = {Similar Image Search for Histopathology: {{SMILY}}},
  author = {Hegde, Narayan and Hipp, Jason D and Liu, Yun and Emmert-Buck, Michael and Reif, Emily and Smilkov, Daniel and Terry, Michael and Cai, Carrie J and Amin, Mahul B and Mermel, Craig H},
  date = {2019},
  journaltitle = {npj Digital Medicine},
  volume = {2},
  pages = {56},
  publisher = {{Nature Publishing Group}},
  issn = {2398-6352},
  number = {1}
}

@article{Hekler2019,
  title = {Pathologist-Level Classification of Histopathological Melanoma Images with Deep Neural Networks},
  author = {Hekler, Achim and Utikal, Jochen Sven and Enk, Alexander H and Berking, Carola and Klode, Joachim and Schadendorf, Dirk and Jansen, Philipp and Franklin, Cindy and Holland-Letz, Tim and Krahl, Dieter},
  date = {2019},
  journaltitle = {European Journal of Cancer},
  volume = {115},
  pages = {79--83},
  publisher = {{Elsevier}},
  issn = {0959-8049}
}

@article{Heljakka2019,
  title = {Pioneer Networks: {{Progressively}} Growing Generative Autoencoder},
  author = {Heljakka, Ari and Solin, Arno and Kannala, Juho},
  date = {2019},
  journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {11361 LNCS},
  pages = {22--38},
  issn = {16113349},
  doi = {10.1007/978-3-030-20887-5_2},
  abstract = {We introduce a novel generative autoencoder network model that learns to encode and reconstruct images with high quality and resolution, and supports smooth random sampling from the latent space of the encoder. Generative adversarial networks (GANs) are known for their ability to simulate random high-quality images, but they cannot reconstruct existing images. Previous works have attempted to extend GANs to support such inference but, so far, have not delivered satisfactory high-quality results. Instead, we propose the Progressively Growing Generative Autoencoder (Pioneer) network which achieves high-quality reconstruction with images without requiring a GAN discriminator. We merge recent techniques for progressively building up the parts of the network with the recently introduced adversarial encoder–generator network. The ability to reconstruct input images is crucial in many real-world applications, and allows for precise intelligent manipulation of existing images. We show promising results in image synthesis and inference, with state-of-the-art results in CelebA inference tasks.},
  archivePrefix = {arXiv},
  arxivid = {1807.03026},
  eprint = {1807.03026},
  eprinttype = {arxiv},
  isbn = {9783030208868},
  keywords = {Autoencoder,Computer vision,Generative models}
}

@article{Heljakka2020,
  title = {Towards Photographic Image Manipulation with Balanced Growing of Generative Autoencoders},
  author = {Heljakka, Ari and Solin, Arno and Kannala, Juho},
  date = {2020},
  journaltitle = {Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020},
  pages = {3109--3118},
  doi = {10.1109/WACV45572.2020.9093375},
  abstract = {We present a generative autoencoder that provides fast encoding, faithful reconstructions (e.g. retaining the identity of a face), sharp generated/reconstructed samples in high resolutions, and a well-structured latent space that supports semantic manipulation of the inputs. There are no current autoencoder or GAN models that satisfactorily achieve all of these. We build on the progressively growing autoencoder model PIONEER , for which we completely alter the training dynamics based on a careful analysis of recently introduced normalization schemes. We show significantly improved visual and quantitative results for face identity conservation in CELEBA-HQ. Our model achieves state-of-the-art disentanglement of latent space, both quantitatively and via realistic image attribute manipulations. On the LSUN Bedrooms dataset, we improve the disentanglement performance of the vanilla PIONEER, despite having a simpler model. Overall, our results indicate that the PIONEER networks provide a way towards photorealistic face manipulation.},
  archivePrefix = {arXiv},
  arxivid = {1904.06145},
  eprint = {1904.06145},
  eprinttype = {arxiv},
  isbn = {9781728165530}
}

@article{Hendee2010-ru,
  title = {Addressing Overutilization in Medical Imaging},
  author = {Hendee, William R and Becker, Gary J and Borgstede, James P and Bosma, Jennifer and Casarella, William J and Erickson, Beth A and Maynard, C Douglas and Thrall, James H and Wallner, Paul E},
  date = {2010-10},
  journaltitle = {Radiology},
  volume = {257},
  pages = {240--245},
  abstract = {The growth in medical imaging over the past 2 decades has yielded unarguable benefits to patients in terms of longer lives of higher quality. This growth reflects new technologies and applications, including high-tech services such as multisection computed tomography (CT), magnetic resonance (MR) imaging, and positron emission tomography (PET). Some part of the growth, however, can be attributed to the overutilization of imaging services. This report examines the causes of the overutilization of imaging and identifies ways of addressing the causes so that overutilization can be reduced. In August 2009, the American Board of Radiology Foundation hosted a 2-day summit to discuss the causes and effects of the overutilization of imaging. More than 60 organizations were represented at the meeting, including health care accreditation and certification entities, foundations, government agencies, hospital and health systems, insurers, medical societies, health care quality consortia, and standards and regulatory agencies. Key forces influencing overutilization were identified. These include the payment mechanisms and financial incentives in the U.S. health care system; the practice behavior of referring physicians; self-referral, including referral for additional radiologic examinations; defensive medicine; missed educational opportunities when inappropriate procedures are requested; patient expectations; and duplicate imaging studies. Summit participants suggested several areas for improvement to reduce overutilization, including a national collaborative effort to develop evidence-based appropriateness criteria for imaging; greater use of practice guidelines in requesting and conducting imaging studies; decision support at point of care; education of referring physicians, patients, and the public; accreditation of imaging facilities; management of self-referral and defensive medicine; and payment reform.},
  number = {1}
}

@article{Heusel2017,
  title = {{{GANs}} Trained by a Two Time-Scale Update Rule Converge to a Local {{Nash}} Equilibrium},
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  date = {2017},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {2017-Decem},
  pages = {6627--6638},
  issn = {10495258},
  abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the 'Fréchet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
  issue = {Nips}
}

@online{Hinton2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  author = {Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
  date = {2012},
  archivePrefix = {arXiv},
  eprint = {1207.0580},
  eprinttype = {arxiv}
}

@article{Hinton2012-sv,
  title = {Deep Neural Networks for Acoustic Modeling in Speech Recognition: {{The}} Shared Views of Four Research Groups},
  author = {Hinton, G and Deng, L and Yu, D and Dahl, G E and r. Mohamed, A and Jaitly, N and Senior, A and Vanhoucke, V and Nguyen, P and Sainath, T N and Kingsbury, B},
  date = {2012-11},
  journaltitle = {IEEE Signal Processing Magazine (1991-present)},
  shortjournal = {IEEE Signal Process. Mag.},
  volume = {29},
  pages = {82--97},
  publisher = {{ieeexplore.ieee.org}},
  abstract = {Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.},
  keywords = {feedforward neural nets,Gaussian processes,hidden},
  number = {6},
  options = {useprefix=true}
}

@article{Hoffman2014-tu,
  title = {Comparison of Normalization Algorithms for Cross-Batch Color Segmentation of Histopathological Images},
  author = {Hoffman, Ryan A and Kothari, Sonal and Wang, May D},
  date = {2014},
  journaltitle = {Conf. Proc. IEEE Eng. Med. Biol. Soc.},
  volume = {2014},
  pages = {194--197},
  abstract = {Automated processing of digital histopathology slides has the potential to streamline patient care and provide new tools for cancer classification and grading. Before automatic analysis is possible, quality control procedures are applied to ensure that each image can be read consistently. One important quality control step is color normalization of the slide image, which adjusts for color variances (batch-effects) caused by differences in stain preparation and image acquisition equipment. Color batch-effects affect color-based features and reduce the performance of supervised color segmentation algorithms on images acquired separately. To identify an optimal normalization technique for histopathological color segmentation applications, five color normalization algorithms were compared in this study using 204 images from four image batches. Among the normalization methods, two global color normalization methods normalized colors from all stain simultaneously and three stain color normalization methods normalized colors from individual stains extracted using color deconvolution. Stain color normalization methods performed significantly better than global color normalization methods in 11 of 12 cross-batch experiments (p{$<$};0.05). Specifically, the stain color normalization method using k-means clustering was found to be the best choice because of high stain segmentation accuracy and low computational complexity.}
}

@article{Holzinger2016-oh,
  title = {Interactive Machine Learning for Health Informatics: When Do We Need the Human-in-the-Loop?},
  author = {Holzinger, Andreas},
  date = {2016-06},
  journaltitle = {Brain Inform},
  volume = {3},
  pages = {119--131},
  abstract = {Machine learning (ML) is the fastest growing field in computer science, and health informatics is among the greatest challenges. The goal of ML is to develop algorithms which can learn and improve over time and can be used for predictions. Most ML researchers concentrate on automatic machine learning (aML), where great advances have been made, for example, in speech recognition, recommender systems, or autonomous vehicles. Automatic approaches greatly benefit from big data with many training sets. However, in the health domain, sometimes we are confronted with a small number of data sets or rare events, where aML-approaches suffer of insufficient training samples. Here interactive machine learning (iML) may be of help, having its roots in reinforcement learning, preference learning, and active learning. The term iML is not yet well used, so we define it as “algorithms that can interact with agents and can optimize their learning behavior through these interactions, where the agents can also be human.” This “human-in-the-loop” can be beneficial in solving computationally hard problems, e.g., subspace clustering, protein folding, or k-anonymization of health data, where human expertise can help to reduce an exponential search space through heuristic selection of samples. Therefore, what would otherwise be an NP-hard problem, reduces greatly in complexity through the input and the assistance of a human agent involved in the learning phase.},
  keywords = {Health informatics,Interactive machine learning},
  number = {2}
}

@article{Horvitz1988,
  title = {Decision Theory in Expert Systems and \{\vphantom\}{{AI}}\vphantom\{\}},
  author = {Horvitz, Eric J and Breese, J S and Henrion, M},
  date = {1988},
  journaltitle = {Intl. J. Approximate Reasoning},
  volume = {2},
  pages = {247--302},
  url = {https://www.sciencedirect.com/science/article/pii/0888613X8890120X},
  keywords = {artificial intelligence,belief networks,decision analysis,decision theory,explanation,influence diagrams,knowledge engi-,neering,operations research,probability,uncertainty},
  number = {3}
}

@article{Hou2016-la,
  title = {Patch-Based Convolutional Neural Network for Whole Slide Tissue Image Classification},
  author = {Hou, Le and Samaras, Dimitris and Kurc, Tahsin M and Gao, Yi and Davis, James E and Saltz, Joel H},
  date = {2016-06},
  journaltitle = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
  volume = {2016},
  pages = {2424--2433},
  publisher = {{cv-foundation.org}},
  abstract = {Convolutional Neural Networks (CNN) are state-of-the-art models for many image classification tasks. However, to recognize cancer subtypes automatically, training a CNN on gigapixel resolution Whole Slide Tissue Images (WSI) is currently computationally impossible. The differentiation of cancer subtypes is based on cellular-level visual features observed on image patch scale. Therefore, we argue that in this situation, training a patch-level classifier on image patches will perform better than or similar to an image-level classifier. The challenge becomes how to intelligently combine patch-level classification results and model the fact that not all patches will be discriminative. We propose to train a decision fusion model to aggregate patch-level predictions given by patch-level CNNs, which to the best of our knowledge has not been shown before. Furthermore, we formulate a novel Expectation-Maximization (EM) based method that automatically locates discriminative patches robustly by utilizing the spatial relationships of patches. We apply our method to the classification of glioma and non-small-cell lung carcinoma cases into subtypes. The classification accuracy of our method is similar to the inter-observer agreement between pathologists. Although it is impossible to train CNNs on WSIs, we experimentally demonstrate using a comparable non-cancer dataset of smaller images that a patch-based CNN can outperform an image-based CNN.}
}

@article{Hou2017,
  title = {Deep Feature Consistent Variational Autoencoder},
  author = {Hou, Xianxu and Shen, Linlin and Sun, Ke and Qiu, Guoping},
  date = {2017},
  journaltitle = {Proceedings - 2017 IEEE Winter Conference on Applications of Computer Vision, WACV 2017},
  pages = {1133--1141},
  doi = {10.1109/WACV.2017.131},
  abstract = {We present a novel method for constructing Variational Autoencoder (VAE). Instead of using pixel-by-pixel loss, we enforce deep feature consistency between the input and the output of a VAE, which ensures the VAE's output to preserve the spatial correlation characteristics of the input, thus leading the output to have a more natural visual appearance and better perceptual quality. Based on recent deep learning works such as style transfer, we employ a pre-Trained deep convolutional neural network (CNN) and use its hidden features to define a feature perceptual loss for VAE training. Evaluated on the CelebA face dataset, we show that our model produces better results than other methods in the literature. We also show that our method can produce latent vectors that can capture the semantic information of face expressions and can be used to achieve state-of-The-Art performance in facial attribute prediction.},
  archivePrefix = {arXiv},
  arxivid = {1610.00291},
  eprint = {1610.00291},
  eprinttype = {arxiv},
  isbn = {9781509048229}
}

@online{Howard2017,
  title = {Mobilenets: {{Efficient}} Convolutional Neural Networks for Mobile Vision Applications},
  author = {Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  date = {2017},
  archivePrefix = {arXiv},
  eprint = {1704.04861},
  eprinttype = {arxiv}
}

@article{Hu2018,
  title = {Deep Learning for Image-Based Cancer Detection and Diagnosis − {{A}} Survey},
  author = {Hu, Zilong and Tang, Jinshan and Wang, Ziming and Zhang, Kai and Zhang, Lin and Sun, Qingling},
  date = {2018-11},
  journaltitle = {Pattern Recognition},
  volume = {83},
  pages = {134--149},
  publisher = {{Elsevier Ltd}},
  issn = {00313203},
  doi = {10.1016/j.patcog.2018.05.014},
  abstract = {In this paper, we aim to provide a survey on the applications of deep learning for cancer detection and diagnosis and hope to provide an overview of the progress in this field. In the survey, we firstly provide an overview on deep learning and the popular architectures used for cancer detection and diagnosis. Especially we present four popular deep learning architectures, including convolutional neural networks, fully convolutional networks, auto-encoders, and deep belief networks in the survey. Secondly, we provide a survey on the studies exploiting deep learning for cancer detection and diagnosis. The surveys in this part are organized based on the types of cancers. Thirdly, we provide a summary and comments on the recent work on the applications of deep learning to cancer detection and diagnosis and propose some future research directions.}
}

@inproceedings{Huang2017,
  title = {Densely Connected Convolutional Networks},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  date = {2017},
  pages = {4700--4708}
}

@article{Huang2017-xd,
  title = {\{\vphantom\}{{Epithelium}}-{{Stroma}}\vphantom\{\} Classification via Convolutional Neural Networks and Unsupervised Domain Adaptation in Histopathological Images},
  author = {Huang, Yue and Zheng, Han and Liu, Chi and Ding, Xinghao and Rohde, Gustavo K},
  date = {2017-11},
  journaltitle = {IEEE J Biomed Health Inform},
  volume = {21},
  pages = {1625--1632},
  abstract = {Epithelium-stroma classification is a necessary preprocessing step in histopathological image analysis. Current deep learning based recognition methods for histology data require collection of large volumes of labeled data in order to train a new neural network when there are changes to the image acquisition procedure. However, it is extremely expensive for pathologists to manually label sufficient volumes of data for each pathology study in a professional manner, which results in limitations in real-world applications. A very simple but effective deep learning method, that introduces the concept of unsupervised domain adaptation to a simple convolutional neural network (CNN), has been proposed in this paper. Inspired by transfer learning, our paper assumes that the training data and testing data follow different distributions, and there is an adaptation operation to more accurately estimate the kernels in CNN in feature extraction, in order to enhance performance by transferring knowledge from labeled data in source domain to unlabeled data in target domain. The model has been evaluated using three independent public epithelium-stroma datasets by cross-dataset validations. The experimental results demonstrate that for epithelium-stroma classification, the proposed framework outperforms the state-of-the-art deep neural network model, and it also achieves better performance than other existing deep domain adaptation methods. The proposed model can be considered to be a better option for real-world applications in histopathological image analysis, since there is no longer a requirement for large-scale labeled data in each specified domain.},
  number = {6}
}

@article{Huang2018,
  title = {Introvae: {{Introspective}} Variational Autoencoders for Photographic Image Synthesis},
  author = {Huang, Huaibo and Li, Zhihang and He, Ran and Sun, Zhenan and Tan, Tieniu},
  date = {2018},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {2018-Decem},
  pages = {52--63},
  issn = {10495258},
  abstract = {We present a novel introspective variational autoencoder (IntroVAE) model for synthesizing high-resolution photographic images. IntroVAE is capable of self-evaluating the quality of its generated samples and improving itself accordingly. Its inference and generator models are jointly trained in an introspective way. On one hand, the generator is required to reconstruct the input images from the noisy outputs of the inference model as normal VAEs. On the other hand, the inference model is encouraged to classify between the generated and real samples while the generator tries to fool it as GANs. These two famous generative frameworks are integrated in a simple yet efficient single-stream architecture that can be trained in a single stage. IntroVAE preserves the advantages of VAEs, such as stable training and nice latent manifold. Unlike most other hybrid models of VAEs and GANs, IntroVAE requires no extra discriminators, because the inference model itself serves as a discriminator to distinguish between the generated and real samples. Experiments demonstrate that our method produces high-resolution photo-realistic images (e.g., CELEBA images at 10242), which are comparable to or better than the state-of-the-art GANs.},
  archivePrefix = {arXiv},
  arxivid = {1807.06358},
  eprint = {1807.06358},
  eprinttype = {arxiv},
  issue = {Nips}
}

@online{Huang2018a,
  title = {Gpipe: {{Efficient}} Training of Giant Neural Networks Using Pipeline Parallelism},
  author = {Huang, Yanping and Cheng, Yonglong and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Chen, Zhifeng},
  date = {2018},
  archivePrefix = {arXiv},
  eprint = {1811.06965},
  eprinttype = {arxiv}
}

@inproceedings{Huang2019,
  title = {Automatic Detection of Translucency Using a Deep Learning Method from Patches of Clinical Basal Cell Carcinoma Images},
  booktitle = {2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, {{APSIPA ASC}} 2018 - Proceedings},
  author = {Huang, He and Kharazmi, Pegah and McLean, David I. and Lui, Harvey and Wang, Z. Jane and Lee, Tim K.},
  date = {2019-03},
  pages = {685--688},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.23919/APSIPA.2018.8659685},
  abstract = {Translucency, defined as a jelly-like appearance, is a common clinical feature of basal cell carcinoma, the most common skin cancer. The feature plays an important role in diagnosing basal cell carcinoma in an early stage because the feature can be observed readily in clinical examinations with a high specificity of 93\%. Therefore, translucency detection is a critical component of computer aided systems which aim at early detection of basal cell carcinoma. To address this problem, we proposed an automated method for analyzing patches of clinical basal cell carcinoma images using stacked sparse autoencoder (SSAE). SSAE learns high-level features in unsupervised manner and all learned features are fed into a softmax classifier for translucency detection. Across the 4401 patches generated from 32 clinical images, the proposed method achieved a 93\% detection accuracy from a five-fold cross-validation. The preliminary result suggested that the proposed method could detect translucency from skin images. © 2018 APSIPA organization.},
  isbn = {978-988-14768-5-2},
  keywords = {basal cell carcinoma,deep learning,stacked sparse autoencoder,Translucency}
}

@online{Huval2015,
  title = {An Empirical Evaluation of Deep Learning on Highway Driving},
  author = {Huval, Brody and Wang, Tao and Tandon, Sameep and Kiske, Jeff and Song, Will and Pazhayampallil, Joel and Andriluka, Mykhaylo and Rajpurkar, Pranav and Migimatsu, Toki and Cheng-Yue, Royce},
  date = {2015},
  archivePrefix = {arXiv},
  eprint = {1504.01716},
  eprinttype = {arxiv}
}

@online{Iglovikov2018,
  title = {{{TernausNet}}: {{U}}-Net with {{VGG11}} Encoder Pre-Trained on {{ImageNet}} for Image Segmentation},
  author = {Iglovikov, Vladimir and Shvets, Alexey},
  date = {2018-01},
  url = {http://arxiv.org/abs/1801.05746},
  abstract = {Pixel-wise image segmentation is demanding task in computer vision. Classical U-Net architectures composed of encoders and decoders are very popular for segmentation of medical images, satellite images etc. Typically, neural network initialized with weights from a network pre-trained on a large data set like ImageNet shows better performance than those trained from scratch on a small dataset. In some practical applications, particularly in medicine and traffic safety, the accuracy of the models is of utmost importance. In this paper, we demonstrate how the U-Net type architecture can be improved by the use of the pre-trained encoder. Our code and corresponding pre-trained weights are publicly available at https://github.com/ternaus/TernausNet. We compare three weight initialization schemes: LeCun uniform, the encoder with weights from VGG11 and full network trained on the Carvana dataset. This network architecture was a part of the winning solution (1st out of 735) in the Kaggle: Carvana Image Masking Challenge.},
  archivePrefix = {arXiv},
  arxivid = {1801.05746},
  eprint = {1801.05746},
  eprinttype = {arxiv}
}

@online{Ilyas2019,
  title = {Adversarial Examples Are Not Bugs, They Are Features},
  author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
  date = {2019},
  archivePrefix = {arXiv},
  eprint = {1905.02175},
  eprinttype = {arxiv}
}

@inproceedings{imagenet_cvpr09,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  booktitle = {{{CVPR09}}},
  author = {Deng, J and Dong, W and Socher, R and Li, L.-J. and Li, K and Fei-Fei, L},
  date = {2009}
}

@article{Indu2016,
  title = {"{{Slide}} Less Pathology": {{Fairy}} Tale or Reality?},
  author = {Indu, M and Rathy, R and Binu, M P},
  date = {2016},
  journaltitle = {Journal of oral and maxillofacial pathology : JOMFP},
  volume = {20},
  pages = {284--288},
  issn = {0973-029X (Print)},
  doi = {10.4103/0973-029X.185921},
  abstract = {Pathology practice is significantly advanced in various frontiers. Therefore, "slide less digital" pathology will not be a mere imagination in near future. Digitalization of histopathological slides (whole slide imaging [WSI]) is possible with the help of whole slide scanner. The WSI has a positive impact not only in routine practice but also in research field, medical education and bioindustry. Even if digital pathology has definitive advantages, its widespread use is not yet possible. As it is an upcoming technology in our field, this article is aimed to discussessential aspects of WSI.},
  eprint = {27601824},
  eprinttype = {pmid},
  langid = {english},
  number = {2}
}

@online{Ioffe2015,
  title = {Batch Normalization: {{Accelerating}} Deep Network Training by Reducing Internal Covariate Shift},
  author = {Ioffe, Sergey and Szegedy, Christian},
  date = {2015-02},
  url = {http://arxiv.org/abs/1502.03167},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archivePrefix = {arXiv},
  arxivid = {1502.03167},
  eprint = {1502.03167},
  eprinttype = {arxiv}
}

@article{Ioffe2015a,
  title = {Batch Normalization: {{Accelerating}} Deep Network Training by Reducing Internal Covariate Shift},
  author = {Ioffe, Sergey and Szegedy, Christian},
  date = {2015},
  journaltitle = {International Conference on Machine Learning},
  pages = {448--456},
  url = {http://arxiv.org/abs/1502.03167},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archivePrefix = {arXiv},
  arxivid = {1502.03167},
  eprint = {1502.03167},
  eprinttype = {arxiv}
}

@article{Janowczyk,
  title = {Deep Learning for Digital Pathology Image Analysis: {{A}} Comprehensive Tutorial with Selected Use Cases},
  author = {Janowczyk, Andrew and Madabhushi, Anant},
  volume = {7},
  number = {1}
}

@online{Javanmardi2016,
  title = {Unsupervised Total Variation Loss for Semi-Supervised Deep Learning of Semantic Segmentation},
  author = {Javanmardi, Mehran and Sajjadi, Mehdi and Liu, Ting and Tasdizen, Tolga},
  date = {2016},
  archivePrefix = {arXiv},
  eprint = {1605.01368},
  eprinttype = {arxiv}
}

@article{Jha2016,
  title = {Adapting to Artificial Intelligence: {{Radiologists}} and Pathologists as Information Specialists},
  author = {Jha, Saurabh and Topol, Eric J.},
  date = {2016},
  journaltitle = {JAMA - Journal of the American Medical Association},
  volume = {316},
  pages = {2353--2354},
  issn = {15383598},
  doi = {10.1001/jama.2016.17438},
  abstract = {Artificial intelligence—the mimicking of human cogni-tion by computers—was once a fable in science fiction but is becoming reality in medicine. The combination of big data and artificial intelligence, referred to by some as the fourth industrial revolution, 1 will change radiol-ogy and pathology along with other medical specialties. Although reports of radiologists and pathologists being replaced by computers seem exaggerated, 2 these spe-cialties must plan strategically for a future in which arti-ficial intelligence is part of the health care workforce. Radiologists have always revered machines and tech-nology. In 1960, Lusted predicted " an electronic scanner-computer to examine chest photofluorograms, to sepa-rate the clearly normal chest films from the abnormal chest films. " 3 Lusted further suggested that " the abnor-mal chest films would be marked for later study by the radiologists. " 3 Lusted's intuitions were prescient: inter-preting radiographs is pattern recognition; computers can recognize patterns and may be helpful because some roentgenographic analyses can be automated. Nearly 60 years after Lusted's prediction, Enlitic, a technology company in Silicon Valley, inputted im-ages of normal radiographs and radiographs with frac-tures into a computerized database. 4 Using deep learn-ing, a refined version of artificial neural networks, the},
  number = {22}
}

@inproceedings{Jia,
  title = {Caffe: {{Convolutional}} Architecture for Fast Feature Embedding},
  booktitle = {Proceedings of the 22nd {{ACM}} International Conference on {{Multimedia}}},
  author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  pages = {675--678},
  publisher = {{ACM}},
  isbn = {1-4503-3063-0}
}

@article{Jiang2017,
  title = {Artificial Intelligence in Healthcare: {{Past}}, Present and Future},
  author = {Jiang, Fei and Jiang, Yong and Zhi, Hui and Dong, Yi and Li, Hao and Ma, Sufeng and Wang, Yilong and Dong, Qiang and Shen, Haipeng and Wang, Yongjun},
  date = {2017},
  journaltitle = {Stroke and Vascular Neurology},
  volume = {2},
  pages = {230--243},
  issn = {20598696},
  doi = {10.1136/svn-2017-000101},
  abstract = {Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. We survey the current status of AI applications in healthcare and discuss its future. AI can be applied to various types of healthcare data (structured and unstructured). Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data. Major disease areas that use AI tools include cancer, neurology and cardiology. We then review in more details the AI applications in stroke, in the three major areas of early detection and diagnosis, treatment, as well as outcome prediction and prognosis evaluation. We conclude with discussion about pioneer AI systems, such as IBM Watson, and hurdles for real-life deployment of AI.},
  number = {4}
}

@article{Jiang2019,
  title = {Recognizing Basal Cell Carcinoma on {{Smartphone}}‐{{Captured}} Digital Histopathology Images with Deep Neural Network},
  author = {Jiang, Y Q and Xiong, J H and Li, H Y and Yang, X H and Yu, W T and Gao, M and Zhao, X and Ma, Y P and Zhang, W and Guan, Y F},
  date = {2019},
  journaltitle = {British Journal of Dermatology},
  publisher = {{Wiley Online Library}},
  issn = {0007-0963}
}

@article{Jiang2019a,
  title = {Recognizing Basal Cell Carcinoma on {{Smartphone}}‐{{Captured}} Digital Histopathology Images with Deep Neural Network},
  author = {Jiang, Y.Q. and Xiong, J.H. and Li, H.Y. and Yang, X.H. and Yu, W.T. and Gao, M. and Zhao, X. and Ma, Y.P. and Zhang, W. and Guan, Y.F. and Gu, H. and Sun, J.F.},
  date = {2019-04},
  journaltitle = {British Journal of Dermatology},
  publisher = {{Wiley}},
  issn = {0007-0963},
  doi = {10.1111/bjd.18026},
  abstract = {BACKGROUND Pioneering effort has been made to facilitate the pathology recognition in malignancies based on whole slide images(WSI) through deep learning approaches. It remains unclear whether we can accurately detect and locate BCC using smartphone-captured images. OBJECTIVES To develop deep neural network frameworks for accurate BCC recognition and segmentation based on smartphone-captured microscopic ocular images (MOI). METHODS We collected a total of 8046 MOIs, 6610 of which had binary classification labels and the other 1436 had pixel-wise annotations. Meanwhile, 128 WSIs were collected for comparison. Two deep learning frameworks were created. "Cascade" framework had a classification model for identifying hard cases (images with low prediction confidence), and a segmentation model for further in-depth analysis of the hard cases. "Segmentation" framework directly segmented and classified all images. Sensitivity, specificity, and AUC were used to evaluate the overall performance of BCC recognition. RESULTS The MOI- and WSI-based models achieved comparable AUCs around 0˙95. The "cascade" framework achieved 0˙93 sensitivity and 0˙91 specificity. The "segmentation" framework was more accurate but required more computational resources, achieving 0˙97 sensitivity, 0˙94 specificity, and 0˙987 AUC. The runtime of the "segmentation" framework was 15˙3 ± 3˙9second (s) per image, whereas the "cascade" framework was 4˙1 ± 1˙4s. Additionally the "segmentation" framework achieved 0˙863 mean intersection over union (mIoU). CONCLUSIONS Based on the accessible microscopic ocular images via smartphone photography, we developed two deep learning frameworks for recognizing BCC pathologically with high sensitivity and specificity. This work opens a new avenue for automatic BCC diagnosis in different clinic scenarios. This article is protected by copyright. All rights reserved.}
}

@inproceedings{Johnson2015,
  title = {Image Retrieval Using Scene Graphs},
  booktitle = {Proceedings of the {{IEEE}} Computer Society Conference on Computer Vision and Pattern Recognition},
  author = {Johnson, Justin and Krishna, Ranjay and Stark, Michael and Li, Li Jia and Shamma, David A and Bernstein, Michael S and Li, Fei Fei},
  date = {2015},
  volume = {07-12-June},
  pages = {3668--3678},
  issn = {10636919},
  doi = {10.1109/CVPR.2015.7298990},
  abstract = {Scene graph에 기반하여 semantic image를 검색하는 방법 소개(사진 속에 포함된 내용)},
  isbn = {978-1-4673-6964-0}
}

@article{Johnson2016,
  title = {Perceptual Losses for Real-Time Style Transfer and Super-Resolution},
  author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
  date = {2016},
  journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {9906 LNCS},
  pages = {694--711},
  issn = {16113349},
  doi = {10.1007/978-3-319-46475-6_43},
  abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
  archivePrefix = {arXiv},
  arxivid = {arXiv:1603.08155v1},
  eprint = {1603.08155v1},
  eprinttype = {arxiv},
  isbn = {9783319464749},
  keywords = {Deep learning,Style transfer,Super-resolution}
}

@inproceedings{Johnson2018,
  title = {Image Generation from Scene Graphs},
  booktitle = {Proceedings of the {{IEEE}} Computer Society Conference on Computer Vision and Pattern Recognition},
  author = {Johnson, Justin and Gupta, Agrim and Fei-Fei, Li},
  date = {2018-12},
  pages = {1219--1228},
  publisher = {{IEEE Computer Society}},
  issn = {10636919},
  doi = {10.1109/CVPR.2018.00133},
  abstract = {To truly understand the visual world our models should be able not only to recognize images but also generate them. To this end, there has been exciting recent progress on generating images from natural language descriptions. These methods give stunning results on limited domains such as descriptions of birds or flowers, but struggle to faithfully reproduce complex sentences with many objects and relationships. To overcome this limitation we propose a method for generating images from scene graphs, enabling explicitly reasoning about objects and their relationships. Our model uses graph convolution to process input graphs, computes a scene layout by predicting bounding boxes and segmentation masks for objects, and converts the layout to an image with a cascaded refinement network. The network is trained adversarially against a pair of discriminators to ensure realistic outputs. We validate our approach on Visual Genome and COCO-Stuff, where qualitative results, ablations, and user studies demonstrate our method's ability to generate complex images with multiple objects.},
  isbn = {978-1-5386-6420-9}
}

@article{Kahn1992,
  title = {Artificial Intelligence in Medicine Workshop},
  author = {Kahn, Michael G.},
  date = {1992},
  journaltitle = {Artificial Intelligence in Medicine},
  volume = {4},
  pages = {409--411},
  issn = {09333657},
  doi = {10.1016/0933-3657(92)90023-i},
  url = {https://groups.csail.mit.edu/medg/ftp/psz/AIM82/ch1.html},
  number = {5}
}

@online{Kaiser2017,
  title = {One Model to Learn Them All},
  author = {Kaiser, Lukasz and Gomez, Aidan N. and Shazeer, Noam and Vaswani, Ashish and Parmar, Niki and Jones, Llion and Uszkoreit, Jakob},
  date = {2017-06},
  url = {http://arxiv.org/abs/1706.05137},
  abstract = {Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.},
  archivePrefix = {arXiv},
  arxivid = {1706.05137},
  eprint = {1706.05137},
  eprinttype = {arxiv}
}

@article{Kamnitsas2017,
  title = {Efficient Multi-Scale {{3D CNN}} with Fully Connected {{CRF}} for Accurate Brain Lesion Segmentation},
  author = {Kamnitsas, Konstantinos and Ledig, Christian and Newcombe, Virginia F.J. and Simpson, Joanna P. and Kane, Andrew D. and Menon, David K. and Rueckert, Daniel and Glocker, Ben},
  date = {2017-02},
  journaltitle = {Medical Image Analysis},
  volume = {36},
  pages = {61--78},
  publisher = {{Elsevier B.V.}},
  issn = {13618423},
  doi = {10.1016/j.media.2016.10.004},
  abstract = {We propose a dual pathway, 11-layers deep, three-dimensional Convolutional Neural Network for the challenging task of brain lesion segmentation. The devised architecture is the result of an in-depth analysis of the limitations of current networks proposed for similar applications. To overcome the computational burden of processing 3D medical scans, we have devised an efficient and effective dense training scheme which joins the processing of adjacent image patches into one pass through the network while automatically adapting to the inherent class imbalance present in the data. Further, we analyze the development of deeper, thus more discriminative 3D CNNs. In order to incorporate both local and larger contextual information, we employ a dual pathway architecture that processes the input images at multiple scales simultaneously. For post-processing of the network's soft segmentation, we use a 3D fully connected Conditional Random Field which effectively removes false positives. Our pipeline is extensively evaluated on three challenging tasks of lesion segmentation in multi-channel MRI patient data with traumatic brain injuries, brain tumours, and ischemic stroke. We improve on the state-of-the-art for all three applications, with top ranking performance on the public benchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient, which allows its adoption in a variety of research and clinical settings. The source code of our implementation is made publicly available.},
  keywords = {3D convolutional neural network,Brain lesions,Deep learning,Fully connected CRF,Segmentation}
}

@online{Karras2017,
  title = {Progressive Growing of Gans for Improved Quality, Stability, and Variation},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  date = {2017},
  archivePrefix = {arXiv},
  eprint = {1710.10196},
  eprinttype = {arxiv}
}

@online{karras2017progressive,
  title = {Progressive Growing of Gans for Improved Quality, Stability, and Variation},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  date = {2017},
  archivePrefix = {arXiv},
  eprint = {1710.10196},
  eprinttype = {arxiv}
}

@inproceedings{Karras2019,
  title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Karras, Tero and Laine, Samuli and Aila, Timo},
  date = {2019},
  pages = {4401--4410}
}

@inproceedings{karras2019style,
  title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Karras, Tero and Laine, Samuli and Aila, Timo},
  date = {2019},
  pages = {4401--4410}
}

@inproceedings{Karras2020,
  title = {Analyzing and Improving the Image Quality of Stylegan},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  date = {2020},
  pages = {8110--8119}
}

@article{Kather2015-bg,
  title = {Continuous Representation of Tumor Microvessel Density and Detection of Angiogenic Hotspots in Histological Whole-Slide Images},
  author = {Kather, Jakob Nikolas and Marx, Alexander and Reyes-Aldasoro, Constantino Carlos and Schad, Lothar R and Zöllner, Frank Gerrit and Weis, Cleo-Aron},
  date = {2015-08},
  journaltitle = {Oncotarget},
  volume = {6},
  pages = {19163--19176},
  abstract = {Blood vessels in solid tumors are not randomly distributed, but are clustered in angiogenic hotspots. Tumor microvessel density (MVD) within these hotspots correlates with patient survival and is widely used both in diagnostic routine and in clinical trials. Still, these hotspots are usually subjectively defined. There is no unbiased, continuous and explicit representation of tumor vessel distribution in histological whole slide images. This shortcoming distorts angiogenesis measurements and may account for ambiguous results in the literature. In the present study, we describe and evaluate a new method that eliminates this bias and makes angiogenesis quantification more objective and more efficient. Our approach involves automatic slide scanning, automatic image analysis and spatial statistical analysis. By comparing a continuous MVD function of the actual sample to random point patterns, we introduce an objective criterion for hotspot detection: An angiogenic hotspot is defined as a clustering of blood vessels that is very unlikely to occur randomly. We evaluate the proposed method in N=11 images of human colorectal carcinoma samples and compare the results to a blinded human observer. For the first time, we demonstrate the existence of statistically significant hotspots in tumor images and provide a tool to accurately detect these hotspots.},
  keywords = {digital pathology,spatial statistics,tumor angio},
  number = {22}
}

@book{Kevin_Zhou2017-do,
  title = {Deep Learning for Medical Image Analysis},
  author = {Kevin Zhou, S and Greenspan, Hayit and Shen, Dinggang},
  date = {2017-01},
  publisher = {{Academic Press}},
  abstract = {Deep learning is providing exciting solutions for medical image analysis problems and is seen as a key method for future applications. This book gives a clear understanding of the principles and methods of neural network and deep learning concepts, showing how the algorithms that integrate deep learning as a core component have been applied to medical image detection, segmentation and registration, and computer-aided analysis, using a wide variety of application areas. Deep Learning for Medical Image Analysis is a great learning resource for academic and industry researchers in medical imaging analysis, and for graduate students taking courses on machine learning and deep learning for computer vision and medical image computing and analysis.Covers common research problems in medical image analysis and their challengesDescribes deep learning methods and the theories behind approaches for medical image analysisTeaches how algorithms are applied to a broad range of application areas, including Chest X-ray, breast CAD, lung and chest, microscopy and pathology, etc.Includes a Foreword written by Nicholas Ayache}
}

@book{Kevin_Zhou2017-pj,
  title = {Deep Learning for Medical Image Analysis},
  author = {Kevin Zhou, S and Greenspan, Hayit and Shen, Dinggang},
  date = {2017-01},
  publisher = {{Academic Press}},
  abstract = {Deep learning is providing exciting solutions for medical image analysis problems and is seen as a key method for future applications. This book gives a clear understanding of the principles and methods of neural network and deep learning concepts, showing how the algorithms that integrate deep learning as a core component have been applied to medical image detection, segmentation and registration, and computer-aided analysis, using a wide variety of application areas. Deep Learning for Medical Image Analysis is a great learning resource for academic and industry researchers in medical imaging analysis, and for graduate students taking courses on machine learning and deep learning for computer vision and medical image computing and analysis.Covers common research problems in medical image analysis and their challengesDescribes deep learning methods and the theories behind approaches for medical image analysisTeaches how algorithms are applied to a broad range of application areas, including Chest X-ray, breast CAD, lung and chest, microscopy and pathology, etc.Includes a Foreword written by Nicholas Ayache}
}

@article{Kharazmi2017-pj,
  title = {A Feature Fusion System for Basal Cell Carcinoma Detection through Data‐driven Feature Learning and Patient Profile},
  author = {Kharazmi, P and Kalia, S and Lui, H and Wang, Z J and {Others}},
  date = {2017},
  journaltitle = {Skin Research and Technology},
  shortjournal = {Skin Res. Technol.},
  publisher = {{Wiley Online Library}},
  abstract = {… is still relatively small for the proposed deep learning - based method … formulations resulting in limited applicability and extensive computa- tional complexity, we propose a data- driven feature learning framework based on sparse autoencoder to directly learn the hidden …}
}

@article{Kharazmi2018,
  title = {A Feature Fusion System for Basal Cell Carcinoma Detection through Data-Driven Feature Learning and Patient Profile},
  author = {Kharazmi, P. and Kalia, S. and Lui, H. and Wang, Z. J. and Lee, T. K.},
  date = {2018-05},
  journaltitle = {Skin Research and Technology},
  volume = {24},
  pages = {256--264},
  publisher = {{Blackwell Publishing Ltd}},
  issn = {16000846},
  doi = {10.1111/srt.12422},
  abstract = {BACKGROUND: Basal cell carcinoma (BCC) is the most common skin cancer, which is highly damaging in its advanced stages. Computer-aided techniques provide a feasible option for early detection of BCC. However, automated BCC detection techniques immensely rely on handcrafting high-level precise features. Such features are not only computationally complex to design but can also represent a very limited aspect of the lesion characteristics. This paper proposes an automated BCC detection technique that directly learns the features from image data, eliminating the need for handcrafted feature design. METHODS: The proposed method is composed of 2 parts. First, an unsupervised feature learning framework is proposed which attempts to learn hidden characteristics of the data including vascular patterns directly from the images. This is done through the design of a sparse autoencoder (SAE). After the unsupervised learning, we treat each of the learned kernel weights of the SAE as a filter. Convolving each filter with the lesion image yields a feature map. Feature maps are condensed to reduce the dimensionality and are further integrated with patient profile information. The overall features are then fed into a softmax classifier for BCC classification. RESULTS: On a set of 1199 BCC images, the proposed framework achieved an area under the curve of 91.1\%, while the visualization of learned features confirmed meaningful clinical interpretation of the features. CONCLUSION: The proposed framework provides a non-invasive fast BCC detection tool that incorporates both dermoscopic lesional features and clinical patient information, without the need for complex handcrafted feature extraction.},
  keywords = {automated basal cell carcinoma detection,blood vessels,dermoscopy,feature learning,sparse autoencoders},
  number = {2}
}

@article{Kim2016-na,
  title = {A Deep Semantic Mobile Application for Thyroid Cytopathology},
  author = {Kim, E and Corte-Real, M and Baloch, Z},
  date = {2016},
  journaltitle = {Proceedings of SPIE},
  shortjournal = {Proc. SPIE},
  publisher = {{proceedings.spiedigitallibrary.org}},
  abstract = {ABSTRACT Cytopathology is the study of disease at the cellular level and often used as a screening tool for cancer. Thyroid cytopathology is a branch of pathology that studies the diagnosis of thyroid lesions and diseases. A pathologist views cell images that may have high visual variance due to different anatomical structures and pathological characteristics. To assist the physician with identifying and searching through images, we propose a deep semantic mobile application. Our work augments recent advances in the digitization of}
}

@online{Kim2017,
  title = {Interpretability beyond Feature Attribution: {{Quantitative}} Testing with Concept Activation Vectors (Tcav)},
  author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
  date = {2017},
  archivePrefix = {arXiv},
  eprint = {1711.11279},
  eprinttype = {arxiv}
}

@inproceedings{Kim2018,
  title = {Interpretability beyond Feature Attribution: {{Quantitative}} Testing with Concept Activation Vectors ({{TCAV}})},
  booktitle = {International Conference on Machine Learning},
  author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda},
  date = {2018},
  pages = {2673--2682}
}

@online{Kingma2013,
  title = {Auto-Encoding Variational Bayes},
  author = {Kingma, Diederik P and Welling, Max},
  date = {2013},
  archivePrefix = {arXiv},
  eprint = {1312.6114},
  eprinttype = {arxiv}
}

@online{Kingma2014,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  author = {Kingma, Diederik and Ba, Jimmy},
  date = {2014},
  archivePrefix = {arXiv},
  eprint = {1412.6980},
  eprinttype = {arxiv}
}

@online{kingma2014adam,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  author = {Kingma, Diederik P and Ba, Jimmy},
  date = {2014},
  archivePrefix = {arXiv},
  eprint = {1412.6980},
  eprinttype = {arxiv}
}

@online{Kodali2017,
  title = {On Convergence and Stability of Gans},
  author = {Kodali, Naveen and Abernethy, Jacob and Hays, James and Kira, Zsolt},
  date = {2017},
  archivePrefix = {arXiv},
  eprint = {1705.07215},
  eprinttype = {arxiv}
}

@article{Kothari2013-jh,
  title = {Pathology Imaging Informatics for Quantitative Analysis of Whole-Slide Images},
  author = {Kothari, Sonal and Phan, John H and Stokes, Todd H and Wang, May D},
  date = {2013-11},
  journaltitle = {Journal of the American Medical Informatics Association},
  shortjournal = {J. Am. Med. Inform. Assoc.},
  volume = {20},
  pages = {1099--1108},
  abstract = {OBJECTIVES: With the objective of bringing clinical decision support systems to reality, this article reviews histopathological whole-slide imaging informatics methods, associated challenges, and future research opportunities. TARGET AUDIENCE: This review targets pathologists and informaticians who have a limited understanding of the key aspects of whole-slide image (WSI) analysis and/or a limited knowledge of state-of-the-art technologies and analysis methods. SCOPE: First, we discuss the importance of imaging informatics in pathology and highlight the challenges posed by histopathological WSI. Next, we provide a thorough review of current methods for: quality control of histopathological images; feature extraction that captures image properties at the pixel, object, and semantic levels; predictive modeling that utilizes image features for diagnostic or prognostic applications; and data and information visualization that explores WSI for de novo discovery. In addition, we highlight future research directions and discuss the impact of large public repositories of histopathological data, such as the Cancer Genome Atlas, on the field of pathology informatics. Following the review, we present a case study to illustrate a clinical decision support system that begins with quality control and ends with predictive modeling for several cancer endpoints. Currently, state-of-the-art software tools only provide limited image processing capabilities instead of complete data analysis for clinical decision-making. We aim to inspire researchers to conduct more research in pathology imaging informatics so that clinical decision support can become a reality.},
  keywords = {cancer prediction,computer-aided diagnosis,decis},
  number = {6}
}

@article{Kothari2014-je,
  title = {Removing Batch Effects from Histopathological Images for Enhanced Cancer Diagnosis},
  author = {Kothari, Sonal and Phan, John H and Stokes, Todd H and Osunkoya, Adeboye O and Young, Andrew N and Wang, May D},
  date = {2014-05},
  journaltitle = {IEEE J Biomed Health Inform},
  volume = {18},
  pages = {765--772},
  publisher = {{ieeexplore.ieee.org}},
  abstract = {Researchers have developed computer-aided decision support systems for translational medicine that aim to objectively and efficiently diagnose cancer using histopathological images. However, the performance of such systems is confounded by nonbiological experimental variations or “batch effects” that can commonly occur in histopathological data, especially when images are acquired using different imaging devices and patient samples. This is even more problematic in large-scale studies in which cross-laboratory sharing of large volumes of data is necessary. Batch effects can change quantitative morphological image features and decrease the prediction performance. Using four batches of renal tumor images, we compare one image-level and five feature-level batch effect removal methods. Principal component variation analysis shows that batch is a large source of variance in image features. Results show that feature-level normalization methods reduce batch-contributed variance to almost zero. Moreover, feature-level normalization, especially ComBatN, improves cross-batch and combined-batch prediction performance. Compared to no normalization, ComBatN improves performance in 83\% and 90\% of cross-batch and combined-batch prediction models, respectively.},
  number = {3}
}

@inproceedings{kovalev2019examining,
  title = {Examining the Capability of {{GANs}} to Replace Real Biomedical Images in Classification Models Training},
  booktitle = {International Conference on Pattern Recognition and Information Processing},
  author = {Kovalev, Vassili and Kazlouski, Siarhei},
  date = {2019},
  pages = {98--107},
  organization = {{Springer}}
}

@article{Krittanawong2017,
  title = {Artificial Intelligence in Precision Cardiovascular Medicine},
  author = {Krittanawong, Chayakrit and Zhang, Hong Ju and Wang, Zhen and Aydar, Mehmet and Kitai, Takeshi},
  date = {2017-05},
  journaltitle = {Journal of the american college of cardiology},
  volume = {69},
  pages = {2657--2664},
  publisher = {{Elsevier USA}},
  issn = {15583597},
  doi = {10.1016/j.jacc.2017.03.571},
  abstract = {Artificial intelligence (AI) is a field of computer science that aims to mimic human thought processes, learning capacity, and knowledge storage. AI techniques have been applied in cardiovascular medicine to explore novel genotypes and phenotypes in existing diseases, improve the quality of patient care, enable cost-effectiveness, and reduce readmission and mortality rates. Over the past decade, several machine-learning techniques have been used for cardiovascular disease diagnosis and prediction. Each problem requires some degree of understanding of the problem, in terms of cardiovascular medicine and statistics, to apply the optimal machine-learning algorithm. In the near future, AI will result in a paradigm shift toward precision cardiovascular medicine. The potential of AI in cardiovascular medicine is tremendous; however, ignorance of the challenges may overshadow its potential clinical impact. This paper gives a glimpse of AI's application in cardiovascular clinical care and discusses its potential role in facilitating precision cardiovascular medicine.},
  keywords = {big data,cognitive computing,deep learning,machine learning},
  number = {21}
}

@article{Krittanawong2018,
  title = {The Rise of Artificial Intelligence and the Uncertain Future for Physicians},
  author = {Krittanawong, C.},
  date = {2018-02},
  journaltitle = {European journal of internal medicine},
  volume = {48},
  pages = {e13--e14},
  publisher = {{Elsevier B.V.}},
  issn = {18790828},
  doi = {10.1016/j.ejim.2017.06.017},
  abstract = {Physicians in everyday clinical practice are under pressure to innovate faster than ever because of the rapid, exponential growth in healthcare data. “Big data” refers to extremely large data sets that cannot be analyzed or interpreted using traditional data processing methods. In fact, big data itself is meaningless, but processing it offers the promise of unlocking novel insights and accelerating breakthroughs in medicine—which in turn has the potential to transform current clinical practice. Physicians can analyze big data, but at present it requires a large amount of time and sophisticated analytic tools such as supercomputers. However, the rise of artificial intelligence (AI) in the era of big data could assist physicians in shortening processing times and improving the quality of patient care in clinical practice. This editorial provides a glimpse at the potential uses of AI technology in clinical practice and considers the possibility of AI replacing physicians, perhaps altogether. Physicians diagnose diseases based on personal medical histories, individual biomarkers, simple scores (e.g., CURB-65, MELD), and their physical examinations of individual patients. In contrast, AI can diagnose diseases based on a complex algorithm using hundreds of biomarkers, imaging results from millions of patients, aggregated published clinical research from PubMed, and thousands of physician's notes from electronic health records (EHRs). While AI could assist physicians in many ways, it is unlikely to replace physicians in the foreseeable future. Let us look at the emerging uses of AI in medicine.},
  keywords = {Artificial intelligence,Big data,Precision medicine}
}

@inproceedings{Krizhevsky2012,
  title = {Imagenet Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  date = {2012},
  pages = {1097--1105}
}

@online{KS2019,
  title = {{{SECNLP}}: {{A}} Survey of Embeddings in Clinical Natural Language Processing},
  author = {KS, Kalyan and Sangeetha, S},
  date = {2019},
  pages = {1--45},
  url = {http://arxiv.org/abs/1903.01039},
  abstract = {Traditional representations like Bag of words are high dimensional, sparse and ignore the order as well as syntactic and semantic information. Distributed vector representations or embeddings map variable length text to dense fixed length vectors as well as capture the prior knowledge which can transferred to downstream tasks. Even though embedding has become de facto standard for representations in deep learning based NLP tasks in both general and clinical domains, there is no survey paper which presents a detailed review of embeddings in Clinical Natural Language Processing. In this survey paper, we discuss various medical corpora and their characteristics, medical codes and present a brief overview as well as comparison of popular embeddings models. We classify clinical embeddings into nine types and discuss each embedding type in detail. We discuss various evaluation methods followed by possible solutions to various challenges in clinical embeddings. Finally, we conclude with some of the future directions which will advance the research in clinical embeddings.},
  archivePrefix = {arXiv},
  arxivid = {1903.01039},
  eprint = {1903.01039},
  eprinttype = {arxiv},
  keywords = {distributed representations,embeddings,medical,natural language processing,survey}
}

@article{Kuipers1987,
  title = {New Reasoning Methods for Artificial Intelligence in Medicine},
  author = {Kuipers, Benjamin},
  date = {1987},
  journaltitle = {International Journal of Man-Machine Studies},
  volume = {26},
  pages = {707--718},
  issn = {00207373},
  doi = {10.1016/S0020-7373(87)80062-7},
  url = {https://www.sciencedirect.com/science/article/pii/S0020737387800627},
  abstract = {The discovery and validation of knowledge representations for new types of reasoning is a vital step in artificial intelligence (AI) research. A clear example of this process arises in our recent study of expert physicians' knowledge of the physiological mechanisms of the body. First, we observed that the reliance on weighted associations between findings and hypotheses in first-generation medical expert systems made it impossible for them to express knowledge of disease mechanisms. Second, to obtain empirical constraints on the nature of this knowledge of mechanism in human experts, we collected and analysed verbatim transcripts of expert physicians solving selected clinical problems. This analysis led us to the key aspects of a qualitative representation for the structure and behavior of mechanisms. The third step required a computational study of the problem of inferring behavior from structure, and resulted in a completely specified and implemented knowledge representation and a qualitative simulation algorithm (QSIM). Within this representation, we built a structural description for the mechanism studied in the transcripts, and the simulation produced the same qualitative prediction made by the physicians. Finally, the system is validated in two ways. A mathematical analysis demonstrates the power and limitations of the representation and algorithm as a qualitative abstraction of differential equations. The medical content of the knowledge base is evaluated and refined using the standard knowledge-engineering methodology. We believe that this combination of cognitive, computational, mathematical, and domain knowledge constraints provides a useful paradigm for the development of new knowledge representations in artificial intelligence. © 1987, Academic Press Limited. All rights reserved.},
  number = {6}
}

@article{Kulikowski2019,
  title = {Beginnings of Artificial Intelligence in Medicine ({{AIM}}): {{Computational}} Artifice Assisting Scientific Inquiry and Clinical Art – with Reflections on Present {{AIM}} Challenges},
  author = {Kulikowski, Casimir},
  date = {2019-04},
  journaltitle = {Yearbook of Medical Informatics},
  publisher = {{Georg Thieme Verlag KG}},
  issn = {0943-4747},
  doi = {10.1055/s-0039-1677895},
  abstract = {Background: The rise of biomedical expert heuristic knowledge-based approaches for computational modeling and problem solving, for scientific inquiry and medical decision-making, and for consultation in the 1970's led to a major change in the paradigm that affected all of artificial intelligence (AI) research. Since then, AI has evolved, surviving several “winters”, as it has oscillated between relying on expensive and hard-to-validate knowledge-based approaches, and the alternative of using machine learning methods for inferring classification rules from labelled datasets. In the past couple of decades, we are seeing a gradual but progressive intertwining of the two.Objectives: To give an overview of early directions in AI in medicine and threads of some subsequent developments motivated by the very different goals of scientific inquiry for biomedical research, and for computational modeling of clinical reasoning and more general healthcare problem solving from the perspective of today's “AI-Deep Learning Boom”. To show how, from the beginning, AI was central to Biomedical and Health Informatics (BMHI), as a field investigating how to understand intelligent thinking in dealing professionally with the practice for healthcare, developing mathematical models, technology, and software tools to aid human experts in biomedicine, despite many previous bouts of “exuberant optimism” about the methodologies deployed.Methods: An overview and commentary on some of the early research and publications in AI in biomedicine, emphasizing the different approaches to the modeling of problems involved in clinical practice in contrast to those of biomedical science. A concluding reflection of a few current challenges and pitfalls of AI in some biomedical applications.Conclusion: While biomedical knowledge-based systems played a critical role in influencing AI in its early days, 50 years later they have taken a back seat behind “Deep Learning” which promises to discover knowledge structures for inference and prediction, both in science and for clinical decision-support. Early work on AI for medical consultation turned out to be more useful for explanation and teaching than for clinical practice, as had been originally intended. Today, despite the many reported successes of deep learning, fundamental scientific challenges arise in drawing on models of brain science, cognition, and language, if AI is to augment and complement rather than replace human judgment and expertise in biomedicine while also incorporating these advances for translational medicine. Understanding clinical phenotypes and how they relate to precision and personalization of care requires not only scientific inquiry, but also humanistic models of treatment that respond to patient and practitioner narrative exchanges, since it is the stories and insights of human experts which encourage what Norbert Weiner termed the ethical “human use of human beings”, so central to adherence to the Hippocratic Oath}
}

@article{Kumar2017-vp,
  title = {A Dataset and a Technique for Generalized Nuclear Segmentation for Computational Pathology},
  author = {Kumar, Neeraj and Verma, Ruchika and Sharma, Sanuj and Bhargava, Surabhi and Vahadane, Abhishek and Sethi, Amit},
  date = {2017-07},
  journaltitle = {IEEE Transactions on Medical Imaging},
  shortjournal = {IEEE Trans. Med. Imaging},
  volume = {36},
  pages = {1550--1560},
  abstract = {Nuclear segmentation in digital microscopic tissue images can enable extraction of high-quality features for nuclear morphometrics and other analysis in computational pathology. Conventional image processing techniques, such as Otsu thresholding and watershed segmentation, do not work effectively on challenging cases, such as chromatin-sparse and crowded nuclei. In contrast, machine learning-based segmentation can generalize across various nuclear appearances. However, training machine learning algorithms requires data sets of images, in which a vast number of nuclei have been annotated. Publicly accessible and annotated data sets, along with widely agreed upon metrics to compare techniques, have catalyzed tremendous innovation and progress on other image classification problems, particularly in object recognition. Inspired by their success, we introduce a large publicly accessible data set of hematoxylin and eosin (H\&E)-stained tissue images with more than 21000 painstakingly annotated nuclear boundaries, whose quality was validated by a medical doctor. Because our data set is taken from multiple hospitals and includes a diversity of nuclear appearances from several patients, disease states, and organs, techniques trained on it are likely to generalize well and work right out-of-the-box on other H\&E-stained images. We also propose a new metric to evaluate nuclear segmentation results that penalizes object- and pixel-level errors in a unified manner, unlike previous metrics that penalize only one type of error. We also propose a segmentation technique based on deep learning that lays a special emphasis on identifying the nuclear boundaries, including those between the touching or overlapping nuclei, and works well on a diverse set of test images.},
  number = {7}
}

@online{Kumar2017-yi,
  title = {A Comparative Study of \{\vphantom\}{{CNN}}\vphantom\{\}, \{\vphantom\}{{BoVW}}\vphantom\{\} and \{\vphantom\}{{LBP}}\vphantom\{\} for Classification of Histopathological Images},
  author = {Kumar, Meghana Dinesh and Babaie, Morteza and Zhu, Shujin and Kalra, Shivam and Tizhoosh, H R},
  date = {2017-09},
  abstract = {Despite the progress made in the field of medical imaging, it remains a large area of open research, especially due to the variety of imaging modalities and disease-specific characteristics. This paper is a comparative study describing the potential of using local binary patterns (LBP), deep features and the bag-of-visual words (BoVW) scheme for the classification of histopathological images. We introduce a new dataset, \textbackslash textbackslashemph\{KIMIA Path960\}, that contains 960 histopathology images belonging to 20 different classes (different tissue types). We make this dataset publicly available. The small size of the dataset and its inter- and intra-class variability makes it ideal for initial investigations when comparing image descriptors for search and classification in complex medical imaging cases like histopathology. We investigate deep features, LBP histograms and BoVW to classify the images via leave-one-out validation. The accuracy of image classification obtained using LBP was 90.62\% while the highest accuracy using deep features reached 94.72\%. The dictionary approach (BoVW) achieved 96.50\%. Deep solutions may be able to deliver higher accuracies but they need extensive training with a large number of (balanced) image datasets.},
  archivePrefix = {arXiv},
  arxivid = {cs.CV/1710.01249},
  eprint = {1710.01249},
  eprinttype = {arxiv},
  primaryClass = {cs.CV}
}

@article{L1986,
  title = {The Evaluation of Artificial Intelligence Systems in Medicine},
  author = {Miller, P L},
  date = {1986},
  journaltitle = {Computer Methods and Programs in Biomedicine},
  volume = {22},
  pages = {5},
  url = {https://www.sciencedirect.com/science/article/pii/0169260786900878}
}

@article{Larson,
  title = {Application of Surgical Safety Standards to Robotic Surgery: Five Principles of Ethics for Nonmaleficence},
  author = {Larson, JA and Johnson, MH and of the American, SB Bhayani - Journal and 2014, undefined},
  journaltitle = {journalacs.org},
  url = {https://www.journalacs.org/article/s1072-7515(13)01193-9/abstract},
  options = {useprefix=true}
}

@article{LeCun1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {LeCun, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
  date = {1998},
  journaltitle = {Proceedings of the IEEE},
  volume = {86},
  pages = {2278--2324},
  issn = {0018-9219},
  number = {11}
}

@article{LeCun2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date = {2015},
  journaltitle = {Nature},
  volume = {521},
  pages = {436--444},
  issn = {0028-0836},
  number = {7553}
}

@article{LeCunYann;Cortes1999,
  title = {The {{MNIST}} Dataset of Handwritten Digits},
  author = {LeCun Yann; Cortes, Corinna; Burges Christopher},
  date = {1999},
  url = {http://yann.lecun.com/exdb/mnist/}
}

@inproceedings{Lee2015,
  title = {Deep-Plant: {{Plant}} Identification with Convolutional Neural Networks},
  booktitle = {Proceedings - International Conference on Image Processing, {{ICIP}}},
  author = {Lee, Sue Han and Chan, Chee Seng and Wilkin, Paul and Remagnino, Paolo},
  date = {2015},
  volume = {2015-Decem},
  pages = {452--456},
  publisher = {{IEEE}},
  issn = {15224880},
  doi = {10.1109/ICIP.2015.7350839},
  abstract = {This paper studies convolutional neural networks (CNN) to learn unsupervised feature representations for 44 different plant species, collected at the Royal Botanic Gardens, Kew, England. To gain intuition on the chosen features from the CNN model (opposed to a 'black box' solution), a visualisation technique based on the deconvolutional networks (DN) is utilized. It is found that venations of different order have been chosen to uniquely represent each of the plant species. Experimental results using these CNN features with different classifiers show consistency and superiority compared to the state-of-the art solutions which rely on hand-crafted features.},
  isbn = {978-1-4799-8339-1},
  keywords = {deep learning,feature visualisation,plant classification}
}

@article{Lee2017,
  title = {Nuclear Shape and Architecture in Benign Fields Predict Biochemical Recurrence in Prostate Cancer Patients Following Radical Prostatectomy: {{Preliminary}} Findings.},
  author = {Lee, George and Veltri, Robert W and Zhu, Guangjing and Ali, Sahirzeeshan and Epstein, Jonathan I and Madabhushi, Anant},
  date = {2017-10},
  journaltitle = {European urology focus},
  volume = {3},
  pages = {457--466},
  issn = {2405-4569 (Electronic)},
  doi = {10.1016/j.euf.2016.05.009},
  abstract = {BACKGROUND: Gleason scoring represents the standard for diagnosis of prostate cancer (PCa) and assessment of prognosis following radical prostatectomy (RP), but it does not account for patterns in neighboring normal-appearing benign fields that may be predictive of disease recurrence. OBJECTIVE: To investigate (1) whether computer-extracted image features within tumor-adjacent benign regions on digital pathology images could predict recurrence in PCa patients after surgery and (2) whether a tumor plus adjacent benign signature (TABS) could better predict recurrence compared with Gleason score or features from benign or cancerous regions alone. DESIGN, SETTING, AND PARTICIPANTS: We studied 140 tissue microarray cores (0.6mm each) from 70 PCa patients following surgery between 2000 and 2004 with up to 14 yr of follow-up. Overall, 22 patients experienced recurrence (biochemical [prostate-specific antigen], local, or distant recurrence and cancer death) and 48 did not. INTERVENTION: RP was performed in all patients. OUTCOME MEASUREMENTS AND STATISTICAL ANALYSIS: The top 10 features identified as most predictive of recurrence within both the benign and cancerous regions were combined into a 10-feature signature (TABS). Computer-extracted nuclear shape and architectural features from cancerous regions, adjacent benign fields, and TABS were evaluated via random forest classification accuracy and Kaplan-Meier survival analysis. RESULTS AND LIMITATIONS: Tumor-adjacent benign field features were predictive of recurrence (area under the receiver operating characteristic curve [AUC]: 0.72). Tumor-field nuclear shape descriptors and benign-field local nuclear arrangement were the predominant features found for TABS (AUC: 0.77). Combining TABS with Gleason sum further improved identification of recurrence (AUC: 0.81). All experiments were performed using threefold cross-validation without independent test set validation. CONCLUSIONS: Computer-extracted nuclear features within cancerous and benign regions predict recurrence following RP. Furthermore, TABS was shown to provide added value to common predictors including Gleason sum and Kattan and Stephenson nomograms. PATIENT SUMMARY: Future studies may benefit from evaluation of benign regions proximal to the tumor on surgically excised prostate cancer tissue for assessing risk of disease recurrence.},
  eprint = {28753763},
  eprinttype = {pmid},
  keywords = {Aged,analysis,Biomarkers,Cell Nucleus,Computer-Assisted,Diagnosis,Humans,Local,Male,methods,Middle Aged,Neoplasm Grading,Neoplasm Recurrence,pathology,Predictive Value of Tests,Prognosis,Prostate-Specific Antigen,Prostatectomy,Prostatic Neoplasms,surgery,Tumor},
  langid = {english},
  number = {4-5}
}

@article{Lee2017-eg,
  title = {Deep Learning Is Effective for Classifying Normal versus Age-Related Macular Degeneration \{\vphantom\}{{OCT}}\vphantom\{\} Images},
  author = {Lee, Cecilia S and Baughman, Doug M and Lee, Aaron Y},
  date = {2017},
  journaltitle = {Ophthalmology Retina},
  volume = {1},
  pages = {322--327},
  number = {4}
}

@article{LeicaBiosystems2019,
  title = {Leica Biosystems Receives {{FDA}} 510(k) Clearance to Market a Digital Pathology System for Primary Diagnosis},
  author = {{Leica Biosystems}},
  date = {2019},
  url = {https://www.leicabiosystems.com/news-events/news-details/article/leica-biosystems-receives-fda-510k-clearance-to-market-a-digital-pathology-system-for-primary-diag/News/detail/},
  urldate = {2019-08-23}
}

@inproceedings{Levi2015-qj,
  title = {Age and Gender Classification Using Convolutional Neural Networks},
  booktitle = {Proceedings of the \{\vphantom\}{{IEEE}}\vphantom\{\} Conference on Computer Vision and Pattern Recognition Workshops},
  author = {Levi, Gil and Hassner, Tal},
  date = {2015},
  pages = {34--42},
  publisher = {{cv-foundation.org}},
  abstract = {Abstract Automatic age and gender classification has become relevant to an increasing amount of applications, particularly since the rise of social platforms and social media. Nevertheless, performance of existing methods on real-world images is still significantly lacking, especially when compared to the tremendous leaps in performance recently reported for the related task of face recognition. In this paper we show that by learning representations through the use of deep-convolutional neural networks (CNN), a significant}
}

@article{Levine2019,
  title = {Rise of the Machines: {{Advances}} in Deep Learning for Cancer Diagnosis},
  author = {Levine, Adrian B. and Schlosser, Colin and Grewal, Jasleen and Coope, Robin and Jones, Steve J.M. and Yip, Stephen},
  date = {2019-03},
  journaltitle = {Trends in cancer},
  volume = {5},
  pages = {157--169},
  publisher = {{Cell Press}},
  issn = {24058033},
  doi = {10.1016/j.trecan.2019.02.002},
  abstract = {Deep learning refers to a set of computer models that have recently been used to make unprecedented progress in the way computers extract information from images. These algorithms have been applied to tasks in numerous medical specialties, most extensively radiology and pathology, and in some cases have attained performance comparable to human experts. Furthermore, it is possible that deep learning could be used to extract data from medical images that would not be apparent by human analysis and could be used to inform on molecular status, prognosis, or treatment sensitivity. In this review, we outline the current developments and state-of-the-art in applying deep learning for cancer diagnosis, and discuss the challenges in adapting the technology for widespread clinical deployment.},
  keywords = {artificial intelligence,deep learning,digital pathology,machine learning,radiomics,whole slide imaging},
  number = {3}
}

@article{Levine2020,
  title = {Synthesis of Diagnostic Quality Cancer Pathology Images by Generative Adversarial Networks},
  author = {Levine, Adrian B. and Peng, Jason and Farnell, David and Nursey, Mitchell and Wang, Yiping and Naso, Julia R. and Ren, Hezhen and Farahani, Hossein and Chen, Colin and Chiu, Derek and Talhouk, Aline and Sheffield, Brandon and Riazy, Maziar and Ip, Philip P. and Parra‐Herran, Carlos and Mills, Anne and Singh, Naveena and Tessier‐Cloutier, Basile and Salisbury, Taylor and Lee, Jonathan and Salcudean, Tim and Jones, Steven J.M. and Huntsman, David G. and Gilks, C. Blake and Yip, Stephen and Bashashati, Ali},
  date = {2020},
  journaltitle = {The Journal of Pathology},
  pages = {path.5509},
  issn = {0022-3417},
  doi = {10.1002/path.5509},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/path.5509},
  abstract = {Deep learning-based computer vision methods have recently made remarkable breakthroughs in the analysis and classification of cancer pathology images. However, there has been relatively little investigation of the utility of deep neural networks to synthesize medical images. In this study, we evaluated the efficacy of generative adversarial networks (GANs) to synthesize high resolution pathology images of ten histological types of cancer, including five cancer types from The Cancer Genome Atlas (TCGA) and the five major histological subtypes of ovarian carcinoma. The quality of these images was assessed using a comprehensive survey of board-certified pathologists (n = 9) and pathology trainees (n = 6). Our results show that the real and synthetic images are classified by histotype with comparable accuracies, and the synthetic images are visually indistinguishable from real images. Furthermore, we trained deep convolutional neural networks (CNNs) to diagnose the different cancer types and determined that the synthetic images perform as well as additional real images when used to supplement a small training set. These findings have important applications in proficiency testing of medical practitioners and quality assurance in clinical laboratories. Furthermore, training of computer-aided diagnostic systems can benefit from synthetic images where labeled datasets are limited (e.g., rare cancers). We have created a publicly available website where clinicians and researchers can attempt questions from the image survey at http://gan.aimlab.ca/. This article is protected by copyright. All rights reserved.}
}

@online{Li2016,
  title = {Revisiting Batch Normalization for Practical Domain Adaptation},
  author = {Li, Yanghao and Wang, Naiyan and Shi, Jianping and Liu, Jiaying and Hou, Xiaodi},
  date = {2016-03},
  url = {http://arxiv.org/abs/1603.04779},
  abstract = {Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study (Tommasi et al. 2015) shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.},
  archivePrefix = {arXiv},
  arxivid = {1603.04779},
  eprint = {1603.04779},
  eprinttype = {arxiv}
}

@article{Li2017,
  title = {A Multi-Scale u-Net for Semantic Segmentation of Histological Images from Radical Prostatectomies.},
  author = {Li, Jiayun and Sarma, Karthik V and Chung Ho, King and Gertych, Arkadiusz and Knudsen, Beatrice S and Arnold, Corey W},
  date = {2017},
  journaltitle = {AMIA ... Annual Symposium proceedings. AMIA Symposium},
  volume = {2017},
  pages = {1140--1148},
  issn = {1942-597X},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/29854182 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5977596},
  abstract = {Gleason grading of histological images is important in risk assessment and treatment planning for prostate cancer patients. Much research has been done in classifying small homogeneous cancer regions within histological images. However, semi-supervised methods published to date depend on pre-selected regions and cannot be easily extended to an image of heterogeneous tissue composition. In this paper, we propose a multi-scale U-Net model to classify images at the pixel-level using 224 histological image tiles from radical prostatectomies of 20 patients. Our model was evaluated by a patient-based 10-fold cross validation, and achieved a mean Jaccard index of 65.8\% across 4 classes (stroma, Gleason 3, Gleason 4 and benign glands), and 75.5\% for 3 classes (stroma, benign glands, prostate cancer), outperforming other methods.},
  eprint = {29854182},
  eprinttype = {pmid}
}

@online{Li2018,
  title = {{{DeepConsensus}}: Using the Consensus of Features from Multiple Layers to Attain Robust Image Classification},
  author = {Li, Yuchen and Hossain, Safwan and Jamali, Kiarash and Rudzicz, Frank},
  date = {2018-11},
  url = {http://arxiv.org/abs/1811.07266},
  abstract = {We consider a classifier whose test set is exposed to various perturbations that are not present in the training set. These test samples still contain enough features to map them to the same class as their unperturbed counterpart. Current architectures exhibit rapid degradation of accuracy when trained on standard datasets but then used to classify perturbed samples of that data. To address this, we present a novel architecture named DeepConsensus that significantly improves generalization to these test-time perturbations. Our key insight is that deep neural networks should directly consider summaries of low and high level features when making classifications. Existing convolutional neural networks can be augmented with DeepConsensus, leading to improved resistance against large and small perturbations on MNIST, EMNIST, FashionMNIST, CIFAR10 and SVHN datasets.},
  archivePrefix = {arXiv},
  arxivid = {1811.07266},
  eprint = {1811.07266},
  eprinttype = {arxiv}
}

@article{Liang2019,
  title = {Evaluation and Accurate Diagnoses of Pediatric Diseases Using Artificial Intelligence},
  author = {Liang, Huiying and Tsui, Brian Y. and Ni, Hao and Valentim, Carolina C.S. and Baxter, Sally L. and Liu, Guangjian and Cai, Wenjia and Kermany, Daniel S. and Sun, Xin and Chen, Jiancong and He, Liya and Zhu, Jie and Tian, Pin and Shao, Hua and Zheng, Lianghong and Hou, Rui and Hewett, Sierra and Li, Gen and Liang, Ping and Zang, Xuan and Zhang, Zhiqi and Pan, Liyan and Cai, Huimin and Ling, Rujuan and Li, Shuhua and Cui, Yongwang and Tang, Shusheng and Ye, Hong and Huang, Xiaoyan and He, Waner and Liang, Wenqing and Zhang, Qing and Jiang, Jianmin and Yu, Wei and Gao, Jianqun and Ou, Wanxing and Deng, Yingmin and Hou, Qiaozhen and Wang, Bei and Yao, Cuichan and Liang, Yan and Zhang, Shu and Duan, Yaou and Zhang, Runze and Gibson, Sarah and Zhang, Charlotte L. and Li, Oulan and Zhang, Edward D. and Karin, Gabriel and Nguyen, Nathan and Wu, Xiaokang and Wen, Cindy and Xu, Jie and Xu, Wenqin and Wang, Bochu and Wang, Winston and Li, Jing and Pizzato, Bianca and Bao, Caroline and Xiang, Daoman and He, Wanting and He, Suiqin and Zhou, Yugui and Haw, Weldon and Goldbaum, Michael and Tremoulet, Adriana and Hsu, Chun Nan and Carter, Hannah and Zhu, Long and Zhang, Kang and Xia, Huimin},
  date = {2019},
  journaltitle = {Nature Medicine},
  volume = {25},
  pages = {433--438},
  issn = {1546170X},
  doi = {10.1038/s41591-018-0335-9},
  abstract = {Artificial intelligence (AI)-based methods have emerged as powerful tools to transform medical care. Although machine learning classifiers (MLCs) have already demonstrated strong performance in image-based diagnoses, analysis of diverse and massive electronic health record (EHR) data remains challenging. Here, we show that MLCs can query EHRs in a manner similar to the hypothetico-deductive reasoning used by physicians and unearth associations that previous statistical methods have not found. Our model applies an automated natural language processing system using deep learning techniques to extract clinically relevant information from EHRs. In total, 101.6 million data points from 1,362,559 pediatric patient visits presenting to a major referral center were analyzed to train and validate the framework. Our model demonstrates high diagnostic accuracy across multiple organ systems and is comparable to experienced pediatricians in diagnosing common childhood diseases. Our study provides a proof of concept for implementing an AI-based system as a means to aid physicians in tackling large amounts of data, augmenting diagnostic evaluations, and to provide clinical decision support in cases of diagnostic uncertainty or complexity. Although this impact may be most evident in areas where healthcare providers are in relative shortage, the benefits of such an AI system are likely to be universal.},
  number = {3}
}

@article{Liew2018,
  title = {The Future of Radiology Augmented with {{Artificial Intelligence}}: {{A}} Strategy for Success},
  author = {Liew, Charlene},
  date = {2018},
  journaltitle = {European Journal of Radiology},
  volume = {102},
  pages = {152--156},
  issn = {18727727},
  doi = {10.1016/j.ejrad.2018.03.019},
  abstract = {The rapid development of Artificial Intelligence/deep learning technology and its implementation into routine clinical imaging will cause a major transformation to the practice of radiology. Strategic positioning will ensure the successful transition of radiologists into their new roles as augmented clinicians. This paper describes an overall vision on how to achieve a smooth transition through the practice of augmented radiology where radiologists-in-the-loop ensure the safe implementation of Artificial Intelligence systems.},
  issue = {December 2017},
  keywords = {Artificial Intelligence,Business,Deep learning,Health policy,Informatics,Strategy}
}

@inproceedings{Lin2014,
  title = {Microsoft Coco: {{Common}} Objects in Context},
  booktitle = {European Conference on Computer Vision},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C Lawrence},
  date = {2014},
  pages = {740--755},
  publisher = {{Springer}}
}

@inproceedings{Lin2017,
  title = {Focal Loss for Dense Object Detection},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
  date = {2017},
  pages = {2980--2988}
}

@inproceedings{Lin2017a,
  title = {{{RefineNet}}: {{Multi}}-Path Refinement Networks for High-Resolution Semantic Segmentation},
  booktitle = {Proceedings - 30th {{IEEE}} Conference on Computer Vision and Pattern Recognition, {{CVPR}} 2017},
  author = {Lin, Guosheng and Milan, Anton and Shen, Chunhua and Reid, Ian},
  date = {2017-11},
  volume = {2017-Janua},
  pages = {5168--5177},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/CVPR.2017.549},
  abstract = {Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.},
  isbn = {978-1-5386-0457-1}
}

@article{Linder2012-fg,
  title = {Identification of Tumor Epithelium and Stroma in Tissue Microarrays Using Texture Analysis},
  author = {Linder, Nina and Konsti, Juho and Turkki, Riku and Rahtu, Esa and Lundin, Mikael and Nordling, Stig and Haglund, Caj and Ahonen, Timo and Pietikäinen, Matti and Lundin, Johan},
  date = {2012-03},
  journaltitle = {Diagnostic Pathology},
  shortjournal = {Diagn. Pathol.},
  volume = {7},
  pages = {22},
  abstract = {BACKGROUND: The aim of the study was to assess whether texture analysis is feasible for automated identification of epithelium and stroma in digitized tumor tissue microarrays (TMAs). Texture analysis based on local binary patterns (LBP) has previously been used successfully in applications such as face recognition and industrial machine vision. TMAs with tissue samples from 643 patients with colorectal cancer were digitized using a whole slide scanner and areas representing epithelium and stroma were annotated in the images. Well-defined images of epithelium (n = 41) and stroma (n = 39) were used for training a support vector machine (SVM) classifier with LBP texture features and a contrast measure C (LBP/C) as input. We optimized the classifier on a validation set (n = 576) and then assessed its performance on an independent test set of images (n = 720). Finally, the performance of the LBP/C classifier was evaluated against classifiers based on Haralick texture features and Gabor filtered images. RESULTS: The proposed approach using LPB/C texture features was able to correctly differentiate epithelium from stroma according to texture: the agreement between the classifier and the human observer was 97 per cent (kappa value = 0.934, P {$<$} 0.0001) and the accuracy (area under the ROC curve) of the LBP/C classifier was 0.995 (CI95\% 0.991-0.998). The accuracy of the corresponding classifiers based on Haralick features and Gabor-filter images were 0.976 and 0.981 respectively. CONCLUSIONS: The method illustrates the capability of automated segmentation of epithelial and stromal tissue in TMAs based on texture features and an SVM classifier. Applications include tissue specific assessment of gene and protein expression, as well as computerized analysis of the tumor microenvironment. VIRTUAL SLIDES: The virtual slide(s) for this article can be found here: http://www.diagnosticpathology.diagnomx.eu/vs/4123422336534537.}
}

@article{Litjens2016,
  title = {Deep Learning as a Tool for Increased Accuracy and Efficiency of Histopathological Diagnosis},
  author = {Litjens, G and Sanchez, C I and Timofeeva, N and Hermsen, M and Nagtegaal, I and Kovacs, I and de Kaa, C and Bult, P and van Ginneken, B and van der Laak, J},
  date = {2016},
  journaltitle = {Sci Rep},
  volume = {6},
  pages = {26286},
  issn = {2045-2322},
  doi = {10.1038/srep26286},
  options = {useprefix=true}
}

@article{Litjens2016a,
  title = {Deep Learning as a Tool for Increased Accuracy and Efficiency of Histopathological Diagnosis},
  author = {Litjens, G and Sanchez, C I and Timofeeva, N and Hermsen, M and Nagtegaal, I and Kovacs, I and de Kaa, C and Bult, P and van Ginneken, B and van der Laak, J},
  date = {2016},
  journaltitle = {Scientific Reports},
  volume = {6},
  issn = {2045-2322},
  doi = {10.1038/srep26286},
  url = {\{%\}3CGo to},
  options = {useprefix=true}
}

@article{Litjens2017,
  title = {A Survey on Deep Learning in Medical Image Analysis},
  author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen A.W.M. and van Ginneken, Bram and Sánchez, Clara I.},
  date = {2017-12},
  journaltitle = {Medical image analysis},
  volume = {42},
  pages = {60--88},
  publisher = {{Elsevier B.V.}},
  issn = {13618423},
  doi = {10.1016/j.media.2017.07.005},
  abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.},
  keywords = {Convolutional neural networks,Deep learning,Medical imaging,Survey},
  options = {useprefix=true}
}

@article{Litjens2017a,
  title = {A Survey on Deep Learning in Medical Image Analysis},
  author = {Litjens, G and Kooi, T and Bejnordi, B E and Setio, A A A and Ciompi, F and Ghafoorian, M and van der Laak, Jawm and van Ginneken, B and Sanchez, C I},
  date = {2017},
  journaltitle = {Medical Image Analysis},
  volume = {42},
  pages = {60--88},
  issn = {1361-8415},
  doi = {10.1016/j.media.2017.07.005},
  url = {\{%\}3CGo to},
  options = {useprefix=true}
}

@article{Litjens2018,
  title = {1399 {{H}}\&{{E}}-Stained Sentinel Lymph Node Sections of Breast Cancer Patients: The {{CAMELYON}} Dataset},
  author = {Litjens, Geert and Bandi, Peter and Ehteshami Bejnordi, Babak and Geessink, Oscar and Balkenhol, Maschenka and Bult, Peter and Halilovic, Altuna and Hermsen, Meyke and van de Loo, Rob and Vogels, Rob},
  date = {2018},
  journaltitle = {GigaScience},
  volume = {7},
  pages = {giy065},
  publisher = {{Oxford University Press}},
  issn = {2047-217X},
  number = {6},
  options = {useprefix=true}
}

@online{Liu2017-uy,
  title = {Detecting Cancer Metastases on Gigapixel Pathology Images},
  author = {Liu, Yun and Gadepalli, Krishna and Norouzi, Mohammad and Dahl, George E and Kohlberger, Timo and Boyko, Aleksey and Venugopalan, Subhashini and Timofeev, Aleksei and Nelson, Philip Q and Corrado, Greg S and Hipp, Jason D and Peng, Lily and Stumpe, Martin C},
  date = {2017-03},
  abstract = {Each year, the treatment decisions for more than 230,000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x 100,000 pixels. Our method leverages a convolutional neural network (CNN) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4\% of the tumors, relative to 82.7\% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2\% sensitivity. We achieve image-level AUC scores above 97\% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection.},
  archivePrefix = {arXiv},
  arxivid = {cs.CV/1703.02442},
  eprint = {1703.02442},
  eprinttype = {arxiv},
  primaryClass = {cs.CV}
}

@article{Liu2018,
  title = {Large-Scale Celebfaces Attributes (Celeba) Dataset},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  date = {2018},
  journaltitle = {Retrieved August},
  volume = {15},
  pages = {2018}
}

@article{Liu2018a,
  title = {Artificial {{Intelligence}}–{{Based}} Breast Cancer Nodal Metastasis Detection: {{Insights}} into the Black Box for Pathologists},
  author = {Liu, Yun and Kohlberger, Timo and Norouzi, Mohammad and Dahl, George E and Smith, Jenny L and Mohtashamian, Arash and Olson, Niels and Peng, Lily H and Hipp, Jason D and Stumpe, Martin C},
  date = {2018},
  journaltitle = {Archives of pathology \& laboratory medicine},
  publisher = {{the College of American Pathologists}},
  issn = {1543-2165}
}

@article{Liu2019,
  title = {A Comparison of Deep Learning Performance against Health-Care Professionals in Detecting Diseases from Medical Imaging: A Systematic Review and Meta-Analysis},
  author = {Liu, Xiaoxuan and Faes, Livia and Kale, Aditya U and Wagner, Siegfried K and Fu, Dun Jack and Bruynseels, Alice and Mahendiran, Thushika and Moraes, Gabriella and Shamdas, Mohith and Kern, Christoph},
  date = {2019},
  journaltitle = {The Lancet Digital Health},
  publisher = {{Elsevier}},
  issn = {2589-7500}
}

@article{Long2015,
  title = {Fully Convolutional Networks for Semantic Segmentation},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  date = {2015},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {39},
  pages = {640--651},
  issn = {01628828},
  doi = {10.1109/TPAMI.2016.2572683},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
  archivePrefix = {arXiv},
  arxivid = {1411.4038v2},
  eprint = {16190471},
  eprinttype = {pmid},
  isbn = {9781467369640},
  keywords = {Convolutional Networks,Deep Learning,Semantic Segmentation,Transfer Learning},
  number = {4}
}

@inproceedings{Long2015-uf,
  title = {Fully Convolutional Networks for Semantic Segmentation},
  booktitle = {Proceedings of the \{\vphantom\}{{IEEE}}\vphantom\{\} Conference on Computer Vision and Pattern Recognition},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  date = {2015},
  pages = {3431--3440}
}

@article{Lu2015-at,
  title = {Automated Analysis and Diagnosis of Skin Melanoma on Whole Slide Histopathological Images},
  author = {Lu, Cheng and Mandal, Mrinal},
  date = {2015-08},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognit.},
  volume = {48},
  pages = {2738--2750},
  abstract = {Melanoma is the most aggressive type of skin cancer, and the pathological examination remains the gold standard for the final diagnosis. Traditionally, the histopathology slides are examined under a microscope by pathologists which typically leads to inter- and intra-observer variations. In addition, it is time consuming and tedious to analyze a whole glass slide manually. In this paper, we propose an efficient technique for automated analysis and diagnosis of the skin whole slide image. The proposed technique consists of five modules: epidermis segmentation, keratinocytes segmentation, melanocytes detection, feature construction and classification. Since the epidermis, keratinocytes and melanocytes are important cues for the pathologists, these regions are first segmented. Based on the segmented regions of interest, the spatial distribution and morphological features are constructed. These features, representing a skin tissue, are classified by a multi-class support vector machine classifier. Experimental results show that the proposed technique is able to provide a satisfactory performance (with about 90\% classification accuracy) and is able to assist the pathologist for the skin tissue analysis and diagnosis.},
  keywords = {Histopathological image analysis,Object detection},
  number = {8}
}

@article{Lu2016,
  title = {Multi-Pass Adaptive Voting for Nuclei Detection in Histopathological Images},
  author = {Lu, Cheng and Xu, Hongming and Xu, Jun and Gilmore, Hannah and Mandal, Mrinal and Madabhushi, Anant},
  date = {2016},
  journaltitle = {Scientific Reports},
  volume = {6},
  pages = {1--18},
  publisher = {{Nature Publishing Group}},
  issn = {20452322},
  doi = {10.1038/srep33985},
  url = {http://dx.doi.org/10.1038/srep33985},
  abstract = {Nuclei detection is often a critical initial step in the development of computer aided diagnosis and prognosis schemes in the context of digital pathology images. While over the last few years, a number of nuclei detection methods have been proposed, most of these approaches make idealistic assumptions about the staining quality of the tissue. In this paper, we present a new Multi-Pass Adaptive Voting (MPAV) for nuclei detection which is specifically geared towards images with poor quality staining and noise on account of tissue preparation artifacts. The MPAV utilizes the symmetric property of nuclear boundary and adaptively selects gradient from edge fragments to perform voting for a potential nucleus location. The MPAV was evaluated in three cohorts with different staining methods: Hematoxylin \&Eosin, CD31 \&Hematoxylin, and Ki-67 and where most of the nuclei were unevenly and imprecisely stained. Across a total of 47 images and nearly 17,700 manually labeled nuclei serving as the ground truth, MPAV was able to achieve a superior performance, with an area under the precision-recall curve (AUC) of 0.73. Additionally, MPAV also outperformed three state-of-the-art nuclei detection methods, a single pass voting method, a multi-pass voting method, and a deep learning based method.},
  issue = {February}
}

@book{Lu2017-kj,
  title = {Deep Learning and Convolutional Neural Networks for Medical Image Computing: {{Precision}} Medicine, High Performance and \{\vphantom\}{{Large}}-{{Scale}}\vphantom\{\} Datasets},
  author = {Lu, L and Zheng, Y and Carneiro, G and Yang, L},
  editor = {Lu, Le and Zheng, Yefeng and Carneiro, Gustavo and Yang, Lin},
  date = {2017},
  publisher = {{Springer}},
  abstract = {This book was partially motivated by the recent rapid progress on deep convolutional and recurrent neural network models and the abundance of important applications in computer vision, where quantitative performance has significantly improved in object recognition, detection, and automatic image caption. However publicly available image database with generally well-annotated image or object labels (but with some labeling noise), such as ImageNet (1.2 million images), PASCAL Visual Object Classes (VOC) Dataset and},
  series = {Advances in Computer Vision and Pattern Recognition}
}

@article{Lu2018,
  title = {Nuclear Shape and Orientation Features from {{H}}\&{{E}} Images Predict Survival in Early-Stage Estrogen Receptor-Positive Breast Cancers.},
  author = {Lu, Cheng and Romo-Bucheli, David and Wang, Xiangxue and Janowczyk, Andrew and Ganesan, Shridar and Gilmore, Hannah and Rimm, David and Madabhushi, Anant},
  date = {2018-11},
  journaltitle = {Laboratory investigation; a journal of technical methods and pathology},
  volume = {98},
  pages = {1438--1448},
  issn = {1530-0307 (Electronic)},
  doi = {10.1038/s41374-018-0095-7},
  abstract = {Early-stage estrogen receptor-positive (ER+) breast cancer (BCa) is the most common type of BCa in the United States. One critical question with these tumors is identifying which patients will receive added benefit from adjuvant chemotherapy. Nuclear pleomorphism (variance in nuclear shape and morphology) is an important constituent of breast grading schemes, and in ER+ cases, the grade is highly correlated with disease outcome. This study aimed to investigate whether quantitative computer-extracted image features of nuclear shape and orientation on digitized images of hematoxylin-stained and eosin-stained tissue of lymph node-negative (LN-), ER+ BCa could help stratify patients into discrete ({$<$}10 years short-term vs. {$>$}10 years long-term survival) outcome groups independent of standard clinical and pathological parameters. We considered a tissue microarray (TMA) cohort of 276 ER+, LN- patients comprising 150 patients with long-term and 126 patients with short-term overall survival, wherein 177 randomly chosen cases formed the modeling set, and 99 remaining cases the test set. Segmentation of individual nuclei was performed using multiresolution watershed; subsequently, 615 features relating to nuclear shape/texture and orientation disorder were extracted from each TMA spot. The Wilcoxon's rank-sum test identified the 15 most prognostic quantitative histomorphometric features within the modeling set. These features were then subsequently combined via a linear discriminant analysis classifier and evaluated on the test set to assign a probability of long-term vs. short-term disease-specific survival. In univariate survival analysis, patients identified by the image classifier as high risk had significantly poorer survival outcome: hazard ratio (95\% confident interval) = 2.91(1.23-6.92), p = 0.02786. Multivariate analysis controlling for T-stage, histology grade, and nuclear grade showed the classifier to be independently predictive of poorer survival: hazard ratio (95\% confident interval) = 3.17(0.33-30.46), p = 0.01039. Our results suggest that quantitative histomorphometric features of nuclear shape and orientation are strongly and independently predictive of patient survival in ER+, LN- BCa.},
  eprint = {29959421},
  eprinttype = {pmid},
  keywords = {Adult,Aged,Breast,Breast Neoplasms,Carcinoma,Cell Nucleus Shape,Connecticut,Ductal,Eosine Yellowish-(YS),epidemiology,Female,Hematoxylin,Humans,Machine Learning,Middle Aged,mortality,pathology,Retrospective Studies},
  langid = {english},
  number = {11}
}

@online{lucieriExplainingAIbasedDecision2020,
  title = {Explaining {{AI}}-Based {{Decision Support Systems}} Using {{Concept Localization Maps}}},
  author = {Lucieri, Adriano and Bajwa, Muhammad Naseer and Dengel, Andreas and Ahmed, Sheraz},
  date = {2020},
  archivePrefix = {arXiv},
  eprint = {2005.01399},
  eprinttype = {arxiv}
}

@online{lucieriExplainingAIbasedDecision2020a,
  title = {Explaining {{AI}}-Based {{Decision Support Systems}} Using {{Concept Localization Maps}}},
  author = {Lucieri, Adriano and Bajwa, Muhammad Naseer and Dengel, Andreas and Ahmed, Sheraz},
  date = {2020},
  archivePrefix = {arXiv},
  eprint = {2005.01399},
  eprinttype = {arxiv}
}

@article{Lundberg2019,
  title = {How Many Pathologists Does the United States Need?},
  author = {Lundberg, George D},
  date = {2019},
  journaltitle = {JAMA network open},
  volume = {2},
  pages = {e194308--e194308},
  publisher = {{American Medical Association}},
  number = {5}
}

@article{Lundervold2019,
  title = {An Overview of Deep Learning in Medical Imaging Focusing on {{MRI}}},
  author = {Lundervold, Alexander Selvikvåg and Lundervold, Arvid},
  date = {2019-05},
  journaltitle = {Zeitschrift fur medizinische physik},
  volume = {29},
  pages = {102--127},
  publisher = {{Elsevier GmbH}},
  issn = {18764436},
  doi = {10.1016/j.zemedi.2018.11.002},
  abstract = {What has happened in machine learning lately, and what does it mean for the future of medical image analysis? Machine learning has witnessed a tremendous amount of attention over the last few years. The current boom started around 2009 when so-called deep artificial neural networks began outperforming other established models on a number of important benchmarks. Deep neural networks are now the state-of-the-art machine learning models across a variety of areas, from image analysis to natural language processing, and widely deployed in academia and industry. These developments have a huge potential for medical imaging technology, medical data analysis, medical diagnostics and healthcare in general, slowly being realized. We provide a short overview of recent advances and some associated challenges in machine learning applied to medical image processing and image analysis. As this has become a very broad and fast expanding field we will not survey the entire landscape of applications, but put particular focus on deep learning in MRI. Our aim is threefold: (i)give a brief introduction to deep learning with pointers to core references; (ii)indicate how deep learning has been applied to the entire MRI processing chain, from acquisition to image retrieval, from segmentation to disease prediction; (iii)provide a starting point for people interested in experimenting and perhaps contributing to the field of deep learning for medical imaging by pointing out good educational resources, state-of-the-art open-source code, and interesting sources of data and problems related medical imaging.},
  keywords = {Deep learning,Machine learning,Medical imaging,MRI},
  number = {2}
}

@online{Lutz2018,
  title = {{{AlphaGAN}}: {{Generative}} Adversarial Networks for Natural Image Matting},
  author = {Lutz, Sebastian and Amplianitis, Konstantinos and Smolic, Aljosa},
  date = {2018-07},
  url = {http://arxiv.org/abs/1807.10088},
  abstract = {We present the first generative adversarial network (GAN) for natural image matting. Our novel generator network is trained to predict visually appealing alphas with the addition of the adversarial loss from the discriminator that is trained to classify well-composited images. Further, we improve existing encoder-decoder architectures to better deal with the spatial localization issues inherited in convolutional neural networks (CNN) by using dilated convolutions to capture global context information without downscaling feature maps and losing spatial information. We present state-of-the-art results on the alphamatting online benchmark for the gradient error and give comparable results in others. Our method is particularly well suited for fine structures like hair, which is of great importance in practical matting applications, e.g. in film/TV production.},
  archivePrefix = {arXiv},
  arxivid = {1807.10088},
  eprint = {1807.10088},
  eprinttype = {arxiv}
}

@article{Ma2017-uh,
  title = {Data Integration from Pathology Slides for Quantitative Imaging of Multiple Cell Types within the Tumor Immune Cell Infiltrate},
  author = {Ma, Zhaoxuan and Shiao, Stephen L and Yoshida, Emi J and Swartwood, Steven and Huang, Fangjin and Doche, Michael E and Chung, Alice P and Knudsen, Beatrice S and Gertych, Arkadiusz},
  date = {2017-09},
  journaltitle = {Diagnostic Pathology},
  shortjournal = {Diagn. Pathol.},
  volume = {12},
  pages = {69},
  abstract = {BACKGROUND: Immune cell infiltrates (ICI) of tumors are scored by pathologists around tumor glands. To obtain a better understanding of the immune infiltrate, individual immune cell types, their activation states and location relative to tumor cells need to be determined. This process requires precise identification of the tumor area and enumeration of immune cell subtypes separately in the stroma and inside tumor nests. Such measurements can be accomplished by a multiplex format using immunohistochemistry (IHC). METHOD: We developed a pipeline that combines immunohistochemistry (IHC) and digital image analysis. One slide was stained with pan-cytokeratin and CD45 and the other slide with CD8, CD4 and CD68. The tumor mask generated through pan-cytokeratin staining was transferred from one slide to the other using affine image co-registration. Bland-Altman plots and Pearson correlation were used to investigate differences between densities and counts of immune cell underneath the transferred versus manually annotated tumor masks. One-way ANOVA was used to compare the mask transfer error for tissues with solid and glandular tumor architecture. RESULTS: The overlap between manual and transferred tumor masks ranged from 20\%-90\% across all cases. The error of transferring the mask was 2- to 4-fold greater in tumor regions with glandular compared to solid growth pattern (p {$<$} 10-6). Analyzing data from a single slide, the Pearson correlation coefficients of cell type densities outside and inside tumor regions were highest for CD4 + T-cells (r = 0.8), CD8 + T-cells (r = 0.68) or CD68+ macrophages (r = 0.79). The correlation coefficient for CD45+ T- and B-cells was only 0.45. The transfer of the mask generated an error in the measurement of intra- and extra- tumoral CD68+, CD8+ or CD4+ counts (p {$<$} 10-10). CONCLUSIONS: In summary, we developed a general method to integrate data from IHC stained slides into a single dataset. Because of the transfer error between slides, we recommend applying the antibody for demarcation of the tumor on the same slide as the ICI antibodies.},
  keywords = {Breast cancer,Image analysis,Immunohistochemistr},
  number = {1}
}

@article{Maaten2008-rq,
  title = {Visualizing Data Using T-{{SNE}}},
  author = {van der Maaten, Laurens and Hinton, Geoffrey},
  date = {2008},
  journaltitle = {Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume = {9},
  pages = {2579--2605},
  issue = {Nov},
  options = {useprefix=true}
}

@inproceedings{Macenko2009-ap,
  title = {A Method for Normalizing Histology Slides for Quantitative Analysis},
  booktitle = {2009 \{\vphantom\}{{IEEE}}\vphantom\{\} International Symposium on Biomedical Imaging: {{From}} Nano to Macro},
  author = {Macenko, M and Niethammer, M and Marron, J S and Borland, D and Woosley, J T and Guan, Xiaojun and Schmitt, C and Thomas, N E},
  date = {2009-06},
  pages = {1107--1110},
  abstract = {Inconsistencies in the preparation of histology slides make it difficult to perform quantitative analysis on their results. In this paper we provide two mechanisms for overcoming many of the known inconsistencies in the staining process, thereby bringing slides that were processed or stored under very different conditions into a common, normalized space to enable improved quantitative analysis.},
  keywords = {biological tissues,biology computing,biomedical im}
}

@article{Madabhushi2016-xg,
  title = {Image Analysis and Machine Learning in Digital Pathology: {{Challenges}} and Opportunities},
  author = {Madabhushi, Anant and Lee, George},
  date = {2016-10},
  journaltitle = {Medical Image Analysis},
  shortjournal = {Med. Image Anal.},
  volume = {33},
  pages = {170--175},
  publisher = {{Elsevier}},
  abstract = {With the rise in whole slide scanner technology, large numbers of tissue slides are being scanned and represented and archived digitally. While digital pathology has substantial implications for telepathology, second opinions, and education there are also huge research opportunities in image computing with this new source of “big data”. It is well known that there is fundamental prognostic data embedded in pathology images. The ability to mine “sub-visual” image features from digital pathology slide images, features that may not be visually discernible by a pathologist, offers the opportunity for better quantitative modeling of disease appearance and hence possibly improved prediction of disease aggressiveness and patient outcome. However the compelling opportunities in precision medicine offered by big digital pathology data come with their own set of computational challenges. Image analysis and computer assisted detection and diagnosis tools previously developed in the context of radiographic images are woefully inadequate to deal with the data density in high resolution digitized whole slide images. Additionally there has been recent substantial interest in combining and fusing radiologic imaging and proteomics and genomics based measurements with features extracted from digital pathology images for better prognostic prediction of disease aggressiveness and patient outcome. Again there is a paucity of powerful tools for combining disease specific features that manifest across multiple different length scales. The purpose of this review is to discuss developments in computational image analysis tools for predictive modeling of digital pathology images from a detection, segmentation, feature extraction, and tissue classification perspective. We discuss the emergence of new handcrafted feature approaches for improved predictive modeling of tissue appearance and also review the emergence of deep learning schemes for both object detection and tissue classification. We also briefly review some of the state of the art in fusion of radiology and pathology images and also combining digital pathology derived image measurements with molecular “omics” features for better predictive modeling. The review ends with a brief discussion of some of the technical and computational challenges to be overcome and reflects on future opportunities for the quantitation of histopathology.},
  keywords = {Deep learning,Digital pathology,Omics,Radiology}
}

@inproceedings{Mahajan2018,
  title = {Exploring the Limits of Weakly Supervised Pretraining},
  booktitle = {Proceedings of the European Conference on Computer Vision ({{ECCV}})},
  author = {Mahajan, Dhruv and Girshick, Ross and Ramanathan, Vignesh and He, Kaiming and Paluri, Manohar and Li, Yixuan and Bharambe, Ashwin and van der Maaten, Laurens},
  date = {2018},
  pages = {181--196},
  options = {useprefix=true}
}

@article{Mahar2013,
  title = {Legal Considerations of Consent and Privacy in the Context of Clinical Photography in {{Australian}} Medical Practice},
  author = {Mahar, Patrick D. and Foley, Peter A. and Sheed-Finck, Alexander and Baker, Christopher S.},
  date = {2013-01},
  journaltitle = {Medical Journal of Australia},
  volume = {198},
  pages = {48--49},
  issn = {0025729X},
  doi = {10.5694/mja12.11086},
  abstract = {Clinical photography has become integral to clinical practice, especially in visually oriented specialties. Where a clinical photograph forms part of a patient's medical record, clinicians have a legal obligation to keep that photograph for several years, as determined by federal privacy legislation and various state legislation. Patients may be able to access their own clinical photographs in the context of freedom of information legislation. Consent to take clinical photographs must be informed consent, and clinicians have a legal requirement to only use the photograph for the purpose defined by that consent.},
  number = {1}
}

@inproceedings{Mahbod2019,
  title = {Skin Lesion Classification Using Hybrid Deep Neural Networks},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE}} International Conference on Acoustics, Speech and Signal Processing ({{ICASSP}})},
  author = {Mahbod, Amirreza and Schaefer, Gerald and Wang, Chunliang and Ecker, Rupert and Ellinge, Isabella},
  date = {2019-05},
  pages = {1229--1233},
  publisher = {{IEEE}},
  doi = {10.1109/ICASSP.2019.8683352},
  url = {https://ieeexplore.ieee.org/document/8683352/},
  abstract = {Skin cancer is one of the major types of cancers with an increasing incidence over the past decades. Accurately diagnosing skin lesions to discriminate between benign and malignant skin lesions is crucial to ensure appropriate patient treatment. While there are many computerised methods for skin lesion classification, convolutional neural networks (CNNs) have been shown to be superior over classical methods. In this work, we propose a fully automatic computerised method for skin lesion classification which employs optimised deep features from a number of well-established CNNs and from different abstraction levels. We use three pre-trained deep models, namely AlexNet, VGG16 and ResNet-18, as deep feature generators. The extracted features then are used to train support vector machine classifiers. In the final stage, the classifier outputs are fused to obtain a classification. Evaluated on the 150 validation images from the ISIC 2017 classification challenge, the proposed method is shown to achieve very good classification performance, yielding an area under receiver operating characteristic curve of 83.83\% for melanoma classification and of 97.55\% for seborrheic keratosis classification.},
  isbn = {978-1-4799-8131-1}
}

@inproceedings{Mahbod2019a,
  title = {Skin Lesion Classification Using Hybrid Deep Neural Networks},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE}} International Conference on Acoustics, Speech and Signal Processing ({{ICASSP}})},
  author = {Mahbod, Amirreza and Schaefer, Gerald and Wang, Chunliang and Ecker, Rupert and Ellinge, Isabella},
  date = {2019-05},
  pages = {1229--1233},
  publisher = {{IEEE}},
  doi = {10.1109/ICASSP.2019.8683352},
  url = {https://ieeexplore.ieee.org/document/8683352/},
  abstract = {Skin cancer is one of the major types of cancers with an increasing incidence over the past decades. Accurately diagnosing skin lesions to discriminate between benign and malignant skin lesions is crucial to ensure appropriate patient treatment. While there are many computerised methods for skin lesion classification, convolutional neural networks (CNNs) have been shown to be superior over classical methods. In this work, we propose a fully automatic computerised method for skin lesion classification which employs optimised deep features from a number of well-established CNNs and from different abstraction levels. We use three pre-trained deep models, namely AlexNet, VGG16 and ResNet-18, as deep feature generators. The extracted features then are used to train support vector machine classifiers. In the final stage, the classifier outputs are fused to obtain a classification. Evaluated on the 150 validation images from the ISIC 2017 classification challenge, the proposed method is shown to achieve very good classification performance, yielding an area under receiver operating characteristic curve of 83.83\% for melanoma classification and of 97.55\% for seborrheic keratosis classification.},
  isbn = {978-1-4799-8131-1}
}

@inproceedings{Mahendran2015,
  title = {Understanding Deep Image Representations by Inverting Them},
  booktitle = {Proceedings of the {{IEEE}} Computer Society Conference on Computer Vision and Pattern Recognition},
  author = {Mahendran, Aravindh and Vedaldi, Andrea},
  date = {2015-10},
  volume = {07-12-June-2015},
  pages = {5188--5196},
  publisher = {{IEEE Computer Society}},
  issn = {10636919},
  doi = {10.1109/CVPR.2015.7299155},
  abstract = {Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG and SIFT more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.},
  isbn = {978-1-4673-6964-0}
}

@article{Maier-Hein2018,
  title = {Why Rankings of Biomedical Image Analysis Competitions Should Be Interpreted with Care},
  author = {Maier-Hein, Lena and Eisenmann, Matthias and Reinke, Annika and Onogur, Sinan and Stankovic, Marko and Scholz, Patrick and Arbel, Tal and Bogunovic, Hrvoje and Bradley, Andrew P. and Carass, Aaron and Feldmann, Carolin and Frangi, Alejandro F. and Full, Peter M. and van Ginneken, Bram and Hanbury, Allan and Honauer, Katrin and Kozubek, Michal and Landman, Bennett A. and März, Keno and Maier, Oskar and Maier-Hein, Klaus and Menze, Bjoern H. and Müller, Henning and Neher, Peter F. and Niessen, Wiro and Rajpoot, Nasir and Sharp, Gregory C. and Sirinukunwattana, Korsuk and Speidel, Stefanie and Stock, Christian and Stoyanov, Danail and Taha, Abdel Aziz and van der Sommen, Fons and Wang, Ching Wei and Weber, Marc André and Zheng, Guoyan and Jannin, Pierre and Kopp-Schneider, Annette},
  date = {2018-12},
  journaltitle = {Nature Communications},
  volume = {9},
  publisher = {{Nature Publishing Group}},
  issn = {20411723},
  doi = {10.1038/s41467-018-07619-7},
  abstract = {© 2018, The Author(s). International challenges have become the standard for validation of biomedical image analysis methods. Given their scientific impact, it is surprising that a critical analysis of common practices related to the organization of challenges has not yet been performed. In this paper, we present a comprehensive analysis of biomedical image analysis challenges conducted up to now. We demonstrate the importance of challenges and show that the lack of quality control has critical consequences. First, reproducibility and interpretation of the results is often hampered as only a fraction of relevant information is typically provided. Second, the rank of an algorithm is generally not robust to a number of variables such as the test data used for validation, the ranking scheme applied and the observers that make the reference annotations. To overcome these problems, we recommend best practice guidelines and define open research questions to be addressed in the future.},
  number = {1},
  options = {useprefix=true}
}

@article{makridakis2017forthcoming,
  title = {The Forthcoming {{Artificial Intelligence}} ({{AI}}) Revolution: {{Its}} Impact on Society and Firms},
  author = {Makridakis, Spyros},
  date = {2017},
  journaltitle = {Futures},
  volume = {90},
  pages = {46--60},
  publisher = {{Elsevier}}
}

@article{Malon2013-ni,
  title = {Classification of Mitotic Figures with Convolutional Neural Networks and Seeded Blob Features},
  author = {Malon, Christopher D and Cosatto, Eric},
  date = {2013-05},
  journaltitle = {J. Pathol. Inform.},
  volume = {4},
  pages = {9},
  abstract = {BACKGROUND: The mitotic figure recognition contest at the 2012 International Conference on Pattern Recognition (ICPR) challenges a system to identify all mitotic figures in a region of interest of hematoxylin and eosin stained tissue, using each of three scanners (Aperio, Hamamatsu, and multispectral). METHODS: Our approach combines manually designed nuclear features with the learned features extracted by convolutional neural networks (CNN). The nuclear features capture color, texture, and shape information of segmented regions around a nucleus. The use of a CNN handles the variety of appearances of mitotic figures and decreases sensitivity to the manually crafted features and thresholds. RESULTS: On the test set provided by the contest, the trained system achieves F1 scores up to 0.659 on color scanners and 0.589 on multispectral scanner. CONCLUSIONS: We demonstrate a powerful technique combining segmentation-based features with CNN, identifying the majority of mitotic figures with a fair precision. Further, we show that the approach accommodates information from the additional focal planes and spectral bands from a multi-spectral scanner without major redesign.},
  keywords = {convolutional neural network,digital pat,Mitosis}
}

@article{Malon2013-rk,
  title = {Classification of Mitotic Figures with Convolutional Neural Networks and Seeded Blob Features},
  author = {Malon, Christopher D and Cosatto, Eric},
  date = {2013-05},
  journaltitle = {J. Pathol. Inform.},
  volume = {4},
  pages = {9},
  abstract = {BACKGROUND: The mitotic figure recognition contest at the 2012 International Conference on Pattern Recognition (ICPR) challenges a system to identify all mitotic figures in a region of interest of hematoxylin and eosin stained tissue, using each of three scanners (Aperio, Hamamatsu, and multispectral). METHODS: Our approach combines manually designed nuclear features with the learned features extracted by convolutional neural networks (CNN). The nuclear features capture color, texture, and shape information of segmented regions around a nucleus. The use of a CNN handles the variety of appearances of mitotic figures and decreases sensitivity to the manually crafted features and thresholds. RESULTS: On the test set provided by the contest, the trained system achieves F1 scores up to 0.659 on color scanners and 0.589 on multispectral scanner. CONCLUSIONS: We demonstrate a powerful technique combining segmentation-based features with CNN, identifying the majority of mitotic figures with a fair precision. Further, we show that the approach accommodates information from the additional focal planes and spectral bands from a multi-spectral scanner without major redesign.},
  keywords = {convolutional neural network,digital pat,Mitosis}
}

@inproceedings{Mandache2018,
  title = {Basal Cell Carcinoma Detection in Full Field {{OCT}} Images Using Convolutional Neural Networks},
  booktitle = {Proceedings - International Symposium on Biomedical Imaging},
  author = {Mandache, D. and Dalimier, E. and Durkin, J. R. and Boceara, C. and Olivo-Marin, J. C. and Meas-Yedid, V.},
  date = {2018-05},
  volume = {2018-April},
  pages = {784--787},
  publisher = {{IEEE Computer Society}},
  issn = {19458452},
  doi = {10.1109/ISBI.2018.8363689},
  abstract = {© 2018 IEEE. In this paper we introduce a new application that exploits the emerging imaging modality of full field optical coherence tomography (FFOCT) as a means of optical biopsy. The objective is to build a computer-aided diagnosis (CAD) tool that can speed up the detection of tumoral areas in skin excisions resulting from Mohs surgery. Since there is little prior knowledge about the appearance of cancer cell morphology in this type of imagery, deep learning techniques are applied. Using convolutional neural networks (CNN), we train a feature extractor able to find representative characteristics for FFOCT data and a classifier that learns a generalized distribution of the data. With a dataset of 40 high-resolution images, we obtained a classification accuracy of 95.93\%.},
  isbn = {978-1-5386-3636-7},
  keywords = {Convolutional neural networks,Digital pathology,Full field optical coherence tomography,Nonmelanoma skin cancer}
}

@article{Mar2017,
  title = {Computer-Assisted Diagnosis for Skin Cancer: Have We Been Outsmarted?},
  author = {Mar, Victoria J and Scolyer, Richard A and Long, Georgina V},
  date = {2017},
  journaltitle = {The Lancet},
  volume = {389},
  pages = {1962--1964},
  issn = {0140-6736},
  number = {10083}
}

@article{Marinelli2008-rs,
  title = {The Stanford Tissue Microarray Database},
  author = {Marinelli, Robert J and Montgomery, Kelli and Liu, Chih Long and Shah, Nigam H and Prapong, Wijan and Nitzberg, Michael and Zachariah, Zachariah K and Sherlock, Gavin J and Natkunam, Yasodha and West, Robert B and van de Rijn, Matt and Brown, Patrick O and Ball, Catherine A},
  date = {2008-01},
  journaltitle = {Nucleic Acids Research},
  shortjournal = {Nucleic Acids Res.},
  volume = {36},
  pages = {D871----7},
  abstract = {The Stanford Tissue Microarray Database (TMAD; http://tma.stanford.edu) is a public resource for disseminating annotated tissue images and associated expression data. Stanford University pathologists, researchers and their collaborators worldwide use TMAD for designing, viewing, scoring and analyzing their tissue microarrays. The use of tissue microarrays allows hundreds of human tissue cores to be simultaneously probed by antibodies to detect protein abundance (Immunohistochemistry; IHC), or by labeled nucleic acids (in situ hybridization; ISH) to detect transcript abundance. TMAD archives multi-wavelength fluorescence and bright-field images of tissue microarrays for scoring and analysis. As of July 2007, TMAD contained 205 161 images archiving 349 distinct probes on 1488 tissue microarray slides. Of these, 31 306 images for 68 probes on 125 slides have been released to the public. To date, 12 publications have been based on these raw public data. TMAD incorporates the NCI Thesaurus ontology for searching tissues in the cancer domain. Image processing researchers can extract images and scores for training and testing classification algorithms. The production server uses the Apache HTTP Server, Oracle Database and Perl application code. Source code is available to interested researchers under a no-cost license.},
  issue = {Database issue},
  options = {useprefix=true}
}

@article{Marvdashti2016-uz,
  title = {Classification of Basal Cell Carcinoma in Human Skin Using Machine Learning and Quantitative Features Captured by Polarization Sensitive Optical Coherence Tomography},
  author = {Marvdashti, Tahereh and Duan, Lian and Aasi, Sumaira Z and Tang, Jean Y and Ellerbee Bowden, Audrey K},
  date = {2016-09},
  journaltitle = {Biomedical Optics Express},
  shortjournal = {Biomed. Opt. Express},
  volume = {7},
  pages = {3721--3735},
  abstract = {We report the first fully automated detection of basal cell carcinoma (BCC), the most commonly occurring type of skin cancer, in human skin using polarization-sensitive optical coherence tomography (PS-OCT). Our proposed automated procedure entails building a machine-learning based classifier by extracting image features from the two complementary image contrasts offered by PS-OCT, intensity and phase retardation (PR), and selecting a subset of features that yields a classifier with the highest accuracy. Our classifier achieved 95.4\% sensitivity and specificity, validated by leave-one-patient-out cross validation (LOPOCV), in detecting BCC in human skin samples collected from 42 patients. Moreover, we show the superiority of our classifier over the best possible classifier based on features extracted from intensity-only data, which demonstrates the significance of PR data in detecting BCC.},
  keywords = {(100.0100) Image processing,(110.4500) Optical co},
  number = {9}
}

@online{McInnes2018,
  title = {Umap: {{Uniform}} Manifold Approximation and Projection for Dimension Reduction},
  author = {McInnes, Leland and Healy, John and Melville, James},
  date = {2018},
  archivePrefix = {arXiv},
  eprint = {1802.03426},
  eprinttype = {arxiv}
}

@online{mcinnes2018umap,
  title = {Umap: {{Uniform}} Manifold Approximation and Projection for Dimension Reduction},
  author = {McInnes, Leland and Healy, John and Melville, James},
  date = {2018},
  archivePrefix = {arXiv},
  eprint = {1802.03426},
  eprinttype = {arxiv}
}

@article{mConceptAttributionExplaining2020,
  title = {Concept Attribution: {{Explaining CNN}} Decisions to Physicians},
  author = {M, Graziani and V, Andrearczyk and S, Marchand-Maillet and H, Müller},
  date = {2020},
  journaltitle = {Computers in Biology and Medicine},
  volume = {123},
  pages = {103865},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2020.103865},
  url = {http://www.sciencedirect.com/science/article/pii/S0010482520302225},
  abstract = {Deep learning explainability is often reached by gradient-based approaches that attribute the network output to perturbations of the input pixels. However, the relevance of input pixels may be difficult to relate to relevant image features in some applications, e.g. diagnostic measures in medical imaging. The framework described in this paper shifts the attribution focus from pixel values to user-defined concepts. By checking if certain diagnostic measures are present in the learned representations, experts can explain and entrust the network output. Being post-hoc, our method does not alter the network training and can be easily plugged into the latest state-of-the-art convolutional networks. This paper presents the main components of the framework for attribution to concepts, in addition to the introduction of a spatial pooling operation on top of the feature maps to obtain a solid interpretability analysis. Furthermore, regularized regression is analyzed as a solution to the regression overfitting in high-dimensionality latent spaces. The versatility of the proposed approach is shown by experiments on two medical applications, namely histopathology and retinopathy, and on one non-medical task, the task of handwritten digit classification. The obtained explanations are in line with clinicians’ guidelines and complementary to widely used visualization tools such as saliency maps.},
  keywords = {Biomedical imaging,Deep learning,Interpretability,Machine learning}
}

@inproceedings{Mehta2018,
  title = {Learning to Segment Breast Biopsy Whole Slide Images},
  booktitle = {Proceedings - 2018 {{IEEE}} Winter Conference on Applications of Computer Vision, {{WACV}} 2018},
  author = {Mehta, Sachin and Mercan, Ezgi and Bartlett, Jamen and Weaver, Donald and Elmore, Joann and Shapiro, Linda},
  date = {2018-05},
  volume = {2018-January},
  pages = {663--672},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/WACV.2018.00078},
  abstract = {We trained and applied an encoder-decoder model to semantically segment breast biopsy images into biologically meaningful tissue labels. Since conventional encoder-decoder networks cannot be applied directly on large biopsy images and the different sized structures in biopsies present novel challenges, we propose four modifications: (1) an input-aware encoding block to compensate for information loss, (2) a new dense connection pattern between encoder and decoder, (3) dense and sparse decoders to combine multi-level features, (4) a multi-resolution network that fuses the results of encoder-decoders run on different resolutions. Our model outperforms a feature-based approach and conventional encoder-decoders from the literature. We use semantic segmentations produced with our model in an automated diagnosis task and obtain higher accuracies than a baseline approach that employs an SVM for feature-based segmentation, both using the same segmentation-based diagnostic features.},
  isbn = {978-1-5386-4886-5}
}

@online{Menegola2016-ia,
  title = {Towards Automated Melanoma Screening: {{Exploring}} Transfer Learning Schemes},
  author = {Menegola, Afonso and Fornaciali, Michel and Pires, Ramon and Avila, Sandra and Valle, Eduardo},
  date = {2016-09},
  abstract = {Deep learning is the current bet for image classification. Its greed for huge amounts of annotated data limits its usage in medical imaging context. In this scenario transfer learning appears as a prominent solution. In this report we aim to clarify how transfer learning schemes may influence classification results. We are particularly focused in the automated melanoma screening problem, a case of medical imaging in which transfer learning is still not widely used. We explored transfer with and without fine-tuning, sequential transfers and usage of pre-trained models in general and specific datasets. Although some issues remain open, our findings may drive future researches.},
  archivePrefix = {arXiv},
  arxivid = {cs.CV/1609.01228},
  eprint = {1609.01228},
  eprinttype = {arxiv},
  primaryClass = {cs.CV}
}

@article{Messerli1987-vk,
  title = {Inter- and Intra-Pathologist Variability in the Diagnosis of Gestational Trophoblastic Neoplasia},
  author = {Messerli, M L and Parmley, T and Woodruff, J D and Lilienfeld, A M and Bevilacqua, L and Rosenshein, N B},
  date = {1987-04},
  journaltitle = {Obstetrics and Gynecology},
  shortjournal = {Obstet. Gynecol.},
  volume = {69},
  pages = {622--626},
  publisher = {{journals.lww.com}},
  abstract = {All 190 cases of gestational trophoblastic neoplasia diagnosed in the Baltimore metropolitan area from 1975-1982 were identified. Histologic slides were requested and reviewed independently by two pathologists who agreed upon uniform criteria for the diagnosis of hydatidiform (complete) mole, invasive mole, and choriocarcinoma. A representative sample of the slides was selected and resubmitted to one of the study pathologists for a second review. The inter- and intra-pathologist variability in the diagnosis of gestational trophoblastic neoplasia was calculated using the kappa statistic (K). Our findings indicated that the variability in the diagnosis of gestational trophoblastic neoplasia was low whereas that for the related tumor of incomplete mole was high.},
  number = {4}
}

@article{Metter2019,
  title = {Trends in the {{US}} and Canadian Pathologist Workforces from 2007 to 2017},
  author = {Metter, David M and Colgan, Terence J and Leung, Stanley T and Timmons, Charles F and Park, Jason Y},
  date = {2019},
  journaltitle = {JAMA network open},
  volume = {2},
  pages = {e194337--e194337},
  publisher = {{American Medical Association}},
  number = {5}
}

@article{Meyer2018,
  title = {Survey on Deep Learning for Radiotherapy},
  author = {Meyer, Philippe and Noblet, Vincent and Mazzara, Christophe and Lallement, Alex},
  date = {2018-07},
  journaltitle = {Computers in biology and medicine},
  volume = {98},
  pages = {126--146},
  publisher = {{Elsevier Ltd}},
  issn = {18790534},
  doi = {10.1016/j.compbiomed.2018.05.018},
  abstract = {More than 50\% of cancer patients are treated with radiotherapy, either exclusively or in combination with other methods. The planning and delivery of radiotherapy treatment is a complex process, but can now be greatly facilitated by artificial intelligence technology. Deep learning is the fastest-growing field in artificial intelligence and has been successfully used in recent years in many domains, including medicine. In this article, we first explain the concept of deep learning, addressing it in the broader context of machine learning. The most common network architectures are presented, with a more specific focus on convolutional neural networks. We then present a review of the published works on deep learning methods that can be applied to radiotherapy, which are classified into seven categories related to the patient workflow, and can provide some insights of potential future applications. We have attempted to make this paper accessible to both radiotherapy and deep learning communities, and hope that it will inspire new collaborations between these two communities to develop dedicated radiotherapy applications.},
  keywords = {Convolutional networks,Deep-learning,Radiotherapy}
}

@article{Miller2018,
  title = {Explanation in Artificial Intelligence: {{Insights}} from the Social Sciences},
  author = {Miller, Tim},
  date = {2018},
  journaltitle = {Artificial Intelligence},
  publisher = {{Elsevier}},
  issn = {0004-3702}
}

@article{Miller2018a,
  title = {Artificial Intelligence in Medical Practice: {{The}} Question to the Answer?},
  author = {Miller, D. Douglas and Brown, Eric W.},
  date = {2018},
  journaltitle = {American Journal of Medicine},
  volume = {131},
  pages = {129--133},
  publisher = {{Elsevier Inc.}},
  issn = {15557162},
  doi = {10.1016/j.amjmed.2017.10.035},
  url = {https://doi.org/10.1016/j.amjmed.2017.10.035},
  abstract = {Computer science advances and ultra-fast computing speeds find artificial intelligence (AI) broadly benefitting modern society—forecasting weather, recognizing faces, detecting fraud, and deciphering genomics. AI's future role in medical practice remains an unanswered question. Machines (computers) learn to detect patterns not decipherable using biostatistics by processing massive datasets (big data) through layered mathematical models (algorithms). Correcting algorithm mistakes (training) adds to AI predictive model confidence. AI is being successfully applied for image analysis in radiology, pathology, and dermatology, with diagnostic speed exceeding, and accuracy paralleling, medical experts. While diagnostic confidence never reaches 100\%, combining machines plus physicians reliably enhances system performance. Cognitive programs are impacting medical practice by applying natural language processing to read the rapidly expanding scientific literature and collate years of diverse electronic medical records. In this and other ways, AI may optimize the care trajectory of chronic disease patients, suggest precision therapies for complex illnesses, reduce medical errors, and improve subject enrollment into clinical trials.},
  keywords = {Analytics,Artificial intelligence,Big data,Chronic disease,Deep learning,Electronic medical record,Machine learning,Medical imaging,Natural language processing,Neural networks,Precision medicine},
  number = {2}
}

@article{Miotto2017,
  title = {Deep Learning for Healthcare: Review, Opportunities and Challenges},
  author = {Miotto, Riccardo and Wang, Fei and Wang, Shuang and Jiang, Xiaoqian and Dudley, Joel T},
  date = {2017},
  journaltitle = {Briefings in Bioinformatics},
  pages = {bbx044--bbx044},
  issn = {1467-5463},
  doi = {10.1093/bib/bbx044},
  url = {http://dx.doi.org/10.1093/bib/bbx044}
}

@article{Mishra2016,
  title = {Does Modern Medicine Increase Life-Expectancy: {{Quest}} for the {{Moon Rabbit}}?},
  author = {Mishra, Sundeep},
  date = {2016},
  journaltitle = {Indian Heart Journal},
  volume = {68},
  pages = {19--27},
  issn = {00194832},
  doi = {10.1016/j.ihj.2016.01.003},
  abstract = {The search for elixir of immortality has yielded mixed results. While some of the interventions like percutaneous coronary interventions and coronary artery bypass grafting have been a huge disappointment at least as far as prolongation of life is concerned, their absolute benefit is meager and that too in very sick patients. Cardiac specific drugs like statins and aspirin have fared slightly better, being useful in patients with manifest coronary artery disease, particularly in sicker populations although even their usefulness in primary prevention is rather low. The only strategies of proven benefit in primary/primordial prevention are pursuing a healthy life-style and its modification when appropriate, like cessation of smoking, weight reduction, increasing physical activity, eating a healthy diet and bringing blood pressure, serum cholesterol, and blood glucose under control.},
  number = {1}
}

@inproceedings{Mittelstadt2019,
  title = {Explaining Explanations in {{AI}}},
  booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  author = {Mittelstadt, Brent and Russell, Chris and Wachter, Sandra},
  date = {2019},
  pages = {279--288},
  publisher = {{ACM}},
  isbn = {1-4503-6125-0}
}

@article{Mokhtari2014-qb,
  title = {Computer Aided Measurement of Melanoma Depth of Invasion in Microscopic Images},
  author = {Mokhtari, Mojgan and Rezaeian, Mahdie and Gharibzadeh, Shahriar and Malekian, Vahid},
  date = {2014-06},
  journaltitle = {Micron},
  volume = {61},
  pages = {40--48},
  abstract = {This paper presents a novel computer aided technique for measurement of melanoma depth of invasion. Melanoma is the deadliest form of skin cancer with worldwide increasing incidences. For a conclusive diagnosis of melanoma, skin biopsies should be examined under a microscope. Visual inspection of microscopic samples is often subjective, time-consuming, cumbersome and prone to human errors. This fact demonstrates the necessity of developing an automated method which assists pathologists in evaluating histopathological samples more accurately in the busy clinical environment. To the best of our knowledge, this is the first time that a computer-assisted diagnosis algorithm has been applied in measurement of melanoma invasion depth. The proposed method uses a clustering algorithm for granular layer extraction and a pre-trained SVM classifier for detection of malignant melanocytes. The experimental results with average error of 3.9\$𝜇\$m demonstrate that the proposed method is reliable and effective.},
  keywords = {Computer-assisted diagnosis,Depth of invasion,Me}
}

@article{MOLLER1957,
  title = {Subjective Errors in Interpretation of Lung X-Rays},
  author = {Moller, A V},
  date = {1957},
  journaltitle = {Ugeskrift for laeger},
  volume = {119},
  pages = {1596--1598},
  issn = {0041-5782},
  number = {49}
}

@article{mori1970bukimi,
  title = {Bukimi No Tani [the Uncanny Valley]},
  author = {Mori, Masahiro},
  date = {1970},
  journaltitle = {Energy},
  volume = {7},
  pages = {33--35}
}

@online{Muhammad2019,
  title = {Towards Unsupervised Cancer Subtyping: {{Predicting}} Prognosis Using a Histologic Visual Dictionary},
  author = {Muhammad, Hassan and Sigel, Carlie S. and Campanella, Gabriele and Boerner, Thomas and Pak, Linda M. and Büttner, Stefan and IJzermans, Jan N. M. and Koerkamp, Bas Groot and Doukas, Michael and Jarnagin, William R. and Simpson, Amber and Fuchs, Thomas J.},
  date = {2019-03},
  url = {http://arxiv.org/abs/1903.05257},
  abstract = {Unlike common cancers, such as those of the prostate and breast, tumor grading in rare cancers is difficult and largely undefined because of small sample sizes, the sheer volume of time needed to undertake on such a task, and the inherent difficulty of extracting human-observed patterns. One of the most challenging examples is intrahepatic cholangiocarcinoma (ICC), a primary liver cancer arising from the biliary system, for which there is well-recognized tumor heterogeneity and no grading paradigm or prognostic biomarkers. In this paper, we propose a new unsupervised deep convolutional autoencoder-based clustering model that groups together cellular and structural morphologies of tumor in 246 ICC digitized whole slides, based on visual similarity. From this visual dictionary of histologic patterns, we use the clusters as covariates to train Cox-proportional hazard survival models. In univariate analysis, three clusters were significantly associated with recurrence-free survival. Combinations of these clusters were significant in multivariate analysis. In a multivariate analysis of all clusters, five showed significance to recurrence-free survival, however the overall model was not measured to be significant. Finally, a pathologist assigned clinical terminology to the significant clusters in the visual dictionary and found evidence supporting the hypothesis that collagen-enriched fibrosis plays a role in disease severity. These results offer insight into the future of cancer subtyping and show that computational pathology can contribute to disease prognostication, especially in rare cancers.},
  archivePrefix = {arXiv},
  arxivid = {1903.05257},
  eprint = {1903.05257},
  eprinttype = {arxiv}
}

@article{Mungle2017,
  title = {{{MRF}}-{{ANN}}: A Machine Learning Approach for Automated {{ER}} Scoring of Breast Cancer Immunohistochemical Images.},
  author = {Mungle, T and Tewary, S and Das, D K and Arun, I and Basak, B and Agarwal, S and Ahmed, R and Chatterjee, S and Chakraborty, C},
  date = {2017-08},
  journaltitle = {Journal of microscopy},
  volume = {267},
  pages = {117--129},
  issn = {1365-2818 (Electronic)},
  doi = {10.1111/jmi.12552},
  abstract = {Molecular pathology, especially immunohistochemistry, plays an important role in evaluating hormone receptor status along with diagnosis of breast cancer. Time-consumption and inter-/intraobserver variability are major hindrances for evaluating the receptor score. In view of this, the paper proposes an automated Allred Scoring methodology for estrogen receptor (ER). White balancing is used to normalize the colour image taking into consideration colour variation during staining in different labs. Markov random field model with expectation-maximization optimization is employed to segment the ER cells. The proposed segmentation methodology is found to have F-measure 0.95. Artificial neural network is subsequently used to obtain intensity-based score for ER cells, from pixel colour intensity features. Simultaneously, proportion score - percentage of ER positive cells is computed via cell counting. The final ER score is computed by adding intensity and proportion scores - a standard Allred scoring system followed by pathologists. The classification accuracy for classification of cells by classifier in terms of F-measure is 0.9626. The problem of subjective interobserver ability is addressed by quantifying ER score from two expert pathologist and proposed methodology. The intraclass correlation achieved is greater than 0.90. The study has potential advantage of assisting pathologist in decision making over manual procedure and could evolve as a part of automated decision support system with other receptor scoring/analysis procedure.},
  eprint = {28319275},
  eprinttype = {pmid},
  keywords = {analysis,Automation,Biomarkers,Breast Neoplasms,Computer-Assisted,diagnosis,Estrogen,Female,Humans,Image Processing,Immunohistochemistry,Laboratory,Machine Learning,methods,Neural Networks (Computer),Receptors,Tumor},
  langid = {english},
  number = {2}
}

@article{Nahhas2017,
  title = {A Review of the Global Guidelines on Surgical Margins for Nonmelanoma Skin Cancers},
  author = {Nahhas, Amanda F and Scarbrough, Chase A and Trotter, Shannon},
  date = {2017},
  journaltitle = {The Journal of clinical and aesthetic dermatology},
  volume = {10},
  pages = {37},
  publisher = {{Matrix Medical Communications}},
  number = {4}
}

@inproceedings{Nair2010,
  title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  booktitle = {Proceedings of the 27th International Conference on Machine Learning ({{ICML}}-10)},
  author = {Nair, Vinod and Hinton, Geoffrey E},
  date = {2010},
  pages = {807--814}
}

@article{naveed2015privacy,
  title = {Privacy in the Genomic Era},
  author = {Naveed, Muhammad and Ayday, Erman and Clayton, Ellen W and Fellay, Jacques and Gunter, Carl A and Hubaux, Jean-Pierre and Malin, Bradley A and Wang, XiaoFeng},
  date = {2015},
  journaltitle = {ACM Computing Surveys (CSUR)},
  volume = {48},
  pages = {6},
  publisher = {{ACM}},
  number = {1}
}

@inproceedings{Naylor2017,
  title = {Nuclei Segmentation in Histopathology Images Using Deep Neural Networks},
  booktitle = {Proceedings - International Symposium on Biomedical Imaging},
  author = {Naylor, Peter and Lae, Marick and Reyal, Fabien and Walter, Thomas},
  date = {2017},
  pages = {933--936},
  publisher = {{IEEE}},
  issn = {19458452},
  doi = {10.1109/ISBI.2017.7950669},
  abstract = {Analysis and interpretation of stained tumor sections is one of the main tools in cancer diagnosis and prognosis, which is mainly carried out manually by pathologists. The avent of digital pathology provides us with the challenging opportunity to automatically analyze large amounts of these complex image data in order to draw biological conclusions from them and to study cellular and tissular phenotypes at a large scale. One of the bottlenecks for such approaches is the automatic segmentation of cell nuclei from this type of image data. Here, we present a fully automated workflow to segment nuclei from histopathology image data by using deep neural networks trained from a set of manually annotated images and by processing the posterior probability maps in order to split jointly segmented nuclei. Further, we provide the image data set that has been generated for this study as a benchmark set to the scientific community.},
  isbn = {978-1-5090-1171-1},
  keywords = {Breast Cancer,Cellular Phenotyping,Convolutional Neural Networks,Deep Learning,Digital Pathology,Histopathology,Nuclei Segmentation}
}

@inproceedings{Nguyen2015,
  title = {Deep Neural Networks Are Easily Fooled: {{High}} Confidence Predictions for Unrecognizable Images},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  date = {2015},
  pages = {427--436}
}

@online{Nguyen2016,
  title = {Multifaceted Feature Visualization: {{Uncovering}} the Different Types of Features Learned by Each Neuron in Deep Neural Networks},
  author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  date = {2016},
  archivePrefix = {arXiv},
  eprint = {1602.03616},
  eprinttype = {arxiv}
}

@inproceedings{Nguyen2016a,
  title = {Synthesizing the Preferred Inputs for Neurons in Neural Networks via Deep Generator Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Nguyen, Anh and Dosovitskiy, Alexey and Yosinski, Jason and Brox, Thomas and Clune, Jeff},
  date = {2016},
  pages = {3387--3395}
}

@online{nguyen2016synthesizing,
  title = {Synthesizing the Preferred Inputs for Neurons in Neural Networks via Deep Generator Networks},
  author = {Nguyen, Anh and Dosovitskiy, Alexey and Yosinski, Jason and Brox, Thomas and Clune, Jeff},
  date = {2016},
  archivePrefix = {arXiv},
  eprint = {1605.09304},
  eprinttype = {arxiv}
}

@online{nguyen2016synthesizing,
  title = {Synthesizing the Preferred Inputs for Neurons in Neural Networks via Deep Generator Networks},
  author = {Nguyen, Anh and Dosovitskiy, Alexey and Yosinski, Jason and Brox, Thomas and Clune, Jeff},
  date = {2016},
  archivePrefix = {arXiv},
  eprint = {1605.09304},
  eprinttype = {arxiv}
}

@inproceedings{nguyen2017plug,
  title = {Plug \& Play Generative Networks: {{Conditional}} Iterative Generation of Images in Latent Space},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Nguyen, Anh and Clune, Jeff and Bengio, Yoshua and Dosovitskiy, Alexey and Yosinski, Jason},
  date = {2017},
  pages = {4467--4477}
}

@article{Niazi2019,
  title = {Digital Pathology and Artificial Intelligence},
  author = {Niazi, Muhammad Khalid Khan and Parwani, Anil V. and Gurcan, Metin N.},
  date = {2019-05},
  journaltitle = {The lancet oncology},
  volume = {20},
  pages = {e253--e261},
  publisher = {{Lancet Publishing Group}},
  issn = {14745488},
  doi = {10.1016/S1470-2045(19)30154-8},
  abstract = {In modern clinical practice, digital pathology has a crucial role and is increasingly a technological requirement in the scientific laboratory environment. The advent of whole-slide imaging, availability of faster networks, and cheaper storage solutions has made it easier for pathologists to manage digital slide images and share them for clinical use. In parallel, unprecedented advances in machine learning have enabled the synergy of artificial intelligence and digital pathology, which offers image-based diagnosis possibilities that were once limited only to radiology and cardiology. Integration of digital slides into the pathology workflow, advanced algorithms, and computer-aided diagnostic techniques extend the frontiers of the pathologist's view beyond a microscopic slide and enable true utilisation and integration of knowledge that is beyond human limits and boundaries, and we believe there is clear potential for artificial intelligence breakthroughs in the pathology setting. In this Review, we discuss advancements in digital slide-based image diagnosis for cancer along with some challenges and opportunities for artificial intelligence in digital pathology.},
  number = {5}
}

@article{Noroozi2016-hz,
  title = {Differential Diagnosis of Squamous Cell Carcinoma in Situ Using Skin Histopathological Images},
  author = {Noroozi, Navid and Zakerolhosseini, Ali},
  date = {2016-03},
  journaltitle = {Computers in Biology and Medicine},
  shortjournal = {Comput. Biol. Med.},
  volume = {70},
  pages = {23--39},
  abstract = {Differential diagnosis of squamous cell carcinoma in situ is of great importance for prognosis and decision making in the disease treatment procedure. Currently, differential diagnosis is done by pathologists based on examination of the histopathological slides under the microscope, which is time consuming and prone to inter and intra observer variability. In this paper, we have proposed an automated method for differential diagnosis of SCC in situ from actinic keratosis, which is known to be a precursor of squamous cell carcinoma. The process begins with epidermis segmentation and cornified layer removal. Then, epidermis axis is specified using the paths in its skeleton and the granular layer is removed via connected components analysis. Finally, diagnosis is done based on the classification result of intensity profiles extracted from lines perpendicular to the epidermis axis. The results of the study are in agreement with the gold standards provided by expert pathologists.},
  keywords = {Actinic keratosis,Classification,Cutaneous squam}
}

@article{Noroozi2016-jf,
  title = {Computer Assisted Diagnosis of Basal Cell Carcinoma Using {{Z}}-Transform Features},
  author = {Noroozi, Navid and Zakerolhosseini, Ali},
  date = {2016-10},
  journaltitle = {J. Vis. Commun. Image Represent.},
  volume = {40},
  pages = {128--148},
  abstract = {Detection of basal cell carcinoma tumor is of great importance for decision making in the disease treatment procedure. Visual inspection of the histopathological slides for tumor detection is laborious, time consuming and prone to inter and intra observer variability. In this paper, we have proposed an automated method for discriminating basal cell carcinoma tumor from squamous cell carcinoma tumor in skin histopathological images using Z-transform features, which were not used previously in image classification tasks. For the first time, it is shown that how two or three Fourier transform features can be combined to form one Z-transform feature. Experiments have shown that the tumor classification results obtained by our method are in reasonable agreement with the gold standards provided by expert pathologists.},
  keywords = {Basal cell carcinoma,Skin cancer,Squamous cell c}
}

@article{Odena2016,
  title = {Deconvolution and Checkerboard Artifacts},
  author = {Odena, Augustus and Dumoulin, Vincent and Olah, Chris},
  date = {2016},
  journaltitle = {Distill},
  volume = {1},
  pages = {e3},
  issn = {2476-0757},
  number = {10}
}

@article{Olah2017,
  title = {Feature Visualization},
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  date = {2017},
  journaltitle = {Distill},
  volume = {2},
  pages = {e7},
  issn = {2476-0757},
  number = {11}
}

@article{Olah2018,
  title = {The Building Blocks of Interpretability},
  author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  date = {2018},
  journaltitle = {Distill},
  volume = {3},
  pages = {e10},
  issn = {2476-0757},
  number = {3}
}

@article{Olsen2018,
  title = {Diagnostic Performance of Deep Learning Algorithms Applied to Three Common Diagnoses in Dermatopathology},
  author = {Olsen, ThomasGeorge and Jackson, BHunter and Feeser, TheresaAnn and Kent, MichaelN and Moad, JohnC and Krishnamurthy, Smita and Lunsford, DeniseD and Soans, RajathE},
  date = {2018-09},
  journaltitle = {Journal of Pathology Informatics},
  volume = {9},
  pages = {32},
  publisher = {{Medknow}},
  issn = {2153-3539},
  doi = {10.4103/jpi.jpi_31_18},
  abstract = {Context: Image segmentation pipelines often are sensitive to algorithm input parameters. Algorithm parameters optimized for a set of images do not necessarily produce good‑quality‑segmentation results for other images. Even within an image, some regions may not be well segmented due to a number of factors, including multiple pieces of tissue with distinct characteristics, differences in staining of the tissue, normal versus tumor regions, and tumor heterogeneity. Evaluation of quality of segmentation results is an important step in image analysis. It is very labor intensive to do quality assessment manually with large image datasets because a whole‑slide tissue image may have hundreds of thousands of nuclei. Semi‑automatic mechanisms are needed to assist researchers and application developers to detect image regions with bad segmentations efficiently. Aims: Our goal is to develop and evaluate a machine‑learning‑based semi‑automated workflow to assess quality of nucleus segmentation results in a large set of whole‑slide tissue images. Methods: We propose a quality control methodology, in which machine‑learning algorithms are trained with image intensity and texture features to produce a classification model. This model is applied to image patches in a whole‑slide tissue image to predict the quality of nucleus segmentation in each patch. The training step of our methodology involves the selection and labeling of regions by a pathologist in a set of images to create the training dataset. The image regions are partitioned into patches. A set of intensity and texture features is computed for each patch. A classifier is trained with the features and the labels assigned by the pathologist. At the end of this process, a classification model is generated. The classification step applies the classification model to unlabeled test images. Each test image is partitioned into patches. The classification model is applied to each patch to predict the patch's label. Results: The proposed methodology has been evaluated by assessing the segmentation quality of a segmentation method applied to images from two cancer types in The Cancer Genome Atlas; WHO Grade II lower grade glioma (LGG) and lung adenocarcinoma (LUAD). The results show that our method performs well in predicting patches with good‑quality segmentations and achieves F1 scores 84.7\% for LGG and 75.43\% for LUAD. Conclusions: As image scanning technologies advance, large volumes of whole‑slide tissue images will be available for research and clinical use. Efficient approaches for the assessment of quality and robustness of output from computerized image analysis workflows will become increasingly critical to extracting useful quantitative information from tissue images. Our work demonstrates the feasibility of machine‑learning‑based semi-automated techniques to assist researchers and algorithm developers in this process.},
  number = {1}
}

@article{Pan2017-tf,
  title = {Accurate Segmentation of Nuclei in Pathological Images via Sparse Reconstruction and Deep Convolutional Networks},
  author = {Pan, Xipeng and Li, Lingqiao and Yang, Huihua and Liu, Zhenbing and Yang, Jinxin and Zhao, Lingling and Fan, Yongxian},
  date = {2017-03},
  journaltitle = {Neurocomputing},
  volume = {229},
  pages = {88--99},
  publisher = {{Elsevier}},
  abstract = {Abstract Automated cell segmentation is a critical step for computer assisted pathology related image analysis, such as automated grading of breast cancer tissue specimens. However, automated cell segmentation is complicated by (1) complexity of the data (possibly touching cells, stains, background clutters, and image artifacts) and (2) the variability in size, shape, appearance, and texture of the individual nuclei. Recently, there has been a growing interest in the application of “Deep Learning” strategies for the analysis of natural and pathological images. Histopathology, given its diversity and complexity, represents an excellent use case for application of deep learning strategies. In this paper, we put forward an automated nuclei segmentation method that works with hematoxylin and eosin (H\&E) stained breast cancer histopathology images, which represent regions of whole digital slides. The procedure can be divided into three main stages. Initially, the sparse reconstruction method is employed to roughly remove the background and accentuate the nuclei of pathological images. Then, deep convolutional networks (DCN), cascaded by multi-layer convolution networks, are trained using gradient descent techniques to efficiently segment the cell nuclei from the background. In this stage, input patches and its corresponding labels are randomly sampled from the pathological images and fed to the training networks. The size of the sampled patches can be flexible, and the proposed method is robust when the times of sampling and the number of feature maps vary in a wide range. Finally, morphological operations and some prior knowledge are introduced to improve the segmentation performance and reduce the errors. Our method achieves about 92.45\% pixel-wise segmentation accuracy and the F1-measure is 0.8393. This result leads to a promising segmentation performance, equivalent and sometimes surpassing recently published leading alternative segmentation methods with the same benchmark datasets.},
  keywords = {Deep convolutional networks,Nuclei segmentation}
}

@article{Pan2019,
  title = {Recent Progress on Generative Adversarial Networks ({{GANs}}): {{A}} Survey},
  author = {Pan, Zhaoqing and Yu, Weijie and Yi, Xiaokai and Khan, Asifullah and Yuan, Feng and Zheng, Yuhui},
  date = {2019},
  journaltitle = {IEEE Access},
  volume = {7},
  pages = {36322--36333},
  publisher = {{IEEE}},
  issn = {21693536},
  doi = {10.1109/ACCESS.2019.2905015},
  abstract = {GANのアーキテクチャ、正則化などの学習方法、応用タスクに注目したサーベイ。評価指標についても簡単な紹介がある。 大量発生しているGANsを区別して理解するのに有用。},
  keywords = {Deep learning,generative adversarial networks,machine learning,unsupervised learning}
}

@article{Pantanowitz2010,
  title = {Digital Images and the Future of Digital Pathology.},
  author = {Pantanowitz, Liron},
  date = {2010-08},
  journaltitle = {Journal of pathology informatics},
  volume = {1},
  issn = {2153-3539 (Electronic)},
  doi = {10.4103/2153-3539.68332},
  eprint = {20922032},
  eprinttype = {pmid},
  langid = {english}
}

@inproceedings{Parkhi2015,
  title = {Deep Face Recognition},
  booktitle = {Procedings of the British Machine Vision Conference 2015},
  author = {Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew},
  date = {2015},
  volume = {1},
  pages = {41.1--41.12},
  doi = {10.5244/C.29.41},
  url = {http://www.bmva.org/bmvc/2015/papers/paper041/index.html},
  abstract = {The goal of this paper is face recognition – from either a single photograph or from a set of faces tracked in a video. Recent progress in this area has been due to two factors: (i) end to end learning for the task using a convolutional neural network (CNN), and (ii) the availability of very large scale training datasets. We make two contributions: first, we show how a very large scale dataset (2.6M im- ages, over 2.6K people) can be assembled by a combination of automation and human in the loop, and discuss the trade off between data purity and time; second, we traverse through the complexities of deep network training and face recognition to present meth- ods and procedures to achieve comparable state of the art results on the standard LFW and YTF face benchmarks.},
  isbn = {1-901725-53-7}
}

@inproceedings{paszke2017automatic,
  title = {Automatic Differentiation in {{PyTorch}}},
  booktitle = {{{NIPS}}-{{W}}},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  date = {2017}
}

@incollection{Patil1988,
  title = {Artificial Intelligence Techniques for Diagnostic Reasoning in Medicine},
  booktitle = {Exploring Artificial Intelligence},
  author = {Patil, Ramesh S.},
  date = {1988},
  pages = {347--379},
  doi = {10.1016/b978-0-934613-67-5.50013-7},
  url = {https://www.sciencedirect.com/science/article/pii/B9780934613675500137}
}

@incollection{Patterson2016,
  title = {Tumors of the Epidermis},
  booktitle = {Weedon's Skin Pathology},
  author = {Patterson, James W},
  editor = {Patterson MD, FACP, FAAD, James W},
  date = {2016},
  pages = {783--835.e29},
  doi = {http://dx.doi.org/10.1016/B978-0-7020-5183-8.00031-X},
  url = {https://www.clinicalkey.com.au/\{#\}!/content/3-s2.0-B978070205183800031X},
  isbn = {978-0-7020-5183-8 BT - Weedon's Skin Pathology}
}

@article{Pearce2019,
  title = {Artificial Intelligence and the Clinical World: A View from the Front Line},
  author = {Pearce, Christopher and McLeod, Adam and Rinehart, Natalie and Whyte, Robin and Deveny, Elizabeth and Shearer, Marianne},
  date = {2019-04},
  journaltitle = {Medical Journal of Australia},
  volume = {210},
  pages = {S38--S40},
  publisher = {{John Wiley and Sons Inc.}},
  issn = {13265377},
  doi = {10.5694/mja2.50025},
  keywords = {Computing methodologies,Datasets as topic,Emergency services medical,General practice,Risk management},
  number = {S6}
}

@article{Pedregosa2011,
  title = {Scikit-Learn: {{Machine}} Learning in Python},
  author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent},
  date = {2011},
  journaltitle = {Journal of machine learning research},
  volume = {12},
  pages = {2825--2830},
  issue = {Oct}
}

@article{Pell2019,
  title = {The Use of Digital Pathology and Image Analysis in Clinical Trials},
  author = {Pell, Robert and Oien, Karin and Robinson, Max and Pitman, Helen and Rajpoot, Nasir and Rittscher, Jens and Snead, David and Verrill, Clare and Driskell, Owen J. and Hall, Andy and James, Jacqueline and Jones, Louise J. and Craig, Clare and Sloan, Philip and Thomas, Gareth J. and Elliott, Philip and Cheang, Maggie and Rodriguez-Justo, Manuel and Rees, Gabrielle and Salto-Tellez, Manuel and West, Nicholas P. and Mirabile, Ilaria and Howlett, Emily and Stevenson, Laura and da Silva, Maria and Hartridge-Lambert, Sidonie and Beecham, Joseph M. and Traub, Stephanie and Katugampola, Sidath and Blagden, Sarah and Morden, James},
  date = {2019-04},
  journaltitle = {Journal of Pathology: Clinical Research},
  volume = {5},
  pages = {81--90},
  publisher = {{Wiley-Blackwell Publishing Ltd}},
  issn = {20564538},
  doi = {10.1002/cjp2.127},
  abstract = {Digital pathology and image analysis potentially provide greater accuracy, reproducibility and standardisation of pathology-based trial entry criteria and endpoints, alongside extracting new insights from both existing and novel features. Image analysis has great potential to identify, extract and quantify features in greater detail in comparison to pathologist assessment, which may produce improved prediction models or perform tasks beyond manual capability. In this article, we provide an overview of the utility of such technologies in clinical trials and provide a discussion of the potential applications, current challenges, limitations and remaining unanswered questions that require addressing prior to routine adoption in such studies. We reiterate the value of central review of pathology in clinical trials, and discuss inherent logistical, cost and performance advantages of using a digital approach. The current and emerging regulatory landscape is outlined. The role of digital platforms and remote learning to improve the training and performance of clinical trial pathologists is discussed. The impact of image analysis on quantitative tissue morphometrics in key areas such as standardisation of immunohistochemical stain interpretation, assessment of tumour cellularity prior to molecular analytical applications and the assessment of novel histological features is described. The standardisation of digital image production, establishment of criteria for digital pathology use in pre-clinical and clinical studies, establishment of performance criteria for image analysis algorithms and liaison with regulatory bodies to facilitate incorporation of image analysis applications into clinical practice are key issues to be addressed to improve digital pathology incorporation into clinical trials.},
  keywords = {computerised image analysis,digital image analysis,digital microscopy},
  number = {2},
  options = {useprefix=true}
}

@article{Perkins2018,
  title = {Precision Medicine Screening Using Whole-Genome Sequencing and Advanced Imaging to Identify Disease Risk in Adults},
  author = {Perkins, Bradley A. and Caskey, C. Thomas and Brar, Pamila and Dec, Eric and Karow, David S. and Kahn, Andrew M. and Hou, Ying-Chen Claire and Shah, Naisha and Boeldt, Debbie and Coughlin, Erin and Hands, Gabby and Lavrenko, Victor and Yu, James and Procko, Andrea and Appis, Julia and Dale, Anders M. and Guo, Lining and Jönsson, Thomas J. and Wittmann, Bryan M. and Bartha, Istvan and Ramakrishnan, Smriti and Bernal, Axel and Brewer, James B. and Brewerton, Suzanne and Biggs, William H. and Turpaz, Yaron and Venter, J. Craig},
  date = {2018},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {115},
  pages = {3686--3691},
  issn = {0027-8424},
  doi = {10.1073/pnas.1706096114},
  abstract = {Reducing premature mortality associated with age-related chronic diseases, such as cancer and cardiovascular disease, is an urgent priority. We report early results using genomics in combination with advanced imaging and other clinical testing to proactively screen for age-related chronic disease risk among adults. We enrolled active, symptom-free adults in a study of screening for age-related chronic diseases associated with premature mortality. In addition to personal and family medical history and other clinical testing, we obtained whole-genome sequencing (WGS), noncontrast whole-body MRI, dual-energy X-ray absorptiometry (DXA), global metabolomics, a new blood test for prediabetes (Quantose IR), echocardiography (ECHO), ECG, and cardiac rhythm monitoring to identify age-related chronic disease risks. Precision medicine screening using WGS and advanced imaging along with other testing among active, symptom-free adults identified a broad set of complementary age-related chronic disease risks associated with premature mortality and strengthened WGS variant interpretation. This and other similarly designed screening approaches anchored by WGS and advanced imaging may have the potential to extend healthy life among active adults through improved prevention and early detection of age-related chronic diseases (and their risk factors) associated with premature mortality.},
  number = {14}
}

@article{Phulari2018-zd,
  title = {Cutaneous Horn: {{A}} Mask to Underlying Malignancy},
  author = {Phulari, Rashmi Gs and Rathore, Rajendrasinh and Talegaon, Trupti Pramod and Shah, Arpan},
  date = {2018-01},
  journaltitle = {J. Oral Maxillofac. Pathol.},
  volume = {22},
  pages = {S87----S90},
  abstract = {Cutaneous horns (cornu cutaneum) are uncommon lesions consisting of keratotic material resembling that of an animal horn. It is a conical- or cylindrical-shaped excessive hyperkeratosis of variable size ranging from few millimeters to several centimeters with a variable in size and shape, such as cylindrical, conical, pointed, transversely or longitudinally corrugated, or curved like a ram's horn. The lesions typically occur in sun-exposed areas, particularly the face, ear, nose, forearms and dorsum of hands. Even though 60\% of the cutaneous horns are benign in nature, the possibility of skin cancer should always be kept in mind. The clinical diagnosis includes various benign and malignant lesions at its base. Lesions associated with cutaneous horn are keratosis, sebaceous molluscum, verruca, trichilemmal, Bowen's disease, epidermoid carcinoma, malignant melanoma and basal cell carcinoma. Herewith, we report a case of cutaneous horn on the upper lip vermillion masking the underlying malignancy at its base.},
  issue = {Suppl 1},
  keywords = {Base of the lesion,cutaneous horn,verrucous carc}
}

@online{Pidhorskyi2020,
  title = {Adversarial Latent Autoencoders},
  author = {Pidhorskyi, Stanislav and Adjeroh, Donald A. and Doretto, Gianfranco},
  date = {2020},
  pages = {14092--14101},
  doi = {10.1109/cvpr42600.2020.01411},
  abstract = {Autoencoder networks are unsupervised approaches aiming at combining generative and representational properties by learning simultaneously an encoder-generator map. Although studied extensively, the issues of whether they have the same generative power of GANs, or learn disentangled representations, have not been fully addressed. We introduce an autoencoder that tackles these issues jointly, which we call Adversarial Latent Autoencoder (ALAE). It is a general architecture that can leverage recent improvements on GAN training procedures. We designed two autoencoders: one based on a MLP encoder, and another based on a StyleGAN generator, which we call StyleALAE. We verify the disentanglement properties of both architectures. We show that StyleALAE can not only generate 1024x1024 face images with comparable quality of StyleGAN, but at the same resolution can also produce face reconstructions and manipulations based on real images. This makes ALAE the first autoencoder able to compare with, and go beyond the capabilities of a generator-only type of architecture.},
  archivePrefix = {arXiv},
  arxivid = {2004.04467},
  eprint = {2004.04467},
  eprinttype = {arxiv},
  number = {C}
}

@inproceedings{pidhorskyi2020adversarial,
  title = {Adversarial Latent Autoencoders},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Pidhorskyi, Stanislav and Adjeroh, Donald A and Doretto, Gianfranco},
  date = {2020},
  pages = {14104--14113}
}

@book{Popper2005,
  title = {The Logic of Scientific Discovery},
  author = {Popper, Karl},
  date = {2005},
  publisher = {{Routledge}},
  isbn = {0-203-99462-0}
}

@article{PressAssociation2018,
  title = {{{AI}} Revolution 'at Risk of Being Stifled in {{UK}} by Fear-Driven Backlash'},
  author = {{Press Association}},
  date = {2018-09},
  journaltitle = {The guardian},
  url = {https://www.theguardian.com/technology/2018/sep/06/ai-revolution-at-risk-of-being-stifled-in-uk-by-fear-driven-backlash}
}

@online{Qi2017-po,
  title = {Global and Local Information Based Deep Network for Skin Lesion Segmentation},
  author = {Qi, Jin and Le, Miao and Li, Chunming and Zhou, Ping},
  date = {2017-03},
  abstract = {With a large influx of dermoscopy images and a growing shortage of dermatologists, automatic dermoscopic image analysis plays an essential role in skin cancer diagnosis. In this paper, a new deep fully convolutional neural network (FCNN) is proposed to automatically segment melanoma out of skin images by end-to-end learning with only pixels and labels as inputs. Our proposed FCNN is capable of using both local and global information to segment melanoma by adopting skipping layers. The public benchmark database consisting of 150 validation images, 600 test images and 2000 training images in the melanoma detection challenge 2017 at International Symposium Biomedical Imaging 2017 is used to test the performance of our algorithm. All large size images (for example, \$40006000\$ pixels) are reduced to much smaller images with \$384384\$ pixels (more than 10 times smaller). We got and submitted preliminary results to the challenge without any pre or post processing. The performance of our proposed method could be further improved by data augmentation and by avoiding image size reduction.},
  archivePrefix = {arXiv},
  arxivid = {cs.CV/1703.05467},
  eprint = {1703.05467},
  eprinttype = {arxiv},
  primaryClass = {cs.CV}
}

@online{Quiros2019,
  title = {Pathology {{GAN}}: Learning Deep Representations of Cancer Tissue},
  author = {Quiros, Adalberto Claudio and Murray-Smith, Roderick and Yuan, Ke},
  date = {2019},
  archivePrefix = {arXiv},
  eprint = {1907.02644},
  eprinttype = {arxiv}
}

@online{Quiros2020,
  title = {Learning a Low Dimensional Manifold of Real Cancer Tissue with {{PathologyGAN}}},
  author = {Quiros, Adalberto Claudio and Murray-Smith, Roderick and Yuan, Ke},
  date = {2020},
  pages = {1--24},
  url = {http://arxiv.org/abs/2004.06517},
  abstract = {Application of deep learning in digital pathology shows promise on improving disease diagnosis and understanding. We present a deep generative model that learns to simulate high-fidelity cancer tissue images while mapping the real images onto an interpretable low dimensional latent space. The key to the model is an encoder trained by a previously developed generative adversarial network, PathologyGAN. We study the latent space using 249K images from two breast cancer cohorts. We find that the latent space encodes morphological characteristics of tissues (e.g. patterns of cancer, lymphocytes, and stromal cells). In addition, the latent space reveals distinctly enriched clusters of tissue architectures in the high-risk patient group.},
  archivePrefix = {arXiv},
  arxivid = {2004.06517},
  eprint = {2004.06517},
  eprinttype = {arxiv},
  keywords = {digital pathology,generative adversarial networks}
}

@article{Rad2020,
  title = {Trophectoderm Segmentation in Human Embryo Images via Inceptioned {{U}}-{{Net}}},
  author = {Rad, Reza Moradi and Saeedi, Parvaneh and Au, Jason and Havelock, Jon},
  date = {2020},
  journaltitle = {Medical Image Analysis},
  volume = {62},
  pages = {101612},
  publisher = {{Elsevier}},
  issn = {1361-8415}
}

@inproceedings{Raina,
  title = {Large-Scale Deep Unsupervised Learning Using Graphics Processors},
  booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
  author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y},
  pages = {873--880},
  publisher = {{ACM}},
  isbn = {1-60558-516-5}
}

@online{Rajpurkar2017,
  title = {{{CheXNet}}: {{Radiologist}}-Level Pneumonia Detection on Chest x-Rays with Deep Learning},
  author = {Rajpurkar, Pranav and Irvin, Jeremy and Zhu, Kaylie and Yang, Brandon and Mehta, Hershel and Duan, Tony and Ding, Daisy and Bagul, Aarti and Langlotz, Curtis and Shpanskaya, Katie},
  date = {2017},
  archivePrefix = {arXiv},
  eprint = {1711.05225},
  eprinttype = {arxiv}
}

@online{Rakin2018,
  title = {Parametric Noise Injection: {{Trainable}} Randomness to Improve Deep Neural Network Robustness against Adversarial Attack},
  author = {Rakin, Adnan Siraj and He, Zhezhi and Fan, Deliang},
  date = {2018},
  archivePrefix = {arXiv},
  eprint = {1811.09310},
  eprinttype = {arxiv}
}

@book{Rapini2012-og,
  title = {Practical Dermatopathology},
  author = {Rapini, Ronald P},
  date = {2012},
  publisher = {{Elsevier Health Sciences}},
  abstract = {Quickly and confidently evaluate and diagnose skin biopsies with Practical Dermatopathology. Written from the perspective of both the histopathologist and clinician, this popular medical reference allows you to search by disease or pathologic feature to quickly locate key criteria and a differential diagnosis. Practical and portable, Practical Dermatopathology is your ideal high-yield microscope companion! Make accurate, complete reports by fully understanding clinical correlations. Accurately diagnose a wide range of conditions using over 700 full-color illustrations, with leader lines pointing out key pathologic and clinical features as they would be seen in daily practice.Find critical information quickly through extensive cross-referencing of differential diagnosis lists by finding and disease. Clinical lists are based on location, symptoms, duration, arrangement, morphology, and color. Pathologic lists include major categories such as epidermal changes, dermal changes, and cell types.Gauge your mastery of the material with online multiple-choice review questions that provide an ideal study resource for board review or recertification.Stay current with comprehensive updates throughout that include the latest advancements in the diagnosis of lymphoma as well as unique differential diagnosis lists, full-color pathology images, supporting clinical photographs, and multiple choice questions.Access the complete contents online - fully searchable, with downloadable image library, bonus clinical correlation images, review questions, and more, at www.expertconsult.com.}
}

@report{RCGP2018,
  title = {Artificial Intelligence and Primary Care},
  author = {(RCGP), Royal College of General Practitioners},
  date = {2018},
  pages = {22},
  institution = {{Royal College of General Practitioners}},
  url = {www.rcgp.org.uk}
}

@online{Real2017,
  title = {Large-Scale Evolution of Image Classifiers},
  author = {Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc and Kurakin, Alex},
  date = {2017-03},
  url = {http://arxiv.org/abs/1703.01041},
  abstract = {Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6\% (95.6\% for ensemble) and 77.0\%, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.},
  archivePrefix = {arXiv},
  arxivid = {1703.01041},
  eprint = {1703.01041},
  eprinttype = {arxiv}
}

@article{reggia1985answer,
  title = {Answer Justification in Diagnostic Expert Systems-{{Part I}}: {{Abductive}} Inference and Its Justification},
  author = {Reggia, James A and Perricone, Barry T and Nau, Dana S and Peng, Yun},
  date = {1985},
  journaltitle = {IEEE transactions on biomedical engineering},
  pages = {263--267},
  publisher = {{IEEE}},
  number = {4}
}

@report{Riedl,
  title = {On Design and Evaluation of Human-Centered Explainable {{AI}} Systems},
  author = {Riedl, Mark},
  url = {https://www.researchgate.net/publication/332683853},
  keywords = {algorithmic decision-making,Artificial Intelligence,Explainable AI,interpretability,Machine Learning,rationale generation,user perception}
}

@article{Riedl2019,
  title = {Human‐centered Artificial Intelligence and Machine Learning},
  author = {Riedl, Mark O.},
  date = {2019-01},
  journaltitle = {Human Behavior and Emerging Technologies},
  volume = {1},
  pages = {33--36},
  publisher = {{Wiley}},
  issn = {2578-1863},
  doi = {10.1002/hbe2.117},
  abstract = {Humans are increasingly coming into contact with artificial intelligence and machine learning systems. Human-centered artificial intelligence is a perspective on AI and ML that algorithms must be designed with awareness that they are part of a larger system consisting of humans. We lay forth an argument that human-centered artificial intelligence can be broken down into two aspects: (1) AI systems that understand humans from a sociocultural perspective, and (2) AI systems that help humans understand them. We further argue that issues of social responsibility such as fairness, accountability, interpretability, and transparency.},
  number = {1}
}

@thesis{roberts1963machine,
  title = {Machine Perception of Three-Dimensional Solids},
  author = {Roberts, Lawrence G},
  date = {1963},
  institution = {{Massachusetts Institute of Technology}}
}

@article{Rodenburg2016,
  title = {Deep Learning in Histopathology},
  author = {Rodenburg, Bram},
  date = {2016}
}

@article{Rogers2015,
  title = {Incidence Estimate of Nonmelanoma Skin Cancer (Keratinocyte Carcinomas) in the {{U}}.{{S}}. Population, 2012.},
  author = {Rogers, Howard W and Weinstock, Martin A and Feldman, Steven R and Coldiron, Brett M},
  date = {2015-10},
  journaltitle = {JAMA dermatology},
  volume = {151},
  pages = {1081--1086},
  issn = {2168-6084 (Electronic)},
  doi = {10.1001/jamadermatol.2015.1187},
  abstract = {IMPORTANCE: Understanding skin cancer incidence is critical for planning prevention and treatment strategies and allocating medical resources. However, owing to lack of national reporting and previously nonspecific diagnosis classification, accurate measurement of the US incidence of nonmelanoma skin cancer (NMSC) has been difficult. OBJECTIVE: To estimate the incidence of NMSC (keratinocyte carcinomas) in the US population in 2012 and the incidence of basal cell carcinoma (BCC) and squamous cell carcinoma (SCC) in the 2012 Medicare fee-for-service population. DESIGN, SETTING, AND PARTICIPANTS: This study analyzes US government administrative data including the Centers for Medicare \& Medicaid Services Physicians Claims databases to calculate totals of skin cancer procedures performed for Medicare beneficiaries from 2006 through 2012 and related parameters. The population-based National Ambulatory Medical Care Survey database was used to estimate NMSC-related office visits for 2012. We combined these analyses to estimate totals of new skin cancer diagnoses and affected individuals in the overall US population. MAIN OUTCOMES AND MEASURES: Incidence of NMSC in the US population in 2012 and BCC and SCC in the 2012 Medicare fee-for-service population. RESULTS: The total number of procedures for skin cancer in the Medicare fee-for-service population increased by 13\% from 2,048,517 in 2006 to 2,321,058 in 2012. The age-adjusted skin cancer procedure rate per 100,000 beneficiaries increased from 6075 in 2006 to 7320 in 2012. The number of procedures in Medicare beneficiaries specific for NMSC increased by 14\% from 1,918,340 in 2006 to 2,191,100 in 2012. The number of persons with at least 1 procedure for NMSC increased by 14\% (from 1,177,618 to 1,336,800) from 2006 through 2012. In the 2012 Medicare fee-for-service population, the age-adjusted procedure rate for BCC and SCC were 3280 and 3278 per 100,000 beneficiaries, respectively. The ratio of BCC to SCC treated in Medicare beneficiaries was 1.0. We estimate the total number of NMSCs in the US population in 2012 at 5,434,193 and the total number of persons in the United States treated for NMSC at 3,315,554. CONCLUSIONS AND RELEVANCE: This study is a thorough nationwide estimate of the incidence of NMSC and provides evidence of continued increases in numbers of skin cancer diagnoses and affected patients in the United States. This study also demonstrates equal incidence rates for BCC and SCC in the Medicare population.},
  eprint = {25928283},
  eprinttype = {pmid},
  keywords = {80 and over,Aged,Basal Cell,Carcinoma,Databases,epidemiology,Factual,Female,Health Care Surveys,Humans,Incidence,Keratinocytes,Male,Medicare,Middle Aged,pathology,Skin Neoplasms,Squamous Cell,United States},
  langid = {english},
  number = {10}
}

@inproceedings{Romo-Bucheli2015-hy,
  title = {Identifying Histological Concepts on Basal Cell Carcinoma Images Using Nuclei Based Sampling and Multi-Scale Descriptors},
  booktitle = {2015 \{\vphantom\}{{IEEE}}\vphantom\{\} 12th International Symposium on Biomedical Imaging (\{\vphantom\}{{ISBI}}\vphantom\{\})},
  author = {Romo-Bucheli, D and Moncayo, R and Cruz-Roa, A and Romero, E},
  date = {2015-04},
  pages = {1008--1011},
  abstract = {Histopathological sample examination involves a sequential analysis of several fields of view (FoV) at different magnification levels. Experts integrate this information by implicitly fusing morphometric and spatial features, mainly related with cell appearance, spatial distribution and organization. By performing this analysis a pathologist recognizes several micro structures such as follicle, epidermis, carcinoma and eccrine glands in basal skin tissue samples. In this article we present a new approach to histopathology classification using a multi-scale nuclei descriptor, located at a set of detected nuclei and constructed as a multiresolution pyramid. The method was evaluated in a multiclass challenging problem, i.e, identifying epidermis, hair follicle, eccrine glands and nodular carcinoma in 240 histopathology images of basal cell carcinoma. The experimental results show an average Area Under the ROC Curve (AUC) of 0.93 in a 6-fold cross-validation for the set of four classes.},
  keywords = {biomedical optical imaging,cancer,cellular biophys}
}

@inproceedings{Ronneberger2015,
  title = {U-Net: {{Convolutional}} Networks for Biomedical Image Segmentation},
  booktitle = {Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  date = {2015},
  volume = {9351},
  pages = {234--241},
  publisher = {{Springer Verlag}},
  issn = {16113349},
  doi = {10.1007/978-3-319-24574-4_28},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  isbn = {978-3-319-24573-7}
}

@article{Rosado2003,
  title = {Accuracy of Computer Diagnosis of Melanoma: A Quantitative Meta-Analysis},
  author = {Rosado, Barbara and Menzies, Scott and Harbauer, Alexandra and Pehamberger, Hubert and Wolff, Klaus and Binder, Michael and Kittler, Harald},
  date = {2003},
  journaltitle = {Archives of Dermatology},
  volume = {139},
  pages = {361--367},
  publisher = {{American Medical Association}},
  issn = {0003-987X},
  number = {3}
}

@article{Rosenbaum2017,
  title = {Computer-Assisted Measurement of Primary Tumor Area Is Prognostic of Recurrence-Free Survival in Stage {{IB}} Melanoma Patients.},
  author = {Rosenbaum, Brooke E and Schafer, Christine N and Han, Sung Won and Osman, Iman and Zhong, Hua and Brinster, Nooshin},
  date = {2017-10},
  journaltitle = {Modern pathology : an official journal of the United States and Canadian Academy of Pathology, Inc},
  volume = {30},
  pages = {1402--1410},
  issn = {1530-0285 (Electronic)},
  doi = {10.1038/modpathol.2017.64},
  abstract = {Current staging guidelines are insufficient to predict which patients with thin primary melanoma are at high risk of recurrence. Computer-assisted image analysis may allow for more practical and objective histopathological analysis of primary tumors than traditional light microscopy. We studied a prospective cohort of stage IB melanoma patients treated at NYU Langone Medical Center from 2002 to 2014. Primary tumor width, manual area, digital area, and conformation were evaluated in a patient subset via computer-assisted image analysis. The associations between histologic variables and survival were evaluated using Cox proportional hazards model. Logistic regressions were used to build a classifier with clinicopathological characteristics to predict recurrence status. Of the 655 patients with stage IB melanoma studied, a subset of 149 patient tumors (63 recurred, 86 did not recur) underwent computer-assisted histopathological analysis. Increasing tumor width (hazard ratios (HR): 1.17, P=0.01) and digital area (HR: 1.08, P{$<$}0.01) were significantly associated with worse recurrence-free survival, whereas non-contiguous conformation (HR: 0.57, P=0.05) was significantly associated with better recurrence-free survival. The novel histopathological classifier composed of digital area, conformation, and baseline variables effectively distinguished recurrent cases from non-recurrent cases (AUC: 0.733, 95\% confidence interval (CI): 0.647-0.818), compared to the baseline classifier alone (AUC: 0.635, 95\% CI: 0.545-0.724). Primary tumor cross-sectional area, width, and conformation measured via computer-assisted analysis may help identify high-risk patients with stage IB melanoma.},
  eprint = {28731044},
  eprinttype = {pmid},
  keywords = {Adult,Aged,Computer-Assisted,Disease-Free Survival,Female,Humans,Image Interpretation,Kaplan-Meier Estimate,Local,Male,Melanoma,methods,Middle Aged,mortality,Neoplasm Recurrence,pathology,Prognosis,Proportional Hazards Models,Skin Neoplasms},
  langid = {english},
  number = {10}
}

@book{rosenblatt1957perceptron,
  title = {The Perceptron, a Perceiving and Recognizing Automaton {{Project Para}}},
  author = {Rosenblatt, Frank},
  date = {1957},
  publisher = {{Cornell Aeronautical Laboratory}}
}

@article{Ross2018,
  title = {{{IBM}}'s {{Watson}} Supercomputer Recommended ‘unsafe and Incorrect' Cancer Treatments, Internal Documents Show},
  author = {Ross, Casey},
  date = {2018},
  url = {https://www.statnews.com/2018/07/25/ibm-watson-recommended-unsafe-incorrect-treatments/},
  urldate = {2019-09-09}
}

@article{Roth2016,
  title = {Improving Computer-Aided Detection Using Convolutional Neural Networks and Random View Aggregation},
  author = {Roth, Holger R and Lu, Le and Liu, Jiamin and Yao, Jianhua and Seff, Ari and Cherry, Kevin and Kim, Lauren and Summers, Ronald M},
  date = {2016},
  journaltitle = {IEEE transactions on medical imaging},
  volume = {35},
  pages = {1170--1181},
  issn = {0278-0062},
  number = {5}
}

@book{RoyalCollegeofPathologistsofAustralasia2014,
  title = {Primary Cutaneous Melanoma Structured Areporting Protocol},
  author = {{Royal College of Pathologists of Australasia}},
  date = {2014},
  edition = {2},
  location = {{Surry Hills, NSW, AUS}},
  url = {https://www.rcpa.edu.au/getattachment/9f61b036-a56c-4f42-bf89-bffed98b5494/Protocol-primary-cutaneous-melanoma.aspx},
  isbn = {978-1-74187-713-7}
}

@article{RoyalCollegeofPathologistsofAustralia2018,
  title = {Australian Pathologist Workforce Study 2018},
  author = {{Royal College of Pathologists of Australia}},
  date = {2018},
  pages = {1--9},
  url = {https://www.rcpa.edu.au/getattachment/4a38b4f9-5f6a-45eb-8947-dfa072797685/APW.aspx}
}

@article{Rudin2019,
  title = {Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead},
  author = {Rudin, Cynthia},
  date = {2019},
  journaltitle = {Nature Machine Intelligence},
  volume = {1},
  pages = {206--215},
  publisher = {{Springer US}},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0048-x},
  url = {http://dx.doi.org/10.1038/s42256-019-0048-x},
  abstract = {Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision.},
  isbn = {4225601900},
  number = {5}
}

@article{rudinStopExplainingBlack2019,
  title = {Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead},
  author = {Rudin, Cynthia},
  date = {2019-05-01},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nature Machine Intelligence},
  volume = {1},
  pages = {206--215},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0048-x},
  url = {https://doi.org/10.1038/s42256-019-0048-x},
  abstract = {Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision.},
  number = {5}
}

@article{Russakovsky2015,
  title = {Imagenet Large Scale Visual Recognition Challenge},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael},
  date = {2015},
  journaltitle = {International Journal of Computer Vision},
  volume = {115},
  pages = {211--252},
  issn = {0920-5691},
  number = {3}
}

@article{RUTKIN201618,
  title = {Digital Discrimination},
  author = {Rutkin, Aviva},
  date = {2016},
  journaltitle = {New Scientist},
  volume = {231},
  pages = {18--19},
  issn = {0262-4079},
  doi = {10.1016/S0262-4079(16)31364-1},
  url = {http://www.sciencedirect.com/science/article/pii/S0262407916313641},
  abstract = {The fight back against biased software starts now, says Aviva Rutkin},
  number = {3084}
}

@article{Saldanha2003,
  title = {Basal Cell Carcinoma: A Dermatopathological and Molecular Biological Update.},
  author = {Saldanha, G and Fletcher, A and Slater, D N},
  date = {2003-02},
  journaltitle = {The British journal of dermatology},
  volume = {148},
  pages = {195--202},
  issn = {0007-0963 (Print)},
  doi = {10.1046/j.1365-2133.2003.05151.x},
  abstract = {The ideal classification of basal cell carcinoma (BCC) should be able to identify subtypes which correlate with clinical behaviour and treatment requirements. Unfortunately, however, such a classification has yet to be defined. In the interim, the currently most favoured classification is one based predominantly on histological growth pattern. This classification contributes to the useful concept of low- and high-risk histological subtypes of BCC. The latter are characterized by an increased probability of subclinical extension and/or incomplete excision and/or aggressive local invasive behaviour and/or local recurrence. The Royal College of Pathologists has published a minimum dataset for the histopathological reporting of BCC and this has been written to be compatible with the British Association of Dermatologists' management guidelines. Growth patterns to be reported include nodular, superficial, infiltrative/morphoeic and micronodular types, together with differentiation when of severely atypical or malignant squamous type (basosquamous carcinoma). Deep and peripheral excision margins will be reported to be either involved or clear. The latter will include a comment of a clearance of less than 1 mm for close margins and a measured distance in whole millimetres for other excisions. Clinical assessment and histology remain the 'gold standard' for evaluating BCC and cancers in general. However, in the postgenomic era emphasis is changing from the gathering and archiving of genomic data to its analysis and use in guiding clinical practice. In this context, a current goal is to define cancer phenotype in terms of molecular abnormalities and use this as a new gold standard. One way to assess whether this goal is being achieved for BCC is to determine whether our knowledge of its molecular pathology has any relevance to the minimum dataset for histological reporting. Knowledge of BCC molecular pathology has been fuelled by the recent discovery that deregulation of the Hedgehog (Hh) signalling pathway, a key player in embryonic patterning, appears to be fundamental to tumour growth. But despite accrual of a large amount of data concerning Hh pathway molecular alterations in neoplasia, little is known about the functional consequences of these changes in BCC, how they lead to tumour development, or how they relate to non-Hh pathway alterations such as TP53 mutation. Recent work suggests that the cellular localization of beta-catenin gives a degree of credence to the growth pattern classification of BCC. Furthermore, it is possible that beta-catenin may have a pathogenetic role in the invasive behaviour of BCC. This review draws on current evidence to discuss these issues and assess whether they are relevant to the minimum dataset.},
  eprint = {12588368},
  eprinttype = {pmid},
  keywords = {analysis,Basal Cell,beta Catenin,Carcinoma,Cell Transformation,classification,Cytoskeletal Proteins,Gene Expression Regulation,Genes,genetics,Hedgehog Proteins,Humans,Local,Mutation,Neoplasm Invasiveness,Neoplasm Recurrence,Neoplastic,p53,pathology,Signal Transduction,Skin Neoplasms,Trans-Activators},
  langid = {english},
  number = {2}
}

@inproceedings{Sandler2018,
  title = {Mobilenetv2: {{Inverted}} Residuals and Linear Bottlenecks},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  date = {2018},
  pages = {4510--4520}
}

@article{Schirrmeister2017,
  title = {Deep Learning with Convolutional Neural Networks for {{EEG}} Decoding and Visualization},
  author = {Schirrmeister, R T and Springenberg, J T and Fiederer, L D J and Glasstetter, M and Eggensperger, K and Tangermann, M and Hutter, F and Burgard, W and Ball, T},
  date = {2017},
  journaltitle = {Human Brain Mapping},
  shortjournal = {Hum Brain Mapp},
  volume = {38},
  pages = {5391--5420},
  issn = {1065-9471},
  doi = {10.1002/hbm.23730},
  number = {11}
}

@article{Schmidhuber2015,
  title = {Deep Learning in Neural Networks: {{An}} Overview},
  author = {Schmidhuber, Jürgen},
  date = {2015},
  journaltitle = {Neural networks},
  volume = {61},
  pages = {85--117},
  issn = {0893-6080}
}

@report{Schreiber2017,
  title = {Adversarial Learning as Co-Evolution (or Why and When Adversarial Learning Works?) What Is Adversarial Learning?},
  author = {Schreiber, Kfir},
  date = {2017},
  abstract = {Adversarial Learning is a sub-field of Machine Learning, where the learning model is trained with a rival model, mainly adaptive adversaries. Such models are highly attractive for AI security, generative models, and other machine learning tasks. In traditional machine learning, the model is trained, evaluated, and selected based on samples assumed the arrive from the same distribution. This assumption creates vulnerabilities to attacks from rational, adaptive attackers. Such attackers can design examples that intentionally violates the fixed distribution assumption and end in a model error. In his paper, "EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES," Ian J. Goodfellow shows an example of a classifier fooled to classify a panda bear just by adding such a small perturbation to the image that it's invisible to the human eye. These "machine optical illusions," that many times repeat over different models which were trained on the same training data, expose underlying vulnerabilities in our training process. Various explanations were suggested throughout the years, varying from nonlinearities and overfitting to the linear nature of neural networks. Whatever the cause might be, it is clear that the attacker is using the fixed distribution assumption against the model.}
}

@article{Schwartz1987,
  title = {Artificial Intelligence in Medicine},
  author = {Schwartz, William B. and Patil, Ramesh S. and Szolovits, Peter},
  date = {1987-03},
  journaltitle = {New England Journal of Medicine},
  volume = {316},
  pages = {685--688},
  issn = {0028-4793},
  doi = {10.1056/NEJM198703123161109},
  url = {http://www.nejm.org/doi/abs/10.1056/NEJM198703123161109},
  number = {11}
}

@inproceedings{Seff2015,
  title = {Leveraging Mid-Level Semantic Boundary Cues for Automated Lymph Node Detection},
  booktitle = {Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  author = {Seff, Ari and Lu, Le and Barbu, Adrian and Roth, Holger and Shin, Hoo Chang and Summers, Ronald M},
  date = {2015},
  volume = {9350},
  pages = {53--61},
  publisher = {{Springer}},
  issn = {16113349},
  doi = {10.1007/978-3-319-24571-3_7},
  abstract = {Histograms of oriented gradients (HOG) are widely employed image descriptors in modern computer-aided diagnosis systems. Built upon a set of local, robust statistics of low-level image gradients, HOG features are usually computed on raw intensity images. In this paper, we explore a learned image transformation scheme for producing higher-level inputs to HOG. Leveraging semantic object boundary cues, our methods compute data-driven image feature maps via a supervised boundary detector. Compared with the raw image map, boundary cues offer mid-level, more object-specific visual responses that can be suited for subsequent HOG encoding. We validate integrations of several image transformation maps with an application of computer-aided detection of lymph nodes on thoracoabdominal CT images. Our experiments demonstrate that semantic boundary cues based HOG descriptors complement and enrich the raw intensity alone. We observe an overall system with substantially improved results (∼ 78\% versus 60\% recall at 3 FP/volume for two target regions). The proposed system also moderately outperforms the state-of-the-art deep convolutional neural network (CNN) system in the mediastinum region, without relying on data augmentation and requiring significantly fewer training samples.},
  isbn = {978-3-319-24570-6}
}

@article{Segal2010,
  title = {Photography Consent and Related Legal Issues},
  author = {Segal, Jeffrey and Sacopulos, Michael J.},
  date = {2010-05},
  journaltitle = {Facial plastic surgery clinics of north america},
  volume = {18},
  pages = {237--244},
  issn = {10647406},
  doi = {10.1016/j.fsc.2010.01.003},
  abstract = {The use of photography is an integral part of any plastic surgery practice. Photographs are part of the patient's medical record and thus are covered by both federal and state privacy laws. Liability issues may arise when patients are photographed without their knowledge and consent. With proper written consent, practices may use " before" and " after" photographs of patients. However, some states have specific requirements as to the manner in which these photographs are taken and what claims may appear as text with the photographs. This article seeks to discuss legal issues associated with the use of photography in plastic surgery practices, and provides sample agreements to serve as a basis for addressing these issues. © 2010 Elsevier Inc.},
  keywords = {Aesthetic photography,Confidentiality,Copyright,Legal consent},
  number = {2}
}

@article{Seita2017,
  title = {Understanding Generative Adversarial Networks},
  author = {Seita, Daniel},
  date = {2017},
  journaltitle = {Danieltakeshi.Github.Io/},
  abstract = {Over the last few weeks, I've been learning more about some mysterious thing called Generative Adversarial Networks (GANs). GANs originally came out of a 2014 NIPS paper (read it here) and have had a remarkable impact on machine learning. I'm surprised that, until I was the TA for Berkeley's Deep Learning class last semester, I had never heard of GANs before.}
}

@online{Selvaraju2016,
  title = {Grad-{{CAM}}: {{Why}} Did You Say That?},
  author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
  date = {2016},
  archivePrefix = {arXiv},
  eprint = {1611.07450},
  eprinttype = {arxiv}
}

@inproceedings{Selvaraju2017,
  title = {Grad-Cam: {{Visual}} Explanations from Deep Networks via Gradient-Based Localization},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  date = {2017},
  pages = {618--626}
}

@report{SemanticSegmentationUsing2017,
  title = {Semantic Segmentation Using Fully Convolutional Networks over the Years Semantic Segmentation},
  date = {2017}
}

@article{Senaras2018,
  title = {{{DeepFocus}}: {{Detection}} of out-of-Focus Regions in Whole Slide Digital Images Using Deep Learning},
  author = {Senaras, Caglar and Niazi, M Khalid Khan and Lozanski, Gerard and Gurcan, Metin N},
  date = {2018},
  journaltitle = {PloS one},
  volume = {13},
  pages = {e0205387},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  number = {10}
}

@online{Sermanet2013-ih,
  title = {\{\vphantom\}{{OverFeat}}\vphantom\{\}: {{Integrated}} Recognition, Localization and Detection Using Convolutional Networks},
  author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
  date = {2013-12},
  abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
  archivePrefix = {arXiv},
  arxivid = {cs.CV/1312.6229},
  eprint = {1312.6229},
  eprinttype = {arxiv},
  primaryClass = {cs.CV}
}

@article{Sertel2009-wy,
  title = {Computer-Aided Prognosis of Neuroblastoma on Whole-Slide Images: {{Classification}} of Stromal Development},
  author = {Sertel, O and Kong, J and Shimada, H and Catalyurek, U V and Saltz, J H and Gurcan, M N},
  date = {2009-06},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognit.},
  volume = {42},
  pages = {1093--1103},
  abstract = {We are developing a computer-aided prognosis system for neuroblastoma (NB), a cancer of the nervous system and one of the most malignant tumors affecting children. Histopathological examination is an important stage for further treatment planning in routine clinical diagnosis of NB. According to the International Neuroblastoma Pathology Classification (the Shimada system), NB patients are classified into favorable and unfavorable histology based on the tissue morphology. In this study, we propose an image analysis system that operates on digitized H\&E stained whole-slide NB tissue samples and classifies each slide as either stroma-rich or stroma-poor based on the degree of Schwannian stromal development. Our statistical framework performs the classification based on texture features extracted using co-occurrence statistics and local binary patterns. Due to the high resolution of digitized whole-slide images, we propose a multi-resolution approach that mimics the evaluation of a pathologist such that the image analysis starts from the lowest resolution and switches to higher resolutions when necessary. We employ an offine feature selection step, which determines the most discriminative features at each resolution level during the training step. A modified k-nearest neighbor classifier is used to determine the confidence level of the classification to make the decision at a particular resolution level. The proposed approach was independently tested on 43 whole-slide samples and provided an overall classification accuracy of 88.4\%.},
  number = {6}
}

@article{Servan-Schreiber1986,
  title = {Artificial Intelligence and Psychiatry},
  author = {Servan-Schreiber, David},
  date = {1986},
  journaltitle = {Journal of nervous and mental disease},
  volume = {174},
  pages = {191--202},
  issn = {1539736X},
  doi = {10.1097/00005053-198604000-00001},
  abstract = {This paper provides a brief historical introduction to the new field of artificial intelligence and describes some applications to psychiatry. It focuses on two successful programs: a model of paranoid processes and an expert system for the pharmacological management of depressive disorders. Finally, it reviews evidence in favor of computerized psychotherapy and offers speculations on the future development of research in this area.},
  number = {4}
}

@online{Shafiee2017,
  title = {Fast {{YOLO}}: {{A}} Fast You Only Look Once System for Real-Time Embedded Object Detection in Video},
  author = {Shafiee, Mohammad Javad and Chywl, Brendan and Li, Francis and Wong, Alexander},
  date = {2017},
  archivePrefix = {arXiv},
  eprint = {1709.05943},
  eprinttype = {arxiv}
}

@article{Shapcott2019,
  title = {Deep Learning with Sampling in Colon Cancer Histology},
  author = {Shapcott, Mary and Hewitt, Katherine J. and Rajpoot, Nasir},
  date = {2019-03},
  journaltitle = {Frontiers in Bioengineering and Biotechnology},
  volume = {7},
  issn = {2296-4185},
  doi = {10.3389/fbioe.2019.00052},
  url = {https://www.frontiersin.org/article/10.3389/fbioe.2019.00052/full},
  abstract = {This study applied a deep learning cell identification algorithm to diagnostic images from the colon cancer repository at The Cancer Digital Archive (TCGA). Within-image sampling improved performance without loss of accuracy. The features thus derived were associated with various clinical variables including metastasis, venous and lymphatic invasion. The deep-learning algorithm was trained using images from a locally available data set, then applied to the TCGA images by tiling them, and identifying cells in each patch defined by the tiling. In this application the average number of patches containing tissue in an image was approximately nine hundred. Processing a random sample of patches greatly reduced computation costs. The cell identification algorithm was applied directly to each sampled patch, resulting in a list of cells. Each cell was labelled with its location and classification (‘epithelial', ‘inflammatory', ‘fibroblast' or ‘other'). The number of cells of a given type in the patch was calculated, resulting in a patch profile containing four features. A morphological profile that applied to the entire image was obtained by averaging profiles over all patches. Two sampling policies were examined. The first policy was random sampling which samples patches with uniform weighting. The second policy was systematic random sampling which takes spatial dependencies into account. Compared with processing of complete whole slide images there was a seven-fold improvement in performance when systematic random spatial sampling was used to select 100 tiles from the whole-slide image for processing, with very little loss of accuracy (approximately 4\% on average). We found links between the predicted features and clinical variables in the TCGA colon cancer data set. Several significant associations were found: increased fibroblast numbers were associated with the presence of metastasis, venous invasion, lymphatic invasion and residual tumour while decreased numbers of inflammatory cells were associated with mucinous carcinomas. Regarding the four different types of cell, deep learning has generated morphological features that are indicators of cell density. The features are related to cellularity, the numbers, degree or quality of cells present in a tumour.}
}

@article{Sharma2017,
  title = {Automatic Segmentation of Kidneys Using Deep Learning for Total Kidney Volume Quantification in Autosomal Dominant Polycystic Kidney Disease},
  author = {Sharma, Kanishka and Rupprecht, Christian and Caroli, Anna and Aparicio, Maria Carolina and Remuzzi, Andrea and Baust, Maximilian and Navab, Nassir},
  date = {2017},
  journaltitle = {Scientific Reports},
  volume = {7}
}

@article{Shen2017,
  title = {Deep Learning in Medical Image Analysis},
  author = {Shen, Dinggang and Wu, Guorong and Suk, Heung-Il},
  date = {2017},
  journaltitle = {Annual Review of Biomedical Engineering},
  issn = {1523-9829},
  number = {0}
}

@inproceedings{shen2020interpreting,
  title = {Interpreting the Latent Space of Gans for Semantic Face Editing},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Shen, Yujun and Gu, Jinjin and Tang, Xiaoou and Zhou, Bolei},
  date = {2020},
  pages = {9243--9252}
}

@article{Shih2017-fp,
  title = {Economic Evaluation of Future Skin Cancer Prevention in {{Australia}}},
  author = {Shih, Sophy Tf and Carter, Rob and Heward, Sue and Sinclair, Craig},
  date = {2017-06},
  journaltitle = {Preventive Medicine},
  shortjournal = {Prev. Med.},
  volume = {99},
  pages = {7--12},
  abstract = {Public health programs to reduce the significant burden of skin cancer have been implemented in Australia and around the world. The economic rationale for prevention needs to be kept up-to-date as relevant disease patterns, risk factors and expenditure patterns change through time. The aim of this study was to update and extend the economic credentials for skin cancer prevention in Australia. Economic evaluations were conducted in 2015 with multiple methods applied, including cost-effectiveness and cost-benefit analysis, multiple study perspectives ('societal', 'health sector', '3rd party funder') and counterfactual analysis sourced from cancer incidence between 1982 and 2011. Modelled outcomes included 'cases prevented', 'deaths averted' and 'health-adjusted life-years'. Cost-benefit Analysis, including productivity impacts in the general economy, was conducted. With an additional \$AUD 0.16 (\$USD 0.12) per capita investment into future skin cancer prevention across Australia, 140,000 skin cancer cases would be prevented over the 20year reference period (2011 to 2030). Depending on study perspective and method, the upgraded program is either dominant (achieving both health gains and cost offsets) or highly cost-effective (health gain at modest net cost). Return on investment (ROI) was \$AUD 3.20 per dollar invested, with net social benefit of \$AUD 1.43 billion. The study confirmed the strong economic credentials for skin cancer prevention and provided sound arguments for increased investment in Australia. The reference case analysis provides a useful benchmark for other countries to consider in the design and funding of their prevention programs.},
  keywords = {Cost-benefit analysis,Cost-effectiveness,Economi}
}

@article{Shin2013,
  title = {Stacked Autoencoders for Unsupervised Feature Learning and Multiple Organ Detection in a Pilot Study Using {{4D}} Patient Data},
  author = {Shin, Hoo-Chang and Orton, Matthew R and Collins, David J and Doran, Simon J and Leach, Martin O},
  date = {2013},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {35},
  pages = {1930--1943},
  issn = {0162-8828},
  number = {8}
}

@article{Shin2016-yc,
  title = {Deep Convolutional Neural Networks for \{\vphantom\}{{Computer}}-{{Aided}}\vphantom\{\} Detection: \{\vphantom\}{{CNN}}\vphantom\{\} Architectures, Dataset Characteristics and Transfer Learning},
  author = {Shin, Hoo-Chang and Roth, Holger R and Gao, Mingchen and Lu, Le and Xu, Ziyue and Nogues, Isabella and Yao, Jianhua and Mollura, Daniel and Summers, Ronald M},
  date = {2016-05},
  journaltitle = {IEEE Transactions on Medical Imaging},
  shortjournal = {IEEE Trans. Med. Imaging},
  volume = {35},
  pages = {1285--1298},
  publisher = {{ieeexplore.ieee.org}},
  abstract = {Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and deep convolutional neural networks (CNNs). CNNs enable learning data-driven, highly representative, hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.},
  number = {5}
}

@inproceedings{Shiraishi,
  title = {Computer-Aided Diagnosis and Artificial Intelligence in Clinical Imaging},
  booktitle = {Seminars in Nuclear Medicine},
  author = {Shiraishi, Junji and Li, Qiang and Appelbaum, Daniel and Doi, Kunio},
  volume = {41},
  pages = {449--462},
  publisher = {{Elsevier}},
  isbn = {0001-2998}
}

@article{Shortliffe1973,
  title = {An {{Artificial Intelligence}} Program to Advise Physicians Regarding Antimicrobial Therapy},
  author = {Shortliffe, Edward H. and Axline, Stanton G. and Buchanan, Bruce G. and Merigan, Thomas C. and Cohen, Stanley N.},
  date = {1973},
  journaltitle = {Computers and Biomedical Research},
  volume = {6},
  pages = {544--560},
  issn = {00104809},
  doi = {10.1016/0010-4809(73)90029-3},
  abstract = {An antimicrobial therapy consultation system has been developed which utilizes a flexible representation of knowledge. The novel design facilitates interactive advice-giving sessions with physicians. An ability to display reasons for making decisions at the request of the user permits the program to serve a tutorial as well as consultative role. The feasibility of the judgmental rule approach which the program uses has been demonstrated with a limited knowledge base of approximately 100 rules. Its ultimate success as a clinically useful tool depends upon acquisition of additional rules and thus upon co-operation of infectious disease experts willing to improve the program's knowledge base. The techniques for acquisition, representation, and utilization of knowledge, plus considerations of natural language processing, draw upon and contribute to current Artificial Intelligence research. © 1973.},
  number = {6}
}

@article{Shringarpure2015,
  title = {Privacy Risks from Genomic Data-Sharing Beacons},
  author = {Shringarpure, Suyash S and Bustamante, Carlos D},
  date = {2015},
  journaltitle = {The American Journal of Human Genetics},
  volume = {97},
  pages = {631--646},
  publisher = {{Elsevier}},
  issn = {0002-9297},
  number = {5}
}

@article{shyvalova2017-qn,
  title = {Cutaneous Squamous Cell Carcinoma: Epidemiological Aspects among the Population of {{Ukraine}}},
  author = {Оshyvalova, Оlena О and Ziukov, Oleg L},
  date = {2017},
  journaltitle = {Wiadomosci Lekarskie},
  shortjournal = {Wiad. Lek.},
  volume = {70},
  pages = {178--181},
  abstract = {INTRODUCTION: In the structure of malignant skin neoplasms, the tumours of epithelial origin take the first place, among them the prevalence of cutaneous squamous cell carcinoma (cSCC) is about 20\%. The aim of the work involves the analysis of cSCC epidemiological features among the contingent of the State Scientific Institution “Scientific and Practical Centre of Preventive and Clinical Medicine” of the State Administration (SIS) over 2005-2014 (Ukraine, Kyiv). MATERIAL AND METHODS: For retrospective epidemiological analysis there was used identifying information of patients with cSCC of the contingent of SIS over 2005-2014 in comparison with the data on patients with cSCC of Kyiv city and Kyiv region over the same period of time. RESULTS: The morbidity rate of cSCC among the contingent of SIS in 2005-2014 increased from 6.7 per 100,000people to 37.1 per 100,000 people. Among men it increased from 8.2 per 100,000 people to 59.2 per 100,000 people, and among women - from 6.6 per 100,000 people to 19.2 per 100,000people. Among the contingent of SIS the highest incidence both in men and women was observed in the age group of 85 and upwards; among men it was 115.2 per 100,000 people, and among women - 112.0 per 100,000 people. CONCLUSIONS: The morbidity rate of cSCC among the contingent of SIS in 2005-2014 was significantly higher (p{$<$}0.05) than the morbidity rate among the population of Kyiv city and Kyiv region over the same period of time, with the prevalence of patients aged 85 and upwards, both men and women.},
  keywords = {disease progression,incidence,mortality,tumour},
  number = {2}
}

@online{Simonyan2014,
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2014},
  archivePrefix = {arXiv},
  eprint = {1409.1556},
  eprinttype = {arxiv}
}

@article{Singh2011,
  title = {Computer-Aided Classification of Breast Masses: Performance and Interobserver Variability of Expert Radiologists versus Residents},
  author = {Singh, Swatee and Maxwell, Jeff and Baker, Jay A and Nicholas, Jennifer L and Lo, Joseph Y},
  date = {2011},
  journaltitle = {Radiology},
  volume = {258},
  pages = {73--80},
  issn = {0033-8419},
  number = {1}
}

@article{Sirinukunwattana2016-os,
  title = {Locality Sensitive Deep Learning for Detection and Classification of Nuclei in Routine Colon Cancer Histology Images},
  author = {Sirinukunwattana, Korsuk and Ahmed Raza, Shan E and {Yee-Wah Tsang} and Snead, David R J and Cree, Ian A and Rajpoot, Nasir M},
  date = {2016-05},
  journaltitle = {IEEE Transactions on Medical Imaging},
  shortjournal = {IEEE Trans. Med. Imaging},
  volume = {35},
  pages = {1196--1206},
  publisher = {{ieeexplore.ieee.org}},
  abstract = {Detection and classification of cell nuclei in histopathology images of cancerous tissue stained with the standard hematoxylin and eosin stain is a challenging task due to cellular heterogeneity. Deep learning approaches have been shown to produce encouraging results on histopathology images in various studies. In this paper, we propose a Spatially Constrained Convolutional Neural Network (SC-CNN) to perform nucleus detection. SC-CNN regresses the likelihood of a pixel being the center of a nucleus, where high probability values are spatially constrained to locate in the vicinity of the centers of nuclei. For classification of nuclei, we propose a novel Neighboring Ensemble Predictor (NEP) coupled with CNN to more accurately predict the class label of detected cell nuclei. The proposed approaches for detection and classification do not require segmentation of nuclei. We have evaluated them on a large dataset of colorectal adenocarcinoma images, consisting of more than 20,000 annotated nuclei belonging to four different classes. Our results show that the joint detection and classification of the proposed SC-CNN and NEP produces the highest average F1 score as compared to other recently published approaches. Prospectively, the proposed methods could offer benefit to pathology practice in terms of quantitative analysis of tissue constituents in whole-slide images, and potentially lead to a better understanding of cancer.},
  number = {5}
}

@article{sirinukunwattana2017gland,
  title = {Gland Segmentation in Colon Histology Images: {{The}} Glas Challenge Contest},
  author = {Sirinukunwattana, Korsuk and Pluim, Josien PW and Chen, Hao and Qi, Xiaojuan and Heng, Pheng-Ann and Guo, Yun Bo and Wang, Li Yang and Matuszewski, Bogdan J and Bruni, Elia and Sanchez, Urko and others},
  date = {2017},
  journaltitle = {Medical image analysis},
  volume = {35},
  pages = {489--502},
  publisher = {{Elsevier}}
}

@article{Slater2019,
  title = {Standards and Datasets for Reporting Cancers {{Dataset}} for the Histological Reporting of Primary Invasive Cutaneous Squamous Cell Carcinoma and Regional Lymph Nodes {{February}} 2019},
  author = {Slater, David and Walsh, Maureen},
  date = {2019},
  pages = {1--36},
  url = {https://www.rcpath.org/asset/9C1D8F71-5D3B-4508-8E6200F11E1F4A39/},
  issue = {May}
}

@report{Slater2019a,
  title = {Dataset for Histopathological Reporting of Primary Invasive Cutaneous Squamous Cell Carcinoma and Regional Lymph Nodes},
  author = {Slater, David and Barrett, Paul},
  date = {2019},
  pages = {57},
  institution = {{Royal College of Pathologists}},
  location = {{London}},
  url = {https://www.rcpath.org/uploads/assets/9c1d8f71-5d3b-4508-8e6200f11e1f4a39/dataset-for-histopathological-reporting-of-primary-invasive-cutaneous-squamous-cell-carcinoma-and-regional-lymph-nodes.pdf}
}

@online{Smilkov2017,
  title = {Smoothgrad: Removing Noise by Adding Noise},
  author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viégas, Fernanda and Wattenberg, Martin},
  date = {2017},
  archivePrefix = {arXiv},
  eprint = {1706.03825},
  eprinttype = {arxiv}
}

@article{Snyder2005,
  title = {Generative Modeling},
  author = {Snyder, John M. and Kajiya, James T.},
  date = {2005},
  journaltitle = {ACM SIGGRAPH Computer Graphics},
  issn = {00978930},
  doi = {10.1145/142920.134094},
  abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.}
}

@article{Sornapudi2018,
  title = {Deep Learning Nuclei Detection in Digitized Histology Images by Superpixels},
  author = {Sornapudi, Sudhir and Stanley, Ronald and Stoecker, William and Almubarak, Haidar and Long, Rodney and Antani, Sameer and Thoma, George and Zuna, Rosemary and Frazier, Shelliane},
  date = {2018-01},
  journaltitle = {Journal of Pathology Informatics},
  volume = {9},
  pages = {5},
  doi = {10.4103/jpi.jpi_74_17},
  url = {http://www.jpathinformatics.org/article.asp?issn=2153-3539 year=2018 volume=9 issue=1 spage=5 epage=5 aulast=Sornapudi},
  abstract = {\textbf{Background:} Advances in image analysis and computational techniques have facilitated automatic detection of critical features in histopathology images. Detection of nuclei is critical for squamous epithelium cervical intraepithelial neoplasia (CIN) classification into normal, CIN1, CIN2, and CIN3 grades. \textbf{Methods:} In this study, a deep learning (DL)-based nuclei segmentation approach is investigated based on gathering localized information through the generation of superpixels using a simple linear iterative clustering algorithm and training with a convolutional neural network. \textbf{Results:} The proposed approach was evaluated on a dataset of 133 digitized histology images and achieved an overall nuclei detection (object-based) accuracy of 95.97\&\#37;, with demonstrated improvement over imaging-based and clustering-based benchmark techniques. \textbf{Conclusions:} The proposed DL-based nuclei segmentation Method with superpixel analysis has shown improved segmentation results in comparison to state-of-the-art methods.},
  number = {1}
}

@article{Srivastava2014,
  title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  author = {Srivastava, Nitish and Hinton, Geoffrey E and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  date = {2014},
  journaltitle = {Journal of machine learning research},
  volume = {15},
  pages = {1929--1958},
  number = {1}
}

@article{Staples2006-kp,
  title = {Non-Melanoma Skin Cancer in {{Australia}}: The 2002 National Survey and Trends since 1985},
  author = {Staples, Margaret P and Elwood, Mark and Burton, Robert C and Williams, Jodie L and Marks, Robin and Giles, Graham G},
  date = {2006-01},
  journaltitle = {Medical Journal of Australia},
  shortjournal = {Med. J. Aust.},
  volume = {184},
  pages = {6--10},
  abstract = {OBJECTIVES: To measure the incidence of treated non-melanoma skin cancer (NMSC) in Australia in 2002 and investigate trends since 1985 by histological type, sex, age group, latitude and skin type. DESIGN: Face-to-face survey between 1 January and 31 December 2002 using stratified sampling of households to identify people treated for skin cancer in the previous 12 months. Self-reported diagnoses were confirmed with treatment providers. Data from similar surveys conducted in 1985, 1990 and 1995 were used to assess trends. SETTING: Whole of Australia (population 19.6 million). PARTICIPANTS: Of 57 215 people interviewed, 4098 said they had been treated for skin cancer in the past year and 3198 gave permission for their diagnoses to be confirmed with their doctor. RESULTS: 817 people were confirmed as having at least one skin cancer treated in the past year. The age-standardised rate per 100 000 population for NMSC was 1170, for basal cell carcinoma (BCC) 884, and for squamous cell carcinoma (SCC) 387. The estimated number of NMSC cases in Australia for 2002 was 374 000. Cumulative risks to age 70 years of having at least one NMSC were 70\% for men and 58\% for women. Rates of BCC and SCC have increased since 1985, and the increases greatest for people aged 60 years and older; rates for those younger than 60 years have stabilised. CONCLUSIONS: The incidence of treated NMSC in Australia in 2002 was more than five times the incidence of all other cancers combined. Although the overall NMSC rates have risen since 1985, the stabilisation of rates for people younger than 60 years who were exposed to skin cancer prevention programs in their youth highlights the importance of maintaining and strengthening these programs.},
  number = {1}
}

@article{Sturm2019,
  title = {Validation of Whole-Slide Digitally Imaged Melanocytic Lesions: {{Does}} z-Stack Scanning Improve Diagnostic Accuracy?},
  author = {Sturm, Bart and Creytens, David and Cook, Martin G and Smits, Jan and van Dijk, Marcory C R F and Eijken, Erik and Kurpershoek, Eline and Küsters-Vandevelde, Heidi V N and Ooms, Ariadne H A G and Wauters, Carla},
  date = {2019},
  journaltitle = {Journal of Pathology Informatics},
  volume = {10},
  publisher = {{Wolters Kluwer–Medknow Publications}},
  options = {useprefix=true}
}

@article{sturmfelsVisualizingImpactFeature2020,
  title = {Visualizing the Impact of Feature Attribution Baselines},
  author = {Sturmfels, Pascal and Lundberg, Scott and Lee, Su-In},
  date = {2020},
  journaltitle = {Distill},
  volume = {5},
  pages = {e22},
  number = {1}
}

@inproceedings{Su2015-bh,
  title = {Region Segmentation in Histopathological Breast Cancer Images Using Deep Convolutional Neural Network},
  booktitle = {2015 \{\vphantom\}{{IEEE}}\vphantom\{\} 12th International Symposium on Biomedical Imaging (\{\vphantom\}{{ISBI}}\vphantom\{\})},
  author = {Su, H and Liu, F and Xie, Y and Xing, F and Meyyappan, S and Yang, L},
  date = {2015-04},
  pages = {55--58},
  publisher = {{ieeexplore.ieee.org}},
  abstract = {Computer aided diagnosis of breast cancers often relies on automatic image analysis of histopathology images. The automatic region segmentation in breast cancer is challenging due to: i) large regional variations, and ii) high computational costs of pixel-wise segmentation. Deep convolutional neural network (CNN) is proven to be an effective method for image recognition and classification. However, it is often computationally expensive. In this paper, we propose to apply a fast scanning deep convolutional neural network (fCNN) to pixel-wise region segmentation. The fCNN removes the redundant computations in the original CNN without sacrificing its performance. In our experiment it takes only 2.3 seconds to segment an image with size 1000 \$\$ 1000. The comparison experiments show that the proposed system outperforms both the LBP feature-based and texton-based pixel-wise methods.},
  keywords = {cancer,imag,image classification,image recognition}
}

@incollection{Su2017-ce,
  title = {Robust Cell Detection and Segmentation in Histopathological Images Using Sparse Reconstruction and Stacked Denoising Autoencoders},
  booktitle = {Deep Learning and Convolutional Neural Networks for Medical Image Computing},
  author = {Su, Hai and Xing, Fuyong and Kong, Xiangfei and Xie, Yuanpu and Zhang, Shaoting and Yang, Lin},
  date = {2017},
  pages = {257--278},
  publisher = {{Springer, Cham}},
  abstract = {Computer-aided diagnosis (CAD) is a promising tool for accurate and consistent diagnosis and prognosis. Cell detection and segmentation are essential steps for CAD. These tasks are challenging due to variations in cell shapes, touching cells, and cluttered background. In this paper, we present a cell detection and segmentation algorithm using the sparse reconstruction with trivial templates and a stacked denoising autoencoder (sDAE) trained with structured labels and discriminative losses. The sparse reconstruction handles the shape variations by representing a testing patch as a linear combination of bases in the learned dictionary. Trivial templates are used to model the touching parts. The sDAE, trained on the original data with their structured labels and discriminative losses, is used for cell segmentation . To the best of our knowledge, this is the first study to apply sparse reconstruction and sDAE with both structured labels and discriminative losses to cell detection and segmentation. It is observed that structured learning can effectively handle weak or misleading edges, and discriminative training encourages the model to learn groups of filters that activate simultaneously for different input images to ensure better segmentation. The proposed method is extensively tested on four data sets containing more than 6000 cells obtained from brain tumor, lung cancer, and breast cancer and neuroendocrine tumor (NET) images. Our algorithm achieves the best performance compared with other state of the arts.},
  series = {Advances in Computer Vision and Pattern Recognition}
}

@article{Suk2015,
  title = {Latent Feature Representation with Stacked Auto-Encoder for {{AD}}/{{MCI}} Diagnosis},
  author = {Suk, Heung-Il and Lee, Seong-Whan and Shen, Dinggang and Initiative, Alzheimer's Disease Neuroimaging},
  date = {2015},
  journaltitle = {Brain Structure and Function},
  volume = {220},
  pages = {841--859},
  issn = {1863-2653},
  number = {2}
}

@inproceedings{Sun2017,
  title = {Revisiting Unreasonable Effectiveness of Data in Deep Learning Era},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  date = {2017},
  pages = {843--852}
}

@inproceedings{Sundararajan2017,
  title = {Axiomatic Attribution for Deep Networks},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  date = {2017},
  pages = {3319--3328},
  publisher = {{JMLR. org}}
}

@online{Szegedy2013,
  title = {Intriguing Properties of Neural Networks},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  date = {2013},
  archivePrefix = {arXiv},
  eprint = {1312.6199},
  eprinttype = {arxiv}
}

@online{Szegedy2014,
  title = {Going Deeper with Convolutions ({{GoogLeNet}})},
  author = {Szegedy, C and Liu, W and Jia, Y and Sermanet, P and Reed, S and Anguelov, D and Erhan, D and Rabinovich, A},
  date = {2014},
  archivePrefix = {arXiv},
  eprint = {1409.4842},
  eprinttype = {arxiv}
}

@inproceedings{Szegedy2016,
  title = {Rethinking the Inception Architecture for Computer Vision},
  booktitle = {Proceedings of the {{IEEE}} Computer Society Conference on Computer Vision and Pattern Recognition},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  date = {2016},
  volume = {2016-Decem},
  pages = {2818--2826},
  issn = {10636919},
  doi = {10.1109/CVPR.2016.308},
  abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error on the validation set (3.6\% error on the test set) and 17.3\% top-1 error on the validation set.},
  isbn = {978-1-4673-8850-4}
}

@inproceedings{Szegedy2017,
  title = {Inception-v4, Inception-Resnet and the Impact of Residual Connections on Learning},
  booktitle = {Thirty-First {{AAAI}} Conference on Artificial Intelligence},
  author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A},
  date = {2017}
}

@inproceedings{Tajbakhsh2015,
  title = {Computer-Aided Pulmonary Embolism Detection Using a Novel Vessel-Aligned Multi-Planar Image Representation and Convolutional Neural Networks},
  booktitle = {Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  author = {Tajbakhsh, Nima and Gotway, Michael B and Liang, Jianming},
  date = {2015},
  volume = {9350},
  pages = {62--69},
  publisher = {{Springer}},
  issn = {16113349},
  doi = {10.1007/978-3-319-24571-3_8},
  abstract = {© Springer International Publishing Switzerland 2015. Computer-aided detection (CAD) can play a major role in diagnosing pulmonary embolism (PE) at CT pulmonary angiography (CTPA). However, despite their demonstrated utility, to achieve a clinically acceptable sensitivity, existing PE CAD systems generate a high number of false positives, imposing extra burdens on radiologists to adjudicate these superfluous CAD findings. In this study, we investigate the feasibility of convolutional neural networks (CNNs) as an effective mechanism for eliminating false positives. A critical issue in successfully utilizing CNNs for detecting an object in 3D images is to develop a “right” image representation for the object. Toward this end, we have developed a vesselaligned multi-planar image representation of emboli. Our image representation offers three advantages: (1) efficiency and compactness—concisely summarizing the 3D contextual information around an embolus in only 2 image channels, (2) consistency—automatically aligning the embolus in the 2-channel images according to the orientation of the affected vessel, and (3) expandability—naturally supporting data augmentation for training CNNs. We have evaluated our CAD approach using 121 CTPA datasets with a total of 326 emboli, achieving a sensitivity of 83\% at 2 false positives per volume. This performance is superior to the best performing CAD system in the literature, which achieves a sensitivity of 71\% at the same level of false positives. We have further evaluated our system using the entire 20 CTPA test datasets from the PE challenge. Our system outperforms the winning system from the challenge at 0mm localization error but is outperformed by it at 2mm and 5mm localization errors. In our view, the performance at 0mm localization error is more important than those at 2mm and 5mm localization errors.},
  isbn = {978-3-319-24570-6},
  keywords = {Computer-aided detection,Convolutional neural networks,Pulmonary embolism,Vessel-aligned image representation}
}

@article{Tan2015,
  title = {Extraction from Genome-Wide Assays of Breast Cancer},
  author = {Tan, Jie and Ung, Matthew and Cheng, Chao and Greene, Casey S},
  date = {2015},
  journaltitle = {Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing},
  shortjournal = {Pac Symp Biocomput},
  pages = {132--143},
  url = {http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC4299935\{&\}blobtype=pdf},
  abstract = {Big data bring new opportunities for methods that efficiently summarize and automatically extract knowledge from such compendia. While both supervised learning algorithms and unsupervised clustering algorithms have been successfully applied to biological data, they are either dependent on known biology or limited to discerning the most significant signals in the data. Here we present denoising autoencoders (DAs), which employ a data-defined learning objective independent of known biology, as a method to identify and extract complex patterns from genomic data. We evaluate the performance of DAs by applying them to a large collection of breast cancer gene expression data. Results show that DAs successfully construct features that contain both clinical and molecular information. There are features that represent tumor or normal samples, estrogen receptor (ER) status, and molecular subtypes. Features constructed by the autoencoder generalize to an independent dataset collected using a distinct experimental platform. By integrating data from ENCODE for feature interpretation, we discover a feature representing ER status through association with key transcription factors in breast cancer. We also identify a feature highly predictive of patient survival and it is enriched by FOXM1 signaling pathway. The features constructed by DAs are often bimodally distributed with one peak near zero and another near one, which facilitates discretization. In summary, we demonstrate that DAs effectively extract key biological principles from gene expression data and summarize them into constructed features with convenient properties.},
  keywords = {breast cancer,denoising autoencoders,feature construction,functional genomics}
}

@online{Tan2019,
  title = {{{EfficientNet}}: {{Rethinking}} Model Scaling for Convolutional Neural Networks},
  author = {Tan, Mingxing and Le, Quoc V},
  date = {2019},
  archivePrefix = {arXiv},
  eprint = {1905.11946},
  eprinttype = {arxiv}
}

@article{tarek2018staingan,
  title = {{{StainGAN}}: {{Stain}} Style Transfer for Digital Histological Images},
  author = {Tarek Shaban, M and Baur, Christoph and Navab, Nassir and Albarqouni, Shadi},
  date = {2018},
  journaltitle = {arXiv e-prints},
  pages = {arXiv--1804}
}

@online{Team_The_Theano_Development2016-gg,
  title = {Theano: {{A Python}} Framework for Fast Computation of Mathematical Expressions},
  author = {{Team, The Theano Development} and Al-Rfou, Rami and Alain, Guillaume and Almahairi, Amjad and Angermueller, Christof and Bahdanau, Dzmitry and Ballas, Nicolas and Bastien, Frédéric and Bayer, Justin and Belikov, Anatoly},
  date = {2016},
  archivePrefix = {arXiv},
  eprint = {1605.02688},
  eprinttype = {arxiv}
}

@online{Team2016,
  title = {Theano: {{A Python}} Framework for Fast Computation of Mathematical Expressions},
  author = {Team, The Theano Development and Al-Rfou, Rami and Alain, Guillaume and Almahairi, Amjad and Angermueller, Christof and Bahdanau, Dzmitry and Ballas, Nicolas and Bastien, Frédéric and Bayer, Justin and Belikov, Anatoly},
  date = {2016},
  archivePrefix = {arXiv},
  eprint = {1605.02688},
  eprinttype = {arxiv}
}

@article{Teramoto2017,
  title = {Automated Classification of Lung Cancer Types from Cytological Images Using Deep Convolutional Neural Networks},
  author = {Teramoto, Atsushi and Tsukamoto, Tetsuya and Kiriyama, Yuka and Fujita, Hiroshi},
  date = {2017},
  journaltitle = {BioMed research international},
  volume = {2017},
  publisher = {{Hindawi}},
  issn = {2314-6133}
}

@article{Thomas2020,
  title = {Pathologist versus Artificial Pathologist: {{What}} Do We Really Want (Need) from Machine Learning},
  author = {Thomas, Simon M},
  date = {2020},
  journaltitle = {Digital pathology association blog},
  url = {https://digitalpathologyassociation.org/blog/pathologist-versus-artificial-pathologist-what-do-we-really-want-need-from-machine-learning/},
  urldate = {2020-08-18}
}

@article{thomas2020interpretable,
  title = {Interpretable Deep Learning Systems for Multi-Class Segmentation and Classification of Non-Melanoma Skin Cancer},
  author = {Thomas, Simon M and Lefevre, James G and Baxter, Glenn and Hamilton, Nicholas A},
  journaltitle = {Medical Image Analysis},
  volume = {68},
  pages = {101915},
  publisher = {{Elsevier}}
}

@article{Titford2006,
  title = {A Short History of Histopathology Technique},
  author = {Titford, Michael},
  date = {2006-06},
  journaltitle = {Journal of Histotechnology},
  volume = {29},
  pages = {99--110},
  publisher = {{Taylor \& Francis}},
  issn = {0147-8885},
  doi = {10.1179/his.2006.29.2.99},
  url = {https://doi.org/10.1179/his.2006.29.2.99},
  number = {2}
}

@article{Tizhoosh2018,
  title = {Artificial Intelligence and Digital Pathology: {{Challenges}} and Opportunities.},
  author = {Tizhoosh, Hamid Reza and Pantanowitz, Liron},
  date = {2018},
  journaltitle = {Journal of pathology informatics},
  volume = {9},
  pages = {38},
  issn = {2229-5089 (Print)},
  doi = {10.4103/jpi.jpi_53_18},
  abstract = {In light of the recent success of artificial intelligence (AI) in computer vision applications, many researchers and physicians expect that AI would be able to assist in many tasks in digital pathology. Although opportunities are both manifest and tangible, there are clearly many challenges that need to be overcome in order to exploit the AI potentials in computational pathology. In this paper, we strive to provide a realistic account of all challenges and opportunities of adopting AI algorithms in digital pathology from both engineering and pathology perspectives.},
  eprint = {30607305},
  eprinttype = {pmid},
  langid = {english}
}

@online{Touvron2019,
  title = {Fixing the Train-Test Resolution Discrepancy},
  author = {Touvron, Hugo and Vedaldi, Andrea and Douze, Matthijs and Jégou, Hervé},
  date = {2019},
  archivePrefix = {arXiv},
  eprint = {1906.06423},
  eprinttype = {arxiv}
}

@article{Trebeschi2017,
  title = {Deep Learning for Fully-Automated Localization and Segmentation of Rectal Cancer on Multiparametric {{MR}}},
  author = {Trebeschi, Stefano and van Griethuysen, Joost J M and Lambregts, Doenja M J and Lahaye, Max J and Parmer, Chintan and Bakers, Frans C H and Peters, Nicky H G M and Beets-Tan, Regina G H and Aerts, Hugo J W L},
  date = {2017},
  journaltitle = {Scientific reports},
  volume = {7},
  pages = {5301},
  issn = {2045-2322},
  options = {useprefix=true}
}

@article{trends.google.com,
  title = {Google Trends},
  author = {{Trends.google.com}},
  date = {2019},
  url = {http://trends.google.com/trends},
  keywords = {google hda_prediction_markets nocache trends}
}

@online{Tschuchnig2020,
  title = {Generative Adversarial Networks in Digital Pathology: {{A}} Survey on Trends and Future Potential},
  author = {Tschuchnig, Maximilian Ernst and Oostingh, Gertie Janneke and Gadermayr, Michael},
  date = {2020},
  pages = {1--10},
  url = {http://arxiv.org/abs/2004.14936},
  abstract = {Image analysis in the field of digital pathology has recently gained increased popularity. The use of high-quality whole slide scanners enables the fast acquisition of large amounts of image data, showing extensive context and microscopic detail at the same time. Simultaneously, novel machine learning algorithms have boosted the performance of image analysis approaches. In this paper, we focus on a particularly powerful class of architectures, called Generative Adversarial Networks (GANs), applied to histological image data. Besides improving performance, GANs also enable application scenarios in this field, which were previously intractable. However, GANs could exhibit a potential for introducing bias. Hereby, we summarize the recent state-of-the-art developments in a generalizing notation, present the main applications of GANs and give an outlook of some chosen promising approaches and their possible future applications. In addition, we identify currently unavailable methods with potential for future applications.},
  archivePrefix = {arXiv},
  arxivid = {2004.14936},
  eprint = {2004.14936},
  eprinttype = {arxiv}
}

@article{Turley2007,
  title = {Diagnostic Accuracy of Automated Computerised Electrocardiogram Interpretation Compared with a Panel of Experienced Cardiologists},
  author = {Turley, A and Roberts, A and Evemy, K and Haq, I and Irvine, T and Adams, P},
  date = {2007},
  journaltitle = {Critical Care},
  volume = {11},
  pages = {P245--P245},
  issn = {1364-8535 1466-609X},
  doi = {10.1186/cc5405},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4095298/},
  issue = {Suppl 2}
}

@incollection{UnitedNations2019,
  title = {[{{World}} Population Prospects 2019].},
  booktitle = {United Nations. {{Department}} of Economic and Social Affairs. {{World}} Population Prospects 2019.},
  author = {{United Nations}},
  date = {2019},
  pages = {49--78},
  issn = {0337-307X},
  abstract = {The author summarizes recent U.N. global population projections up to the year 2025. The focus is on the rates of overall growth, the changing balance of population between the developed and developing worlds, demographic aging, and urbanization.},
  eprint = {12283219},
  eprinttype = {pmid},
  isbn = {978-92-1-148316-1},
  keywords = {age distribution,age structure,ageing,aging,demographic change,demographic dividend,demographic estimates,demographic estimation,demographic projections,demographic transition,demography,dependency,deterministic population projections,estimates,estimation,fertility,fertility decline,fertility projections,fertility rate,fertility transition,future,gender,global population data,growth,human,international migration,levels and trends,life expectancy,long-term forecasting,longevity,migration,mortality,mortality decline,mortality projections,mortality transition,national population,net migration,older persons,population,population – statistics,population ageing,population change,population characteristics,population data,population decline,population density,population distribution,population dynamics,population estimates,population forecasting,population growth,population increase,population projections,population size,prediction,prediction interval,probabilistic population projections,probabilistic projections,projection,projections,scenarios,support ratio,sustainability,sustainable development,transition,trends,uncertainty,United Nations,variants,world demography,world population},
  number = {141}
}

@article{Vandenberghe2017-ir,
  title = {Relevance of Deep Learning to Facilitate the Diagnosis of \{\vphantom\}{{HER2}}\vphantom\{\} Status in Breast Cancer},
  author = {Vandenberghe, Michel E and Scott, Marietta L J and Scorer, Paul W and Söderberg, Magnus and Balcerzak, Denis and Barker, Craig},
  date = {2017-04},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci. Rep.},
  volume = {7},
  pages = {45938},
  publisher = {{ncbi.nlm.nih.gov}},
  abstract = {Tissue biomarker scoring by pathologists is central to defining the appropriate therapy for patients with cancer. Yet, inter-pathologist variability in the interpretation of ambiguous cases can affect diagnostic accuracy. Modern artificial intelligence methods such as deep learning have the potential to supplement pathologist expertise to ensure constant diagnostic accuracy. We developed a computational approach based on deep learning that automatically scores HER2, a biomarker that defines patient eligibility for anti-HER2 targeted therapies in breast cancer. In a cohort of 71 breast tumour resection samples, automated scoring showed a concordance of 83\% with a pathologist. The twelve discordant cases were then independently reviewed, leading to a modification of diagnosis from initial pathologist assessment for eight cases. Diagnostic discordance was found to be largely caused by perceptual differences in assessing HER2 expression due to high HER2 staining heterogeneity. This study provides evidence that deep learning aided diagnosis can facilitate clinical decision making in breast cancer by identifying cases at high risk of misdiagnosis.}
}

@article{VanderLaak2017,
  title = {Deep Learning Based Algorithms Significantly Aid Breast Cancer Histopathology},
  author = {van der Laak, J and Bejnordi, B E and Martin, D T and Bandi, P and Balkenhol, M C A and Bult, P and Litjens, G J S},
  date = {2017},
  journaltitle = {Journal of Pathology},
  volume = {243},
  pages = {S4--S4},
  issn = {0022-3417},
  options = {useprefix=true}
}

@article{VanderWalt2014,
  title = {Scikit-Image: Image Processing in {{Python}}},
  author = {Van der Walt, Stefan and Schönberger, Johannes L and Nunez-Iglesias, Juan and Boulogne, François and Warner, Joshua D and Yager, Neil and Gouillart, Emmanuelle and Yu, Tony},
  date = {2014},
  journaltitle = {PeerJ},
  volume = {2},
  pages = {e453},
  publisher = {{PeerJ Inc.}},
  issn = {2167-8359}
}

@article{VanGinneken2011,
  title = {Computer-Aided Diagnosis: How to Move from the Laboratory to the Clinic},
  author = {van Ginneken, Bram and Schaefer-Prokop, Cornelia M and Prokop, Mathias},
  date = {2011},
  journaltitle = {Radiology},
  volume = {261},
  pages = {719--732},
  issn = {0033-8419},
  number = {3},
  options = {useprefix=true}
}

@online{Vasconcelos2017-vi,
  title = {Convolutional Neural Network Committees for Melanoma Classification with Classical and Expert Knowledge Based Image Transforms Data Augmentation},
  author = {Vasconcelos, Cristina Nader and Vasconcelos, Bárbara Nader},
  date = {2017-02},
  abstract = {Skin cancer is a major public health problem, as is the most common type of cancer and represents more than half of cancer diagnoses worldwide. Early detection influences the outcome of the disease and motivates our work. We investigate the composition of CNN committees and data augmentation for the the ISBI 2017 Melanoma Classification Challenge (named Skin Lesion Analysis towards Melanoma Detection) facing the peculiarities of dealing with such a small, unbalanced, biological database. For that, we explore committees of Convolutional Neural Networks trained over the ISBI challenge training dataset artificially augmented by both classical image processing transforms and image warping guided by specialist knowledge about the lesion axis and improve the final classifier invariance to common melanoma variations.},
  archivePrefix = {arXiv},
  arxivid = {cs.CV/1702.07025},
  eprint = {1702.07025},
  eprinttype = {arxiv},
  primaryClass = {cs.CV}
}

@online{Vasconcelos2017-xu,
  title = {Convolutional Neural Network Committees for Melanoma Classification with Classical and Expert Knowledge Based Image Transforms Data Augmentation},
  author = {Vasconcelos, Cristina Nader and Vasconcelos, Bárbara Nader},
  date = {2017-02},
  abstract = {Skin cancer is a major public health problem, as is the most common type of cancer and represents more than half of cancer diagnoses worldwide. Early detection influences the outcome of the disease and motivates our work. We investigate the composition of CNN committees and data augmentation for the the ISBI 2017 Melanoma Classification Challenge (named Skin Lesion Analysis towards Melanoma Detection) facing the peculiarities of dealing with such a small, unbalanced, biological database. For that, we explore committees of Convolutional Neural Networks trained over the ISBI challenge training dataset artificially augmented by both classical image processing transforms and image warping guided by specialist knowledge about the lesion axis and improve the final classifier invariance to common melanoma variations.},
  archivePrefix = {arXiv},
  arxivid = {cs.CV/1702.07025},
  eprint = {1702.07025},
  eprinttype = {arxiv},
  primaryClass = {cs.CV}
}

@article{Venhuizen2018,
  title = {Deep Learning Approach for the Detection and Quantification of Intraretinal Cystoid Fluid in Multivendor Optical Coherence Tomography},
  author = {Venhuizen, Freerk G. and van Ginneken, Bram and Liefers, Bart and van Asten, Freekje and Schreur, Vivian and Fauser, Sascha and Hoyng, Carel and Theelen, Thomas and Sánchez, Clara I.},
  date = {2018-04},
  journaltitle = {Biomedical Optics Express},
  volume = {9},
  pages = {1545},
  publisher = {{The Optical Society}},
  issn = {2156-7085},
  doi = {10.1364/boe.9.001545},
  abstract = {We developed a deep learning algorithm for the automatic segmentation and quantification of intraretinal cystoid fluid (IRC) in spectral domain optical coherence tomography (SD-OCT) volumes independent of the device used for acquisition. A cascade of neural networks was introduced to include prior information on the retinal anatomy, boosting performance significantly. The proposed algorithm approached human performance reaching an overall Dice coefficient of 0.754 \&\#x000B1; 0.136 and an intraclass correlation coefficient of 0.936, for the task of IRC segmentation and quantification, respectively. The proposed method allows for fast quantitative IRC volume measurements that can be used to improve patient care, reduce costs, and allow fast and reliable analysis in large population studies.},
  number = {4},
  options = {useprefix=true}
}

@article{Wainberg2018,
  title = {Deep Learning in Biomedicine},
  author = {Wainberg, Michael and Merico, Daniele and Delong, Andrew and Frey, Brendan J.},
  date = {2018-10},
  journaltitle = {Nature Biotechnology},
  volume = {36},
  pages = {829--838},
  publisher = {{Nature Publishing Group}},
  issn = {15461696},
  doi = {10.1038/nbt.4233},
  abstract = {Brendan Frey and colleagues provide a personal overview of the machine learning field and in particular deep learning. They outline the technical challenges in applying machine learning to different types of biological and biomedical data and go on to discuss the challenges in implementing these approaches in the clinical realm, in drug discovery programs and within regulatory agencies.},
  number = {9}
}

@article{Wainberg2018a,
  title = {Deep Learning in Biomedicine},
  author = {Wainberg, M and Merico, D and Delong, A and Frey, B J},
  date = {2018},
  journaltitle = {Nature Biotechnology},
  volume = {36},
  pages = {829--838},
  issn = {1087-0156},
  doi = {10.1038/nbt.4233},
  url = {\{%\}3CGo to},
  number = {9}
}

@online{Wang2016-qj,
  title = {Deep Learning for Identifying Metastatic Breast Cancer},
  author = {Wang, Dayong and Khosla, Aditya and Gargeya, Rishab and Irshad, Humayun and Beck, Andrew H},
  date = {2016-06},
  abstract = {The International Symposium on Biomedical Imaging (ISBI) held a grand challenge to evaluate computational systems for the automated detection of metastatic breast cancer in whole slide images of sentinel lymph node biopsies. Our team won both competitions in the grand challenge, obtaining an area under the receiver operating curve (AUC) of 0.925 for the task of whole slide image classification and a score of 0.7051 for the tumor localization task. A pathologist independently reviewed the same images, obtaining a whole slide image classification AUC of 0.966 and a tumor localization score of 0.733. Combining our deep learning system's predictions with the human pathologist's diagnoses increased the pathologist's AUC to 0.995, representing an approximately 85 percent reduction in human error rate. These results demonstrate the power of using deep learning to produce significant improvements in the accuracy of pathological diagnoses.},
  archivePrefix = {arXiv},
  arxivid = {q-bio.QM/1606.05718},
  eprint = {1606.05718},
  eprinttype = {arxiv},
  primaryClass = {q-bio.QM}
}

@article{Wang2017,
  title = {Prediction of Recurrence in Early Stage Non-Small Cell Lung Cancer Using Computer Extracted Nuclear Features from Digital {{H}}\&{{E}} Images},
  author = {Wang, Xiangxue and Janowczyk, Andrew and Zhou, Yu and Thawani, Rajat and Fu, Pingfu and Schalper, Kurt and Velcheti, Vamsidhar and Madabhushi, Anant},
  date = {2017},
  journaltitle = {Scientific Reports},
  volume = {7},
  pages = {1--10},
  publisher = {{Springer US}},
  issn = {20452322},
  doi = {10.1038/s41598-017-13773-7},
  url = {http://dx.doi.org/10.1038/s41598-017-13773-7},
  abstract = {© 2017 The Author(s). Identification of patients with early stage non-small cell lung cancer (NSCLC) with high risk of recurrence could help identify patients who would receive additional benefit from adjuvant therapy. In this work, we present a computational histomorphometric image classifier using nuclear orientation, texture, shape, and tumor architecture to predict disease recurrence in early stage NSCLC from digitized H\&E tissue microarray (TMA) slides. Using a retrospective cohort of early stage NSCLC patients (Cohort \#1, n = 70), we constructed a supervised classification model involving the most predictive features associated with disease recurrence. This model was then validated on two independent sets of early stage NSCLC patients, Cohort \#2 (n = 119) and Cohort \#3 (n = 116). The model yielded an accuracy of 81\% for prediction of recurrence in the training Cohort \#1, 82\% and 75\% in the validation Cohorts \#2 and \#3 respectively. A multivariable Cox proportional hazard model of Cohort \#2, incorporating gender and traditional prognostic variables such as nodal status and stage indicated that the computer extracted histomorphometric score was an independent prognostic factor (hazard ratio = 20.81, 95\% CI: 6.42-67.52, P {$<$} 0.001).},
  isbn = {4159801713773},
  number = {1}
}

@article{Wattenberg2016,
  title = {How to Use T-{{SNE}} Effectively},
  author = {Wattenberg, Martin and Viégas, Fernanda and Johnson, Ian},
  date = {2016},
  journaltitle = {Distill},
  volume = {1},
  pages = {e2},
  issn = {2476-0757},
  number = {10}
}

@article{wei2019pathologist,
  title = {Pathologist-Level Classification of Histologic Patterns on Resected Lung Adenocarcinoma Slides with Deep Neural Networks},
  author = {Wei, Jason W and Tafe, Laura J and Linnik, Yevgeniy A and Vaickus, Louis J and Tomita, Naofumi and Hassanpour, Saeed},
  date = {2019},
  journaltitle = {Scientific reports},
  volume = {9},
  pages = {1--8},
  publisher = {{Nature Publishing Group}},
  number = {1}
}

@article{Weidlich2018,
  title = {Artificial Intelligence in Medicine and Radiation Oncology},
  author = {Weidlich, Vincent and Weidlich, Georg A.},
  date = {2018-04},
  journaltitle = {Cureus},
  publisher = {{Cureus, Inc.}},
  doi = {10.7759/cureus.2475},
  abstract = {Artifical Intelligence (AI) was reviewed with a focus on its potential applicability to radiation oncology. The improvement of process efficiencies and the prevention of errors were found to be the most significant contributions of AI to radiation oncology. It was found that the prevention of errors is most effective when data transfer processes were automated and operational decisions were based on logical or learned evaluations by the system. It was concluded that AI could greatly improve the efficiency and accuracy of radiation oncology operations.}
}

@report{Welfare2016,
  title = {Skin Cancer in Australia},
  author = {and Welfare, Australian Institute of Health},
  date = {2016},
  institution = {{Australian Institute of Health and Welfare}},
  location = {{Canberra}},
  number = {Cat. no. CAN 96.},
  options = {useprefix=true}
}

@article{Whiteman2016,
  title = {The Growing Burden of Invasive Melanoma: Projections of Incidence Rates and Numbers of New Cases in Six Susceptible Populations through 2031},
  author = {Whiteman, David C and Green, Adele C and Olsen, Catherine M},
  date = {2016},
  journaltitle = {Journal of Investigative Dermatology},
  volume = {136},
  pages = {1161--1171},
  issn = {0022-202X},
  number = {6}
}

@article{Wu2016-gi,
  title = {Scalable \{\vphantom\}{{High}}-{{Performance}}\vphantom\{\} Image Registration Framework by Unsupervised Deep Feature Representations Learning},
  author = {Wu, Guorong and Kim, Minjeong and Wang, Qian and Munsell, Brent C and Shen, Dinggang},
  date = {2016-07},
  journaltitle = {IEEE Transactions on Biomedical Engineering},
  shortjournal = {IEEE Trans. Biomed. Eng.},
  volume = {63},
  pages = {1505--1516},
  abstract = {Feature selection is a critical step in deformable image registration. In particular, selecting the most discriminative features that accurately and concisely describe complex morphological patterns in image patches improves correspondence detection, which in turn improves image registration accuracy. Furthermore, since more and more imaging modalities are being invented to better identify morphological changes in medical imaging data, the development of deformable image registration method that scales well to new image modalities or new image applications with little to no human intervention would have a significant impact on the medical image analysis community. To address these concerns, a learning-based image registration framework is proposed that uses deep learning to discover compact and highly discriminative features upon observed imaging data. Specifically, the proposed feature selection method uses a convolutional stacked autoencoder to identify intrinsic deep feature representations in image patches. Since deep learning is an unsupervised learning method, no ground truth label knowledge is required. This makes the proposed feature selection method more flexible to new imaging modalities since feature representations can be directly learned from the observed imaging data in a very short amount of time. Using the LONI and ADNI imaging datasets, image registration performance was compared to two existing state-of-the-art deformable image registration methods that use handcrafted features. To demonstrate the scalability of the proposed image registration framework, image registration experiments were conducted on 7.0-T brain MR images. In all experiments, the results showed that the new image registration framework consistently demonstrated more accurate registration results when compared to state of the art.},
  number = {7}
}

@article{Wu2018,
  title = {Automatic Classification of Ovarian Cancer Types from Cytological Images Using Deep Convolutional Neural Networks},
  author = {Wu, Miao and Yan, Chuanbo and Liu, Huiqiang and Liu, Qian},
  date = {2018},
  journaltitle = {Bioscience reports},
  volume = {38},
  pages = {BSR20180289},
  publisher = {{Portland Press Limited}},
  issn = {0144-8463},
  number = {3}
}

@article{wu2018automatic,
  title = {Automatic Classification of Ovarian Cancer Types from Cytological Images Using Deep Convolutional Neural Networks},
  author = {Wu, Miao and Yan, Chuanbo and Liu, Huiqiang and Liu, Qian},
  date = {2018},
  journaltitle = {Bioscience reports},
  volume = {38},
  publisher = {{Portland Press}},
  number = {3}
}

@online{Xia2017,
  title = {W-Net: {{A}} Deep Model for Fully Unsupervised Image Segmentation},
  author = {Xia, Xide and Kulis, Brian},
  date = {2017-11},
  url = {http://arxiv.org/abs/1711.08506},
  abstract = {While significant attention has been recently focused on designing supervised deep semantic segmentation algorithms for vision tasks, there are many domains in which sufficient supervised pixel-level labels are difficult to obtain. In this paper, we revisit the problem of purely unsupervised image segmentation and propose a novel deep architecture for this problem. We borrow recent ideas from supervised semantic segmentation methods, in particular by concatenating two fully convolutional networks together into an autoencoder–one for encoding and one for decoding. The encoding layer produces a k-way pixelwise prediction, and both the reconstruction error of the autoencoder as well as the normalized cut produced by the encoder are jointly minimized during training. When combined with suitable postprocessing involving conditional random field smoothing and hierarchical segmentation, our resulting algorithm achieves impressive results on the benchmark Berkeley Segmentation Data Set, outperforming a number of competing methods.},
  archivePrefix = {arXiv},
  arxivid = {1711.08506},
  eprint = {1711.08506},
  eprinttype = {arxiv}
}

@inproceedings{Xie2017,
  title = {Aggregated Residual Transformations for Deep Neural Networks},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Xie, Saining and Girshick, Ross and Dollár, Piotr and Tu, Zhuowen and He, Kaiming},
  date = {2017},
  pages = {1492--1500}
}

@article{Xing2018,
  title = {Deep Learning in Microscopy Image Analysis: {{A}} Survey},
  author = {Xing, F Y and Xie, Y P and Su, H and Liu, F J and Yang, L},
  date = {2018},
  journaltitle = {Ieee Transactions on Neural Networks and Learning Systems},
  volume = {29},
  pages = {4550--4568},
  issn = {2162-237X},
  doi = {10.1109/tnnls.2017.2766168},
  url = {\{%\}3CGo to},
  number = {10}
}

@article{Xu2014-sg,
  title = {Weakly Supervised Histopathology Cancer Image Segmentation and Classification},
  author = {Xu, Yan and Zhu, Jun-Yan and Chang, Eric I-Chao and Lai, Maode and Tu, Zhuowen},
  date = {2014-04},
  journaltitle = {Medical Image Analysis},
  shortjournal = {Med. Image Anal.},
  volume = {18},
  pages = {591--604},
  abstract = {Labeling a histopathology image as having cancerous regions or not is a critical task in cancer diagnosis; it is also clinically important to segment the cancer tissues and cluster them into various classes. Existing supervised approaches for image classification and segmentation require detailed manual annotations for the cancer pixels, which are time-consuming to obtain. In this paper, we propose a new learning method, multiple clustered instance learning (MCIL) (along the line of weakly supervised learning) for histopathology image segmentation. The proposed MCIL method simultaneously performs image-level classification (cancer vs. non-cancer image), medical image segmentation (cancer vs. non-cancer tissue), and patch-level clustering (different classes). We embed the clustering concept into the multiple instance learning (MIL) setting and derive a principled solution to performing the above three tasks in an integrated framework. In addition, we introduce contextual constraints as a prior for MCIL, which further reduces the ambiguity in MIL. Experimental results on histopathology colon cancer images and cytology images demonstrate the great advantage of MCIL over the competing methods.},
  keywords = {Classification,Clustering,Histopathology image},
  number = {3}
}

@article{Xu2016-va,
  title = {A {{Deep Convolutional Neural Network}} for Segmenting and Classifying Epithelial and Stromal Regions in Histopathological Images},
  author = {Xu, Jun and Luo, Xiaofei and Wang, Guanhao and Gilmore, Hannah and Madabhushi, Anant},
  date = {2016-05},
  journaltitle = {Neurocomputing},
  volume = {191},
  pages = {214--223},
  publisher = {{Elsevier}},
  abstract = {Epithelial (EP) and stromal (ST) are two types of tissues in histological images. Automated segmentation or classification of EP and ST tissues is important when developing computerized system for analyzing the tumor microenvironment. In this paper, a Deep Convolutional Neural Networks (DCNN) based feature learning is presented to automatically segment or classify EP and ST regions from digitized tumor tissue microarrays (TMAs). Current approaches are based on handcraft feature representation, such as color, texture, and Local Binary Patterns (LBP) in classifying two regions. Compared to handcrafted feature based approaches, which involve task dependent representation, DCNN is an end-to-end feature extractor that may be directly learned from the raw pixel intensity value of EP and ST tissues in a data driven fashion. These high-level features contribute to the construction of a supervised classifier for discriminating the two types of tissues. In this work we compare DCNN based models with three handcraft feature extraction based approaches on two different datasets which consist of 157 Hematoxylin and Eosin (H\&E) stained images of breast cancer and 1376 immunohistological (IHC) stained images of colorectal cancer, respectively. The DCNN based feature learning approach was shown to have a F1 classification score of 85\%, 89\%, and 100\%, accuracy (ACC) of 84\%, 88\%, and 100\%, and Matthews Correlation Coefficient (MCC) of 86\%, 77\%, and 100\% on two H\&E stained (NKI and VGH) and IHC stained data, respectively. Our DNN based approach was shown to outperform three handcraft feature extraction based approaches in terms of the classification of EP and ST regions.},
  keywords = {Breast histopathology,Colorectal cancer,Deep Con}
}

@online{Xu2019,
  title = {Gan-Based Virtual Re-Staining: {{A}} Promising Solution for Whole Slide Image Analysis},
  author = {Xu, Zhaoyang and Moro, Carlos Fernández and Bozóky, Béla and Zhang, Qianni},
  date = {2019},
  archivePrefix = {arXiv},
  eprint = {1901.04059},
  eprinttype = {arxiv}
}

@inproceedings{yang2019exposing,
  title = {Exposing Deep Fakes Using Inconsistent Head Poses},
  booktitle = {{{ICASSP}} 2019-2019 {{IEEE}} International Conference on Acoustics, Speech and Signal Processing ({{ICASSP}})},
  author = {Yang, Xin and Li, Yuezun and Lyu, Siwei},
  date = {2019},
  pages = {8261--8265},
  organization = {{IEEE}}
}

@online{Yi2017,
  title = {Optimizing and Visualizing Deep Learning for {{Benign}}/{{Malignant}} Classification in Breast Tumors},
  author = {Yi, Darvin and Sawyer, Rebecca Lynn and Cohn III, David and Dunnmon, Jared and Lam, Carson and Xiao, Xuerong and Rubin, Daniel},
  date = {2017},
  archivePrefix = {arXiv},
  eprint = {1705.06362},
  eprinttype = {arxiv}
}

@inproceedings{Yosinski,
  title = {How Transferable Are Features in Deep Neural Networks?},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  pages = {3320--3328}
}

@article{Yu2016-qa,
  title = {Predicting Non-Small Cell Lung Cancer Prognosis by Fully Automated Microscopic Pathology Image Features},
  author = {Yu, Kun-Hsing and Zhang, Ce and Berry, Gerald J and Altman, Russ B and Ré, Christopher and Rubin, Daniel L and Snyder, Michael},
  date = {2016-08},
  journaltitle = {Nature Communications},
  shortjournal = {Nat. Commun.},
  volume = {7},
  pages = {12474},
  publisher = {{nature.com}},
  abstract = {Lung cancer is the most prevalent cancer worldwide, and histopathological assessment is indispensable for its diagnosis. However, human evaluation of pathology slides cannot accurately predict patients' prognoses. In this study, we obtain 2,186 haematoxylin and eosin stained histopathology whole-slide images of lung adenocarcinoma and squamous cell carcinoma patients from The Cancer Genome Atlas (TCGA), and 294 additional images from Stanford Tissue Microarray (TMA) Database. We extract 9,879 quantitative image features and use regularized machine-learning methods to select the top features and to distinguish shorter-term survivors from longer-term survivors with stage I adenocarcinoma (P{$<$}0.003) or squamous cell carcinoma (P=0.023) in the TCGA data set. We validate the survival prediction framework with the TMA cohort (P{$<$}0.036 for both tumour types). Our results suggest that automatically derived image features can predict the prognosis of lung cancer patients and thereby contribute to precision oncology. Our methods are extensible to histopathology images of other organs.}
}

@article{Yu2017-le,
  title = {Automated Melanoma Recognition in Dermoscopy Images via Very Deep Residual Networks},
  author = {Yu, Lequan and Chen, Hao and Dou, Qi and Qin, Jing and Heng, Pheng-Ann},
  date = {2017-04},
  journaltitle = {IEEE Transactions on Medical Imaging},
  shortjournal = {IEEE Trans. Med. Imaging},
  volume = {36},
  pages = {994--1004},
  publisher = {{ieeexplore.ieee.org}},
  abstract = {Automated melanoma recognition in dermoscopy images is a very challenging task due to the low contrast of skin lesions, the huge intraclass variation of melanomas, the high degree of visual similarity between melanoma and non-melanoma lesions, and the existence of many artifacts in the image. In order to meet these challenges, we propose a novel method for melanoma recognition by leveraging very deep convolutional neural networks (CNNs). Compared with existing methods employing either low-level hand-crafted features or CNNs with shallower architectures, our substantially deeper networks (more than 50 layers) can acquire richer and more discriminative features for more accurate recognition. To take full advantage of very deep networks, we propose a set of schemes to ensure effective training and learning under limited training data. First, we apply the residual learning to cope with the degradation and overfitting problems when a network goes deeper. This technique can ensure that our networks benefit from the performance gains achieved by increasing network depth. Then, we construct a fully convolutional residual network (FCRN) for accurate skin lesion segmentation, and further enhance its capability by incorporating a multi-scale contextual information integration scheme. Finally, we seamlessly integrate the proposed FCRN (for segmentation) and other very deep residual networks (for classification) to form a two-stage framework. This framework enables the classification network to extract more representative and specific features based on segmented results instead of the whole dermoscopy images, further alleviating the insufficiency of training data. The proposed framework is extensively evaluated on ISBI 2016 Skin Lesion Analysis Towards Melanoma Detection Challenge dataset. Experimental results demonstrate the significant performance gains of the proposed framework, ranking the first in classification and the second in segmentation among 25 teams and 28 teams, respectively. This study corroborates that very deep CNNs with effective training mechanisms can be employed to solve complicated medical image analysis tasks, even with limited training data.},
  number = {4}
}

@article{Yu2018,
  title = {Artificial Intelligence in Healthcare},
  author = {Yu, Kun Hsing and Beam, Andrew L. and Kohane, Isaac S.},
  date = {2018-10},
  journaltitle = {Nature biomedical engineering},
  volume = {2},
  pages = {719--731},
  publisher = {{Nature Publishing Group}},
  issn = {2157846X},
  doi = {10.1038/s41551-018-0305-z},
  abstract = {Artificial intelligence (AI) is gradually changing medical practice. With recent progress in digitized data acquisition, machine learning and computing infrastructure, AI applications are expanding into areas that were previously thought to be only the province of human experts. In this Review Article, we outline recent breakthroughs in AI technologies and their biomedical applications, identify the challenges for further progress in medical AI systems, and summarize the economic, legal and social implications of AI in healthcare.},
  number = {10}
}

@article{Yuan2017-bc,
  title = {Automatic Skin Lesion Segmentation Using Deep Fully Convolutional Networks with Jaccard Distance},
  author = {Yuan, Yading and Chao, Ming and Lo, Yeh-Chi},
  date = {2017-09},
  journaltitle = {IEEE Transactions on Medical Imaging},
  shortjournal = {IEEE Trans. Med. Imaging},
  volume = {36},
  pages = {1876--1886},
  publisher = {{ieeexplore.ieee.org}},
  abstract = {Automatic skin lesion segmentation in dermoscopic images is a challenging task due to the low contrast between lesion and the surrounding skin, the irregular and fuzzy lesion borders, the existence of various artifacts, and various imaging acquisition conditions. In this paper, we present a fully automatic method for skin lesion segmentation by leveraging 19-layer deep convolutional neural networks that is trained end-to-end and does not rely on prior knowledge of the data. We propose a set of strategies to ensure effective and efficient learning with limited training data. Furthermore, we design a novel loss function based on Jaccard distance to eliminate the need of sample re-weighting, a typical procedure when using cross entropy as the loss function for image segmentation due to the strong imbalance between the number of foreground and background pixels. We evaluated the effectiveness, efficiency, as well as the generalization capability of the proposed framework on two publicly available databases. One is from ISBI 2016 skin lesion analysis towards melanoma detection challenge, and the other is the PH2 database. Experimental results showed that the proposed method outperformed other state-of-the-art algorithms on these two databases. Our method is general enough and only needs minimum pre- and post-processing, which allows its adoption in a variety of medical image segmentation tasks.},
  number = {9}
}

@inproceedings{Zeiler2014-qu,
  title = {Visualizing and Understanding Convolutional Networks},
  booktitle = {Computer Vision – \{\vphantom\}{{ECCV}}\vphantom\{\} 2014},
  author = {Zeiler, Matthew D and Fergus, Rob},
  date = {2014},
  pages = {818--833},
  publisher = {{Springer International Publishing}},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.}
}

@article{Zeng2016,
  title = {Preparation of Novel High Copper Ions Removal Membranes by Embedding Organosilane-Functionalized Multi-Walled Carbon Nanotube},
  author = {Zeng, Guangyong and He, Yi and Yu, Zongxue and Yang, Xi and Yang, Ranran and Zhang, Lei},
  date = {2016},
  journaltitle = {Journal of Chemical Technology and Biotechnology},
  volume = {91},
  pages = {2322--2330},
  issn = {10974660},
  doi = {10.1002/jctb.4820},
  abstract = {© 2015 Society of Chemical Industry BACKGROUND: Multi-walled carbon nanotubes (MWCNTs) have attracted considerable interest in the membrane field, but they are prone to aggregate in the polymer matrix and cause membrane defects. Different from blending MWCNTs in the casting solution, different ratios of APTS-functionalized MWCNTs (A-MWCNTs) were embedded on the surface of polyvinylidene fluoride (PVDF) membranes. RESULTS: FTIR and XPS demonstrated the reaction between MWCNTs and APTS. SEM and AFM images showed that new morphology and pore structure appeared in novel A-MWCNTs/PVDF membranes compared with pure PVDF membrane. The embedding was firm because of APTS chains penetrating into the PVDF matrix, and the optimum content of A-MWCNTs was 0.05 wt\%. Besides, A-MWCNTs/PVDF membranes exhibited superior contact angle and BSA rejection than PVDF membrane. In the tests of copper ions rejection and adsorption, 0.05 wt\% A-MWCNTs/PVDF membrane gave 89.1\% removal and 2.067 mg g −1 adsorption capacity compared with PVDF membrane, at just 19.66\% and 0.516 mg g −1 , respectively. CONCLUSIONS: The existence of functional groups in A-MWCNTs such as –NH 2 and –COOH, can not only increase the hydrophilicity of membranes, but also provide more adsorption sites to have a complexation reaction with Cu 2+ ions. This work provides information to solve practical problems in the field of wastewater treatment. © 2015 Society of Chemical Industry.},
  keywords = {A-MWCNTs,embedded modification,microstructure,PVDF membranes,rejection},
  number = {8}
}

@article{Zhang2015,
  title = {Deep Convolutional Neural Networks for Multi-Modality Isointense Infant Brain Image Segmentation},
  author = {Zhang, Wenlu and Li, Rongjian and Deng, Houtao and Wang, Li and Lin, Weili and Ji, Shuiwang and Shen, Dinggang},
  date = {2015},
  journaltitle = {NeuroImage},
  volume = {108},
  pages = {214--224},
  issn = {1053-8119}
}

@inproceedings{Zhang2017,
  title = {Image Segmentation with Pyramid Dilated Convolution Based on {{ResNet}} and {{U}}-{{Net}}},
  booktitle = {International Conference on Neural Information Processing},
  author = {Zhang, Qiao and Cui, Zhipeng and Niu, Xiaoguang and Geng, Shijie and Qiao, Yu},
  date = {2017},
  pages = {364--372},
  publisher = {{Springer}}
}

@article{Zhang2017-lo,
  title = {Deep Learning Based Feature Representation for Automated Skin Histopathological Image Annotation},
  author = {Zhang, Gang and Hsu, Ching-Hsien Robert and Lai, Huadong and Zheng, Xianghan},
  date = {2017-05},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed. Tools Appl.},
  pages = {1--21},
  publisher = {{Springer US}},
  abstract = {Automated annotation of skin biopsy histopathological images provides valuable information and supports for diagnosis, especially for the discrimination between malignant and benign lesions. Currently, computer-aid analysis of skin biopsy images mostly relied on some human-designed features, which requires expensive human efforts and experiences in problem domains. In this study, we propose an annotation framework for automated skin biopsy image analysis which makes use of a deep model for image feature representation. A convolutional neural network (CNN) is designed for local regions of skin biopsy images which learns potential high-level features automatically from input raw pixels. The annotation model is constructed in the multiple-instance multiple-label (MIML) learning framework with the features learned through the network. We achieve significant improvement of the model performance on a real world clinical skin biopsy image dataset and a benchmark dataset. Moreover, our study indicates that deep learning based model could achieve better performance than human designed features.}
}

@inproceedings{Zhang2017a,
  title = {Mdnet: {{A}} Semantically and Visually Interpretable Medical Image Diagnosis Network},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Zhang, Zizhao and Xie, Yuanpu and Xing, Fuyong and McGough, Mason and Yang, Lin},
  date = {2017},
  pages = {6428--6436}
}

@inproceedings{Zhang2017b,
  title = {Stackgan: {{Text}} to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris N},
  date = {2017},
  pages = {5907--5915}
}

@article{Zhang2018,
  title = {Pulmonary Nodule Detection in Medical Images: {{A}} Survey},
  author = {Zhang, J J and Xia, Y and Cui, H F and Zhang, Y N},
  date = {2018},
  journaltitle = {Biomedical Signal Processing and Control},
  volume = {43},
  pages = {138--147},
  issn = {1746-8094},
  doi = {10.1016/j.bspc.2018.01.011},
  url = {\{%\}3CGo to}
}

@article{Zhang2019,
  title = {Pathologist-Level Interpretable Whole-Slide Cancer Diagnosis with Deep Learning},
  author = {Zhang, Zizhao and Chen, Pingjun and McGough, Mason and Xing, Fuyong and Wang, Chunbao and Bui, Marilyn and Xie, Yuanpu and Sapkota, Manish and Cui, Lei and Dhillon, Jasreman and Ahmad, Nazeel and Khalil, Farah K. and Dickinson, Shohreh I. and Shi, Xiaoshuang and Liu, Fujun and Su, Hai and Cai, Jinzheng and Yang, Lin},
  date = {2019},
  journaltitle = {Nature Machine Intelligence},
  volume = {1},
  pages = {289--289},
  publisher = {{Springer US}},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0062-z},
  url = {http://dx.doi.org/10.1038/s42256-019-0052-1},
  abstract = {Diagnostic pathology is the foundation and gold standard for identifying carcinomas. However, high inter-observer variability substantially affects productivity in routine pathology and is especially ubiquitous in diagnostician-deficient medical centres. Despite rapid growth in computer-aided diagnosis (CAD), the application of whole-slide pathology diagnosis remains impractical. Here, we present a novel pathology whole-slide diagnosis method, powered by artificial intelligence, to address the lack of interpretable diagnosis. The proposed method masters the ability to automate the human-like diagnostic reasoning process and translate gigapixels directly to a series of interpretable predictions, providing second opinions and thereby encouraging consensus in clinics. Moreover, using 913 collected examples of whole-slide data representing patients with bladder cancer, we show that our method matches the performance of 17 pathologists in the diagnosis of urothelial carcinoma. We believe that our method provides an innovative and reliable means for making diagnostic suggestions and can be deployed at low cost as next-generation, artificial intelligence-enhanced CAD technology for use in diagnostic pathology.},
  isbn = {4225601900},
  number = {6}
}

@article{zhang2019pathologist,
  title = {Pathologist-Level Interpretable Whole-Slide Cancer Diagnosis with Deep Learning},
  author = {Zhang, Zizhao and Chen, Pingjun and McGough, Mason and Xing, Fuyong and Wang, Chunbao and Bui, Marilyn and Xie, Yuanpu and Sapkota, Manish and Cui, Lei and Dhillon, Jasreman and others},
  date = {2019},
  journaltitle = {Nature Machine Intelligence},
  volume = {1},
  pages = {236--245},
  publisher = {{Nature Publishing Group}},
  number = {5}
}

@inproceedings{zhang2019self,
  title = {Self-Attention Generative Adversarial Networks},
  booktitle = {International Conference on Machine Learning},
  author = {Zhang, Han and Goodfellow, Ian and Metaxas, Dimitris and Odena, Augustus},
  date = {2019},
  pages = {7354--7363},
  organization = {{PMLR}}
}

@article{Zhong2017-qr,
  title = {When Machine Vision Meets Histology: {{A}} Comparative Evaluation of Model Architecture for Classification of Histology Sections},
  author = {Zhong, Cheng and Han, Ju and Borowsky, Alexander and Parvin, Bahram and Wang, Yunfu and Chang, Hang},
  date = {2017-01},
  journaltitle = {Medical Image Analysis},
  shortjournal = {Med. Image Anal.},
  volume = {35},
  pages = {530--543},
  abstract = {Classification of histology sections in large cohorts, in terms of distinct regions of microanatomy (e.g., stromal) and histopathology (e.g., tumor, necrosis), enables the quantification of tumor composition, and the construction of predictive models of genomics and clinical outcome. To tackle the large technical variations and biological heterogeneities, which are intrinsic in large cohorts, emerging systems utilize either prior knowledge from pathologists or unsupervised feature learning for invariant representation of the underlying properties in the data. However, to a large degree, the architecture for tissue histology classification remains unexplored and requires urgent systematical investigation. This paper is the first attempt to provide insights into three fundamental questions in tissue histology classification: I. Is unsupervised feature learning preferable to human engineered features? II. Does cellular saliency help? III. Does the sparse feature encoder contribute to recognition? We show that (a) in I, both Cellular Morphometric Feature and features from unsupervised feature learning lead to superior performance when compared to SIFT and [Color, Texture]; (b) in II, cellular saliency incorporation impairs the performance for systems built upon pixel-/patch-level features; and (c) in III, the effect of the sparse feature encoder is correlated with the robustness of features, and the performance can be consistently improved by the multi-stage extension of systems built upon both Cellular Morphmetric Feature and features from unsupervised feature learning. These insights are validated with two cohorts of Glioblastoma Multiforme (GBM) and Kidney Clear Cell Carcinoma (KIRC).},
  keywords = {Classification,Computational histopathology,Spar}
}

@inproceedings{Zoph2018,
  title = {Learning Transferable Architectures for Scalable Image Recognition},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V},
  date = {2018},
  pages = {8697--8710}
}

@inproceedings{Zormpas-Petridis2018,
  title = {Capturing Global Spatial Context for Accurate Cell Classification in Skin Cancer Histology},
  booktitle = {Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  author = {Zormpas-Petridis, Konstantinos and Failmezger, Henrik and Roxanis, Ioannis and Blackledge, Matthew and Jamin, Yann and Yuan, Yinyin},
  date = {2018},
  volume = {11039 LNCS},
  pages = {52--60},
  publisher = {{Springer Verlag}},
  issn = {16113349},
  doi = {10.1007/978-3-030-00949-6_7},
  abstract = {The spectacular response observed in clinical trials of immunotherapy in patients with previously uncurable Melanoma, a highly aggressive form of skin cancer, calls for a better understanding of the cancer-immune interface. Computational pathology provides a unique opportunity to spatially dissect such interface on digitised pathological slides. Accurate cellular classification is a key to ensure meaningful results, but is often challenging even with state-of-art machine learning and deep learning methods. We propose a hierarchical framework, which mirrors the way pathologists perceive tumour architecture and define tumour heterogeneity to improve cell classification methods that rely solely on cell nuclei morphology. The SLIC superpixel algorithm was used to segment and classify tumour regions in low resolution H\&E-stained histological images of melanoma skin cancer to provide a global context. Classification of superpixels into tumour, stroma, epidermis and lumen/white space, yielded a 97.7\% training set accuracy and 95.7\% testing set accuracy in 58 whole-tumour images of the TCGA melanoma dataset. The superpixel classification was projected down to high resolution images to enhance the performance of a single cell classifier, based on cell nuclear morphological features, and resulted in increasing its accuracy from 86.4\% to 91.6\%. Furthermore, a voting scheme was proposed to use global context as biological a priori knowledge, pushing the accuracy further to 92.8\%. This study demonstrates how using the global spatial context can accurately characterise the tumour microenvironment and allow us to extend significantly beyond single-cell morphological classification.},
  isbn = {978-3-030-00948-9},
  keywords = {Cell classification,Hierarchical model,Histology image processing}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }

