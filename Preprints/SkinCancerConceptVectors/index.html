<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
		<head>
				<meta charset="utf-8" />
				<meta name="generator" content="pandoc" />
				<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
				<title>document</title>
				<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
				</style>
				<link rel="stylesheet" href="./css/base_style.css" />
				<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
				<!--[if lt IE 9]>
						<script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
				<![endif]-->
		</head>
		<body>
				<nav id="TOC">
						<ul>
								<li><a href="#characterization-of-tissue-types-in-basal-cell-carcinoma-images-via-generative-modelling-and-concept-vectors">Characterization of Tissue Types in Basal Cell Carcinoma Images via Generative Modelling and Concept Vectors</a><ul>
												<li><a href="#introduction">1. Introduction</a><ul>
																<li><a href="#the-future-of-pathology">1.1 The Future of Pathology</a></li>
																<li><a href="#interpretable-diagnostic-systems">1.2 Interpretable Diagnostic Systems</a></li>
																<li><a href="#generative-methods-for-interpretability">1.3 Generative Methods for Interpretability</a></li>
																<li><a href="#application-to-basal-cell-carcinoma">1.4 Application to Basal Cell Carcinoma</a></li>
														</ul></li>
														<li><a href="#methods">2. Methods</a><ul>
																		<li><a href="#dataset">2.1 Dataset</a></li>
																		<li><a href="#network-architecture">2.2 Network Architecture</a></li>
																		<li><a href="#training-regime">2.3 Training Regime</a></li>
																		<li><a href="#concept-vectors">2.4 Concept Vectors</a></li>
																</ul></li>
																<li><a href="#results-discussion">3. Results &amp; Discussion</a><ul>
																				<li><a href="#encoding-decoding-and-random-generation-of-images">3.1 Encoding, Decoding and Random Generation of Images</a></li>
																				<li><a href="#training-stability">3.2 Training Stability</a></li>
																				<li><a href="#image-domain-coverage">3.3 Image Domain Coverage</a></li>
																				<li><a href="#feature-discrimination-with-concept-vectors">3.4 Feature Discrimination with Concept Vectors</a></li>
																				<li><a href="#latent-space-structure">3.5 Latent Space Structure</a></li>
																				<li><a href="#whole-image-characterization">3.6 Whole-Image Characterization</a></li>
																				<li><a href="#minimal-concept-vector-definitions">3.7 Minimal Concept Vector Definitions</a></li>
																				<li><a href="#latent-space-complexity">3.8 Latent Space Complexity</a></li>
																				<li><a href="#image-domain-complexity">3.9 Image Domain Complexity</a></li>
																		</ul></li>
																		<li><a href="#conclusion">4. Conclusion</a></li>
																		<li><a href="#declaration-of-competing-interest">Declaration of Competing Interest</a></li>
																		<li><a href="#acknowledgements">Acknowledgements</a></li>
																		<li><a href="#appendix">Appendix</a></li>
										</ul></li>
						</ul>
				</nav>
				<!--


			PREPRINT CODE - Move comment to include/exclude

				-->
				<div id="preprint" style="color: grey; display: block; text-align: center;">
						<h1>
								Preprint
								</h3>
								<p>
								<strong>Posted</strong>: 28 March 2021 | <strong>Updated</strong>: <a id="update">28 March 2021</a> | <strong>Status</strong>: <span style="color:orangered">In Review</span> | <span id="citeToggle"><strong>Cite this Article ▾</strong></span>
								</p>
								<div id="cite" style="display: none;">
										<pre>
@article{thomas2021CVGM,
  title = {Characterization of Tissue Types in Basal Cell Carcinoma Images via Generative Modelling and Concept Vectors},
  author = {Thomas, Simon M and Lefevre, James G and Baxter, Glenn and Hamilton, Nicholas A},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub Pages},
  howpublished = {\url{https://github.com/charlespwd/project-title}},
} 
										</pre>
								</div>
								<script>

										// Sets toggle for cite
										let  toggle = document.getElementById("citeToggle");
let cite = document.getElementById("cite");
toggle.onclick = function() {
		if (cite.style.display === "block"){
				cite.style.display = "none";
		} else {
				cite.style.display = "block";
		}
}

								</script>
								<hr style="border-top: 10px solid #006699">
				</div>
				<h1 id="characterization-of-tissue-types-in-basal-cell-carcinoma-images-via-generative-modelling-and-concept-vectors">Characterization of Tissue Types in Basal Cell Carcinoma Images via Generative Modelling and Concept Vectors</h1>
				<h4 id="authors">Authors</h4>
				<ol type="1">
						<li><a href="https://orcid.org/0000-0003-4609-2732">Thomas, S. M.</a><sup><strong>a</strong></sup></li>
						<li><a href="https://orcid.org/0000-0002-5945-9575">Lefevre, J. G.</a><sup><strong>a</strong></sup></li>
						<li>Baxter, G.<sup><strong>b</strong></sup></li>
						<li><a href="https://orcid.org/0000-0003-0331-3427">Hamilton, N.A.</a><sup><strong>a</strong></sup></li>
				</ol>
				<p><strong>a</strong> - Institute for Molecular Bioscience, The University of Queensland, St Lucia, Australia</p>
				<p><strong>b</strong> - MyLab Pathology, Salisbury, Australia</p>
				<div id="abstract">
						<h4 id="abstract">Abstract</h4>
						<p><i>The promise of machine learning methods to act as decision support systems for pathologists continues to grow. However, central to their successful adoption must be interpretable implementations so that people can trust and learn from them effectively. Generative modeling, most notable in the form of adversarial generative models, is a naturally interpretable technique because the quality of the model is explicit from the quality of images it generates. Such a model can be further assessed by exploring its latent space, using human-meaningful concepts by defining concept vectors. Motivated by these ideas, we apply for the first time generative methods to histological images of basal cell carcinoma (BCC). By simultaneously learning to generate and encode realistic image patches, we extract feature rich latent vectors that correspond to various tissue morphologies, namely BCC, epidermis, keratin, papillary dermis and inflammation. We show that a logistic regression model trained on these latent vectors can achieve high classification accuracies across 6 binary tasks (86-98%). Further, by projecting the latent vectors onto learned concept vectors we can generate a score for the absence or degree of presence for a given concept, providing semantically accurate “conceptual summaries” of the various tissues types within a patch. This can be extended to generate multi-dimensional heat maps for whole-image specimens, which characterizes the tissue in a similar way to a pathologist. We additionally find that accurate concept vectors can be defined using a small labeled dataset. </i></p>
				</div>
				<section id="introduction">
						<h2 id="introduction">1. Introduction</h2>
						<h3 id="the-future-of-pathology">1.1 The Future of Pathology</h3>
						<p>Computational Pathology is a growing field that is concomitant with the increasing availability of digitized histological imaging. Central to the field is the application of machine learning technology, trained to detect, segment, grade and classify cancers of various types<span class="citation" data-cites="bulten2020artificial Campanella2019 sirinukunwattana2017gland wu2018automatic"><sup><a href="#ref-bulten2020artificial">1</a>–<a href="#ref-wu2018automatic">4</a></sup></span>. Designed as decision-support tools, there is growing opportunity to incorporate computational analysis into the pathologist workflow<span class="citation" data-cites="bera2019artificial"><sup><a href="#ref-bera2019artificial">5</a></sup></span>. Indeed, as pathology departments transition to digital imaging, it is expected that Artificial Intelligence (AI) approaches will be necessary to analyse and interpret high-volume data. As the machine learning field matures, it is anticipated that pattern recognition tasks will become entirely automated. This is motivated by recent publications which match expert-level performance on classification and grading tasks<span class="citation" data-cites="zhang2019pathologist wei2019pathologist Hekler2019"><sup><a href="#ref-zhang2019pathologist">6</a>–<a href="#ref-Hekler2019">8</a></sup></span>.</p>
						<p>With the promise of computational pathology, it is important to recognize that pattern recognition is not the entirety of the role of pathologists. Pathologists are disease experts, whose knowledge spans chemistry, molecular biology, genetics and physiology. They have sophisticated models of and explanations for disease, integrating information across several domains. Observed patterns tend to fit within this framework. If they do not, pathologists can also conjecture new explanations which may ultimately further our scientific understanding. We should therefore expect that decision-support systems will feed into the pathologist information pool, alongside immunohistochemistry, genetic testing, individual circumstance and family history.</p>
						<h3 id="interpretable-diagnostic-systems">1.2 Interpretable Diagnostic Systems</h3>
						<p>In developing these tools, it is important to consider what we really want from decision support systems. What are we directing them to learn, and what are we wanting them to do? In the context of histopathology, basal cell carcinoma (BCC) is the most common form of skin cancer (approx. 60% of all diagnoses<span class="citation" data-cites="Staples2006-kp"><sup><a href="#ref-Staples2006-kp">9</a></sup></span>). This disease manifests as varying-sized groupings of small purple cells under H&amp;E staining (<a href="#fig1">Fig. 1</a>). In many cases, a rule of “purple pixels” would enable a sufficiently accurate classification. However, in difficult cases, the morphology can overlap with other cancer types, such as intraepidermal carcinoma or basosquamous carcinoma. The “purple pixels” rule would fail to discriminate in those instances. We know why such a rule fails, and would understand why such a classifier could make that error. But ideally, we would like to train a system to effectively discriminate between those various cancer types. Indeed, it would be valuable, arguably necessary, to know the discriminatory features the system uses, just as a junior pathologist must learn them from a senior pathologist.</p>
						<figure id="fig1">
								<img src="./assets/skin_layers.png">
								<figcaption>
										<strong>Figure 1</strong>: The main tissue types found within the superficial layers of the skin. The keratin layer varies in thickness depending on the site of the specimen. It sits above the epidermis, which is the healthy epithelial tissue of the skin. It is supported by the papillary dermis, which provides elasticity and strength to skin, and contains vessels to nourish the epidermis. Within the dermal region areas of inflammation are common, often associated with disease states, such as basal cell carcinoma (BCC). BCC emerges from the basal layer of the epidermis, and grows down into the dermis.
								</figcaption>
						</figure>
						<p>Unfortunately, the rules which machine learning systems learn on their own are not directly available from the trained model. Instead, we ourselves must guess at what the system is using. Experience tells us that neural networks tend to identify the most discriminatory features, independent of their relevance to the human-meaningful context. This may involve a special case of over-fitting, where the system has learned to use spurious features which do not generalise. For example, one deep neural network learned to predict hip fractures in radiographs by utilising confounding patient and hospital data, performing no better than random on an external test set<span class="citation" data-cites="badgeley2019deep"><sup><a href="#ref-badgeley2019deep">10</a></sup></span>. In this case, the features we would expect it to use (<em>i.e</em>. fractures), were not learned or utilized. The authors conclude that these results impinge on the ability of computers and clinicians to effectively cooperate.</p>
						<p>It is critical to understand what features a system is using in its decision processes. Attempts to find out have lead to the field of machine learning interpretability. Attribution methods using gradients<span class="citation" data-cites="Selvaraju2016 Dabkowski2017 Smilkov2017 Selvaraju2017"><sup><a href="#ref-Selvaraju2016">11</a>–<a href="#ref-Selvaraju2017">14</a></sup></span> emerged as a means of finding what part of the image most contributes to a particular prediction. The exact features are then <em>inferred</em> from a heat-map overlaying the image<span class="citation" data-cites="Adebayo2018"><sup><a href="#ref-Adebayo2018">15</a></sup></span>. Feature visualization techniques<span class="citation" data-cites="Olah2017 Olah2018 Carter2019"><sup><a href="#ref-Olah2017">16</a>–<a href="#ref-Carter2019">18</a></sup></span> attempt to identify features directly, optimizing input images to elicit a desired response from the network. This technique reveals that for well-constrained problems (e.g. 1,000 class object classification), networks tend to utilise human-meaningful features to discriminate object classes. This is because human-meaningful features often are robust means of discrimination. Later work suggests that models can be disincentivized from learning spurious features by additional adversarial training<span class="citation" data-cites="engstrom2019adversarial"><sup><a href="#ref-engstrom2019adversarial">19</a></sup></span>. This enables feature visualisation to be performed natively, without requiring natural-image regularization techniques central to all previous works<span class="citation" data-cites="Olah2017 Mahendran2015 nguyen2016synthesizing"><sup><a href="#ref-Olah2017">16</a>,<a href="#ref-Mahendran2015">20</a>,<a href="#ref-nguyen2016synthesizing">21</a></sup></span>. Indeed, when trying to reconstruct images from their feature representations<span class="citation" data-cites="dosovitskiy2016inverting Mahendran2015"><sup><a href="#ref-Mahendran2015">20</a>,<a href="#ref-dosovitskiy2016inverting">22</a></sup></span>, it was found that adversarially trained networks preserve natural image characteristics<span class="citation" data-cites="engstrom2019adversarial"><sup><a href="#ref-engstrom2019adversarial">19</a></sup></span>, enabling the original image to be reliably reconstructed. Further, using just the implicit knowledge in classifiers, adversarial generator networks (image synthesis) can be trained to produce diverse and realistic images of any class<span class="citation" data-cites="nguyen2017plug"><sup><a href="#ref-nguyen2017plug">23</a></sup></span>.</p>
						<p>Collectively these works lead us to think that there is an important link between the tasks of image classification and generation. They are connected by high quality learned features/representations, which when suitable for one task compliment the other. Critical to learning these features is the inclusion of adversarial training, either as part of classification or image generation tasks. The advantage of generative tasks is that interpretability is built in to the model itself. It is explicit from the ability to generate images realistically whether a network has learned the correct features. It shows us directly what it has and has not learned, and, although still qualitative, can be understood by domain experts. Such a system is thus “naturally interpretable”<span class="citation" data-cites="rudinStopExplainingBlack2019"><sup><a href="#ref-rudinStopExplainingBlack2019">24</a></sup></span>. We therefore consider the exploration of generative modelling a promising means to achieve both a high degree of discriminatory performance and interpretability.</p>
						<h3 id="generative-methods-for-interpretability">1.3 Generative Methods for Interpretability</h3>
						<p>As proposed above, the appeal of generative methods is that they can show explicitly the quality of the knowledge that has been learned. We infer from a network’s ability to generate diverse and realistic images that it has learned important characteristics of the data, while also minimising the presence of spurious features. In other words, instantiated in the network’s latent space is an explanation of how the image domain works.</p>
						<p>Generative Adversarial Networks (GANs) have demonstrated an impressive ability to model complex image distributions<span class="citation" data-cites="Brock2018"><sup><a href="#ref-Brock2018">25</a></sup></span>, in particular for generating realistic human faces<span class="citation" data-cites="karras2017progressive Karras2019 Karras2020"><sup><a href="#ref-karras2017progressive">26</a>–<a href="#ref-Karras2020">28</a></sup></span>. By labeling images as containing a particular concept <em>e.g.</em> “smiling”, the feature representations of these images can be used to define <strong>concept vectors</strong> (Section 2.4). By interpolating points along the vector we can generate images which fall along a spectrum of smiling to not-smiling. The fact that concepts can be defined arbitrarily <em>e.g.</em> gender, hair-color, age<span class="citation" data-cites="pidhorskyi2020adversarial"><sup><a href="#ref-pidhorskyi2020adversarial">29</a></sup></span>, provides enormous flexibility and power to this technique, especially towards model interpretability. We can scrutinize not just the quality of the images, but the quality and structure of the latent space, all in terms of human-meaningful concepts. Visualisation show us whether the space is continuous and cohesive, or arranged in a parsimonious way <em>e.g.</em> transitioning between two faces doesn’t have an intermediate point that exhibits discontinuities.</p>
						<p>GANs have previously been applied to several cancer types and image problems in digital pathology<span class="citation" data-cites="cho2017neural tarek2018staingan kovalev2019examining Levine2020 teramoto2020deep"><sup><a href="#ref-cho2017neural">30</a>–<a href="#ref-teramoto2020deep">34</a></sup></span>, however their full potential for model interpretability has only just begun to be explored. The foundational work towards this end is PathologyGAN<span class="citation" data-cites="Quiros2019"><sup><a href="#ref-Quiros2019">35</a></sup></span>, trained to generate breast cancer tissue, and subsequently invert the network to encode real images<span class="citation" data-cites="Quiros2020"><sup><a href="#ref-Quiros2020">36</a></sup></span>. They demonstrate that high-level features can be learned and manipulated by interacting with the latent space. However, the application to classification tasks is not explored. Further, the dataset is derived from tissue microarrays, which removes the whole-tissue context from the images.</p>
						<h3 id="application-to-basal-cell-carcinoma">1.4 Application to Basal Cell Carcinoma</h3>
						<p>In this study we apply generative modelling to basal cell carcinoma images for the first time. With the recent introduction of the Adversarial Latent Autoencoder (ALAE)<span class="citation" data-cites="pidhorskyi2020adversarial"><sup><a href="#ref-pidhorskyi2020adversarial">29</a></sup></span>, it is possible to train generative models in a stable and reliable manner, as well as simultaneously encode real images into the latent space. We train an ALAE network to encode and generate realistic images of the tissue types found in the superficial layers of the skin (<a href="#fig1">Fig. 1</a>). We build upon previous work by selecting the dataset so that the network can learn the full-context of the image domain, and thus model the relationships between tissue types in an unsupervised manner. We then use labelled images to define concept vectors in terms of tissue types (BCC, epidermis, keratin, papillary dermis, inflammation and background) to inspect the quality of the model and latent space. This additionally enables us to perform classification to characterize image patches and whole-tissue images in a naturally interpretable way.</p>
				</section>
				<section id="methods">
						<h2 id="methods">2. Methods</h2>
						<h3 id="dataset">2.1 Dataset</h3>
						<p>We utilized the non-melanoma skin cancer segmentation dataset created in previous work<span class="citation" data-cites="thomas2020interpretable"><sup><a href="#ref-thomas2020interpretable">37</a></sup></span>. The data consists of 290 H&amp;E slides representing typical cases of basal cell carcinoma, squamous cell carcinoma and intraepidermal carcinoma, across shave, punch and excision biopsy specimens. The data is annotated with segmentation ground-truth at the pixel level for 12 independent tissue classes. Using this data, we created a new dataset of overlapping patches (256×256 pixels with 50% overlap) containing the following tissue types: BCC, keratin (KER), epidermis (EPI), papillary dermis (PAP), inflammation (INF) and background (BKG). These tissue types are located in the superficial layers of the skin, and the overlapping patches capture their contiguous spatial relationships. The only condition of inclusion was that the patch contained at least 1 pixel of either the EPI or BCC class from the segmentation ground-truth (BCC originates in the epidermis). This enabled the surrounding classes to be included within a radius of at most 181 pixels (~121μm). The patches were assigned to training, validation and test splits (70:15:15) based on their original whole-slide image. The resulting dataset contained 53,586 training, 16,862 validation, and 14,076 testing images.</p>
						<h3 id="network-architecture">2.2 Network Architecture</h3>
						<p>The Adversarial Latent Autoencoder (ALAE)<span class="citation" data-cites="pidhorskyi2020adversarial"><sup><a href="#ref-pidhorskyi2020adversarial">29</a></sup></span> provides a highly generalizable approach to adversarial training and showcases impressive results across image reconstruction, generation, and manipulation. The unique training regime of the ALAE derives from decomposing the traditional GAN paradigm of two networks into four networks. The traditional generator is decomposed into a mapping network <span class="math inline">\(F\)</span>, and a generator <span class="math inline">\(G\)</span>, and the traditional discriminator is decomposed into an encoder <span class="math inline">\(E\)</span> and discriminator <span class="math inline">\(D\)</span> (<a href="#fig2">Fig. 2</a>). It is trained with three separate update steps.</p>
						<ol type="I">
								<li>Encoder &amp; Discriminator : train to discriminate between real and fake images</li>
								<li>Mapper &amp; Generator: train to make fake samples match real distribution</li>
								<li>Generator &amp; Encoder: train to auto-encode the latent vector using MSE</li>
						</ol>
						<p>This architecture thus balances both the adversarial and autoencoding components of the loss, resulting in highly stable training. The above update steps can be formalised as follows:</p>
						<p><span class="math display">\[ 
								\min_{F, G}\max_{E, D} V(F, G, E,  D) \\
								\min_{E,G}\Delta(F || E \circ G \circ F)
								\]</span></p>
						<p>where</p>
						<p><span class="math display">\[
								V(F, G, E, D) = E_{p(x)} \bigg[f(D(E(x)))\bigg] + E_{p(z)} \bigg[f(-D(E(G(F(z))))\bigg]
								\]</span></p>
						<p><span class="math inline">\(E[.]\)</span> denotes expectation, <span class="math inline">\(f\)</span> is the softplus activation function<span class="citation" data-cites="glorot2011deep"><sup><a href="#ref-glorot2011deep">38</a></sup></span> and <span class="math inline">\(\Delta\)</span> is a distance measure. Following the original ALAE formulation, we utilised the <span class="math inline">\(R_1\)</span> regularization term, which drives the input gradients towards zero<span class="citation" data-cites="drucker1992improving"><sup><a href="#ref-drucker1992improving">39</a></sup></span>. This was included only for real data, and is defined as:</p>
						<p><span class="math display">\[
								\dfrac{\lambda}{2}E_{p(x)}\bigg[||\nabla \mathbb{D} \circ E(x)||^2\bigg]
								\]</span></p>
						<p>A <span class="math inline">\(\lambda\)</span> constant denotes the weighting of the term in the loss.</p>
						<figure id="fig2">
								<img src="./assets/network.png">
								<figcaption>
										<strong>Figure 2:</strong> An ALAE network is composed of four networks in contrast to the traditional two for GAN training. The fully-connected <span class="math inline">\(F\)</span> network maps from the known distribution <span class="math inline">\(z\)</span> to a learned distribution <span class="math inline">\(w\)</span>. This feeds the generator network <span class="math inline">\(G\)</span>, to produce a fake image <span class="math inline">\(x\)</span>. The encoder network <span class="math inline">\(E\)</span> maps the image back to <span class="math inline">\(w&#39;\)</span>, which then feeds a fully-connected discriminator network <span class="math inline">\(D\)</span>. An additional autoencoding optimization step minimizes <span class="math inline">\(w\)</span> and <span class="math inline">\(w&#39;\)</span> where <span class="math inline">\(w&#39; = E(G(w)\)</span>. This results in stable adversarial training with the addition of being able to encode and decode real images via <span class="math inline">\(G(E(x))\)</span>.
								</figcaption>
						</figure>
						<p>We developed our own implementation using Tensorflow 2.2<span class="citation" data-cites="abadi2016tensorflow"><sup><a href="#ref-abadi2016tensorflow">40</a></sup></span> and followed the Style-based architecture. We included Weight Modulation/Demodulation as an option in addition to Adaptive Instance Normalization, introduced in StyleGAN2<span class="citation" data-cites="Karras2020"><sup><a href="#ref-Karras2020">28</a></sup></span> architecture. We further adapted the network to train end-to-end using the additive and residual layers. Despite successfully training at full-scale on various datasets (up to 256×256), we found that the progress of training was easier to track by using progressive growing (Section 2.3). Therefore, all results that follow were trained with progressively grown networks using Adaptive Instance Normalization.</p>
						<p>For all models we utilised a latent space of size 512, and kept the number of filters per block consistent with similar work<span class="citation" data-cites="karras2019style Huang2018 Heljakka2019 pidhorskyi2020adversarial"><sup><a href="#ref-pidhorskyi2020adversarial">29</a>,<a href="#ref-karras2019style">41</a>–<a href="#ref-Heljakka2019">43</a></sup></span>. This generally follows 4×4×512, 8×8×512, 16×16×512, 32×32×512, 64×64×256, 128×128×128. The mapper network <span class="math inline">\(F\)</span> consisted of eight fully-connected layers, and the discriminator <span class="math inline">\(D\)</span> contained 3 fully-connected layers.</p>
						<p>The code is available at https://github.com/smthomas-sci/SkinCancerConceptVectors</p>
						<h3 id="training-regime">2.3 Training Regime</h3>
						<p>In a progressively grown manner we trained successive blocks up to 128×128 pixels as follows. Each block was made up of 2-3 convolutional and normalization layers, as well as up or down sampling operations (refer to code for full details). After training the 4×4 block for 800,000 images (~14 epochs), each block was then faded to the next scale over 14 epochs, where the outputs from blocks <span class="math inline">\(B_i\)</span> and <span class="math inline">\(B_{i+1}\)</span> were merged using <span class="math inline">\(\alpha \cdot B_{i+1} + (1-\alpha) \cdot B_{i}\)</span> , with <span class="math inline">\(\alpha\)</span> incremented over the 14 epochs from <span class="math inline">\(0\)</span> up to <span class="math inline">\(1\)</span>. We then fine-tuned the network for a further 14 epochs before scaling again. The number of epochs was chosen empirically based on the convergence time at each block, amounting to double the number of images used in the original ALAE paper. Upon reaching 128×128, the model was fine-tuned for 300 epochs while monitoring FID score<span class="citation" data-cites="Heusel2017"><sup><a href="#ref-Heusel2017">44</a></sup></span>. At each respective scale we used batch sizes of 512, 256, 128, 128, 128, and 64, shared across two NVIDIA SXM2 Tesla 32GB V100 GPUs. We used <span class="math inline">\(\lambda=0.1\)</span> for all scales except for 64×64 and 128×128 where <span class="math inline">\(\lambda=10\)</span>. We found the small value encouraged successful training in the early stages. We used an Adam optimizer with <span class="math inline">\(\beta_1 = 0.0\)</span> and <span class="math inline">\(\beta_2 = 0.99\)</span> and a learning rate <span class="math inline">\(\alpha = 0.0010\)</span>. All layers were updated using the learning rate equalization technique<span class="citation" data-cites="karras2017progressive"><sup><a href="#ref-karras2017progressive">26</a></sup></span>. Style-mixing regularization was performed every 16 batches.</p>
						<p>Training was monitored via visual inspection of generated and reconstructed images, as well as FID. The weights were saved every epoch and an exponential moving average (decay=0.999) of the weights for the <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> networks were kept and used for all visualisations. FID was calculated using 10,000 randomly generated images. The final results were derived from our best model, using the exponential moving average (EMA) weights.</p>
						<h3 id="concept-vectors">2.4 Concept Vectors</h3>
						<p>The use of concept vectors has been shown to be helpful in understanding how machine learning systems make decisions<span class="citation" data-cites="Kim2018 mConceptAttributionExplaining2020"><sup><a href="#ref-Kim2018">45</a>,<a href="#ref-mConceptAttributionExplaining2020">46</a></sup></span>. Although there are varied approaches to defining concept vectors, informed by<span class="citation" data-cites="Kim2018"><sup><a href="#ref-Kim2018">45</a></sup></span> we defined a regression model as:</p>
						<p><span class="math display">\[
								z = \alpha + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n
								\]</span></p>
						<p>where <span class="math inline">\(z\)</span> is the real value (<span class="math inline">\(z \in \{-\infty, \infty\}\)</span>) before the logistic transform for classification, <span class="math inline">\(n\)</span> is the dimensionality of the latent vector <span class="math inline">\(x\)</span>, and <span class="math inline">\(\alpha\)</span> is the intercept. When <span class="math inline">\(z=0\)</span> the corresponding probability is <span class="math inline">\(p=0.5\)</span>, defining the decision boundary. We then define the concept vector as the normalized derivative as follows:</p>
						<p><span class="math display">\[
								\dfrac{\nabla z}{|\nabla z|} = 
								\dfrac{(\beta_1, \beta_2, ..., \beta_n)}{\sqrt{\sum\beta_i^2}} = C
								\]</span></p>
						<p>The intercept is additionally normalized as:</p>
						<p><span class="math display">\[
								\alpha&#39; = \dfrac{\alpha}{|\nabla z|}
								\]</span></p>
						<p>Using the segmentation ground truth for each patch, we created positive and negative concept datasets based on the proportion of the tissue type present. We picked a threshold of 0.2, indicating that at least 20% of the image needed to belong to the positive class. If the concept was not present it was allocated to the negative class. The class balances are shown in <a href="#tableA1">Table A1</a>. Using these datasets we trained six logistic regression models to learn the <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> terms for each tissue type. Although the classes were unbalanced this did not meaningfully effect classification performance (Section 3.3).</p>
						<p>For each patch we used the encoder network to get a latent representation, <span class="math inline">\(w = E(x)\)</span>. We then performed a scalar projection, taking the dot product of the latent representation with the learned concept vector to obtain a score <span class="math inline">\(s\)</span>:</p>
						<p><span class="math display">\[
								s = w \cdot C + a&#39;
								\]</span></p>
						<p>This score indicates how far along the concept vector a particular image falls. This enables us to measure both the presence and absence of a concept, and more interestingly, the degree to which it is present.</p>
				</section>
				<section id="results">
						<h2 id="results-discussion">3. Results &amp; Discussion</h2>
						<h3 id="encoding-decoding-and-random-generation-of-images">3.1 Encoding, Decoding and Random Generation of Images</h3>
						<p>Our style-based ALAE architecture successfully learned to both generate and reconstruct realistic images across a wide variety of tissue types and morphological contexts (<a href="#fig3">Fig. 3</a>). It can be seen that the network has modelled their natural variations across the full-range of orientations. In particular we can see several manifestations of BCC morphology, with natural variances in staining, as well as contexts of surrounding tissue. We also note subtle details such as blue ink <a href="#fig3">(Fig 3b, arrows)</a>, which is an orientation marker for the pathologist. Our best performing model achieved an FID score of 44 (<a href="#figA1">Fig. A1</a>), based on 10,000 randomly generated images. Although this would not always be considered a good FID score, visually these images appear to encode and reconstruct the general features of a diverse and complex image domain.</p>
						<figure id="fig3">
								<img src="./assets/reconstructions_and_samples.png">
								<figcaption>
										<strong>Figure 3</strong>: Generated images from the ALAE network. <strong>a)</strong> Representative reconstructions for real images, <span class="math inline">\(G(E(x))\)</span>, show that the both global context of an image as well as relevant tissue type and features can be successfully generated. <strong>b)</strong> Random samples showcase a diverse set of images, <span class="math inline">\(G(F(z))\)</span> where <span class="math inline">\(z_i \sim N(0,1), 1 \le i \le 512\)</span>. The arrows in row 2, column 6 point to blue ink, an orientation marker for pathologists.
								</figcaption>
						</figure>
						<p>It is important to note that the style-based ALAE encoder <span class="math inline">\(E\)</span>, is not optimized to match exact spatial features. Instead, it is trained to extract a statistical summary which is used to generate a clinically similar image. To this end, the placement of particular features is not penalized as long as they represent the image semantics. For example, the reconstructions in <a href="#fig3">Fig. 3</a> show that the proportions of each tissue type as well as their arrangement is largely correct. We note that using a statistical summary for reconstruction is in contrast to a perceptual reconstruction which is most commonly performed<span class="citation" data-cites="abdal2019image2stylegan gabbay2019style"><sup><a href="#ref-abdal2019image2stylegan">47</a>,<a href="#ref-gabbay2019style">48</a></sup></span>. However, given the enormous variability found in cellular morphology, it is arguable that the statistical summary is more effective in this case. This is similar to reconstructions of breast cancer images, learned via minimizing latent space distance, which produce an approximate but semantically similar match to the original image<span class="citation" data-cites="Quiros2020"><sup><a href="#ref-Quiros2020">36</a></sup></span>. We conclude then that semantic feature summaries are useful, and indeed, all that is needed to effectively match input and output image pairs.</p>
						<h3 id="training-stability">3.2 Training Stability</h3>
						<p>As the model was progressively grown, we experimented with images beyond 128×128 pixels. Interestingly, we found that training became unstable when transitioning to 256×256. The reason for this is not immediately apparent given that the 128×128 sized images can be modeled effectively, and the training proceeded stably and reliably without encountering those problems. We considered firstly if it was an issue of fading too quickly, however, even at half the rate (fading over 1,600,000 images) we observed mode collapse and instability. Indeed, even our fade rate was half the original ALAE paper at 400,000 images. Our second approach was to double the number of parameters in the 256×256 block. This again failed to overcome the problem. Interestingly, we found we could train successfully at this resolution on both human faces and dermatoscopic datasets. In contrast, we also failed to converge at this scale for the breast cancer images from PathologyGAN. This lead us to consider whether this was an issue inherent to the way the problem was framed. We will elaborate on this idea in Section 3.9.</p>
						<p>For insight into model stability up to 128×128, we show in appendix <a href="#figA2">Fig. A2</a> the training curves observed from our best model, which was typical of all our experiments. Over the course of training, the adversarial loss components remained fairly constant, with the gradient penalty converging once reaching 128×128 pixels. A noticeable feature is the increasing reconstruction loss. In part, we explain this as being due to the improving quality of the model (<a href="#figA1">Fig. A1</a>). As realism improves, the feature space increases in complexity and accordingly, so does the opportunity for error. However, we also attribute this to the instability of training in the final 100 epochs (<a href="#figA1">Fig. A1</a>). We observe that at this stage, randomly sampled images are more stable across epochs in their <span class="math inline">\(w \rightarrow x\)</span> mapping, whereas the encoder-generator mapping of <span class="math inline">\(x \rightarrow w \rightarrow x&#39;\)</span>, and the complimentary latent autoencoding mapping, <span class="math inline">\(w \rightarrow x \rightarrow w&#39;\)</span> are less so (data not shown). To improve this the encoder network could increase the weight of the reconstruction loss, or alternatively, perform fine-tuning once satisfied with the generator. However, <a href="#fig3">Fig. 3</a> demonstrates that a suitable reconstruction quality can be readily achieved.</p>
						<h3 id="image-domain-coverage">3.3 Image Domain Coverage</h3>
						<p>We further inspected the quality of the model by looking at 2D projections of the latent vectors of random samples. In <a href="#fig4">Fig. 4</a> below, we get an overview of how diverse the learned representation is, as well as an indication of the arrangement of the space. To do this we used 1,600 randomly sampled latent vectors and generated images. We used the UMAP algorithm<span class="citation" data-cites="McInnes2018"><sup><a href="#ref-McInnes2018">49</a></sup></span> to perform dimensionality reduction, alongside the Jonker-Volgenant algorithm<span class="citation" data-cites="jonker1987shortest markovtsevLapjvGitHubRepository2017"><sup><a href="#ref-jonker1987shortest">50</a>,<a href="#ref-markovtsevLapjvGitHubRepository2017">51</a></sup></span> to perform linear assignment. Across the 1,600 images, we recognize morphological and contextual similarities between image neighborhoods, all of which transition naturally within a continuous space. For example, BCC groups are based on staining color (a range of light to dark purples) and tissue proportion, as well as proximity to either the epidermis or papillary dermis. Importantly, there is no evidence of mode collapse, or even repetition of features.</p>
						<p>An obvious pattern in <a href="#fig4">Fig. 4</a> is seen in the space being arranged according to the angle of the tissue with the background. This is also recognizable in the case of the location of BCC in the surrounding dermal tissue. This is an important observation, telling us that images which are essentially rotations of each other are assigned to different locations within the space (see Section 3.8-9 for further discussion). To improve our ability to inspect the model, we return to this mode of inspection in 2D space in Section 3.5 by utilizing concept vectors. This greatly simplified the process as well as provides further insight into the quality of the model.</p>
						<figure id="fig4">
								<img src="./assets/samples_2D_grid.jpg">
								<figcaption>
										<strong>Figure 4</strong>: 1,600 random samples arranged via UMAP dimensionality reduction and linearly assigned to a square space using the Jonker-Volgenant algorithm. The x and y axes constitute dimensions 1 and 2 of the UMAP output. This showcases the diversity of images that can be generated as well as gives some insight into how the latent space is arranged.
								</figcaption>
						</figure>
						<h3 id="feature-discrimination-with-concept-vectors">3.4 Feature Discrimination with Concept Vectors</h3>
						<p>With the encoder network <span class="math inline">\(E\)</span>, we generated latent vectors for the training data, and trained logistic regression models for each of the six tissue types / concepts of interest. In <a href="#table1">Table 1</a> we show the consistently high classification accuracies across the training, validation and test sets for all six concept classes. With the exception of INF at 86.1%, all other classes had a test accuracy between 93% and 98%. High precision and recall across all classes also indicates that the class imbalance in the training data did not affect test performance. This supports the claim that features useful for image generation are similarly useful for image classification. This is not to say that the discriminative task could not have been achieved otherwise, but only that the learned features correspond closely to realistic domain features.</p>
						<figure id="table1">
								<figcaption>
										<strong>Table 1</strong>: Binary classification scores for each class using logistic regression on concepts. Six models were trained to predict concept versus non-concept classes. This shows that individual tissues classes are highly separable. High AUC, Precision and Recall scores indicate that the class imbalances in the training data did not affect performance.
								</figcaption>
								<table>
										<colgroup>
												<col style="width: 7%" />
												<col style="width: 16%" />
												<col style="width: 19%" />
												<col style="width: 14%" />
												<col style="width: 10%" />
												<col style="width: 17%" />
												<col style="width: 14%" />
										</colgroup>
										<thead>
												<tr class="header">
														<th>Class</th>
														<th>Training Acc.</th>
														<th>Validation Acc.</th>
														<th>Test Acc.</th>
														<th>Test AUC</th>
														<th>Test Precision</th>
														<th>Test Recall</th>
												</tr>
										</thead>
										<tbody>
												<tr class="odd">
														<td>BCC</td>
														<td>0.947</td>
														<td>0.938</td>
														<td>0.944</td>
														<td>0.988</td>
														<td>0.918</td>
														<td>0.961</td>
												</tr>
												<tr class="even">
														<td>EPI</td>
														<td>0.945</td>
														<td>0.936</td>
														<td>0.949</td>
														<td>0.987</td>
														<td>0.941</td>
														<td>0.922</td>
												</tr>
												<tr class="odd">
														<td>KER</td>
														<td>0.977</td>
														<td>0.979</td>
														<td>0.971</td>
														<td>0.988</td>
														<td>0.950</td>
														<td>0.938</td>
												</tr>
												<tr class="even">
														<td>PAP</td>
														<td>0.923</td>
														<td>0.904</td>
														<td>0.934</td>
														<td>0.976</td>
														<td>0.892</td>
														<td>0.801</td>
												</tr>
												<tr class="odd">
														<td>INF</td>
														<td>0.843</td>
														<td>0.843</td>
														<td>0.861</td>
														<td>0.926</td>
														<td>0.769</td>
														<td>0.784</td>
												</tr>
												<tr class="even">
														<td>BKG</td>
														<td>0.987</td>
														<td>0.987</td>
														<td>0.985</td>
														<td>0.999</td>
														<td>0.980</td>
														<td>0.973</td>
												</tr>
										</tbody>
								</table>
						</figure>
						<p>Using the coefficients and intercept from the classifier, we measured the scalar projection of all positive and negative concept images onto the six concept vectors. The distributions of the scores are shown in <a href="#fig5">Fig. 5a</a>, demonstrating how the concept vector is able to effectively separate the image groups. We note that the overlap in the INF distribution corresponds to the lower classification performance seen in <a href="#table1">Table 1</a>. This could in part be due to the overlap in features between inflammation, papillary dermis and BCC. For example, INF and BCC often co-occur, yet BCC can also be seen with PAP, with no INF present. The biological context makes it likely that the INF concept vector is associated with BCC (see below and Section 3.3).</p>
						<p>We present in <a href="#fig5">Fig. 5b</a> conceptual summaries of patches containing various combinations of tissue types. The scalar projections of the latent vector onto the six concept vectors provide good characterisations of the image. Scores below 0 indicate a probability less than 50% for the presence of the concept in at least 20% of the image. The positive scores correspond to the presence of the concept in at least 20% of the image. We measured whether the score correlates with the proportion of tissue (<a href="#figA3">Fig. A3</a>), and found strong correlations for BCC (0.88), EPI (0.79), KER(0.86), BKG (0.91) and PAP (0.72). We observed that INF(0.59) scored lower and also had a positive correlation with BCC(0.33), affirming our biological intuition.</p>
						<figure id="fig5">
								<img src="./assets/Distributions_Concept_Patch_Measures.png">
								<figcaption>
										<strong>Figure 5</strong>: Using scalar projections to characterize tissue in image patches. <strong>a)</strong> The distributions of scores for positive and negative concept images for each class. The scores are normalized between -1 and 1, where &gt;0 is positive for the concept. <strong>b)</strong> Individual images with scores for all six concepts provides a way to give a conceptual summary of the image. As shown, multiple concepts can be present in the same image. Scores less than 0 indicate a less than 50% chance of the concept being present in at least 20% of the image.
								</figcaption>
						</figure>
						<h3 id="latent-space-structure">3.5 Latent Space Structure</h3>
						<p>We utilized the ability to score images in terms of concepts to return to the problem of understanding the latent space. In <a href="#fig6">Fig. 6</a>, we show UMAP dimensionality reductions for 10,000 latent vectors from the training images. We then labeled each point as a color gradient mapped from the score for each concept. Interestingly, the figure shows that the concepts are arranged in the space in a similar way to their biological context. It can be seen that BKG, KER and EPI are located next to or co-locating with each other. Similarly, EPI and PAP are highly correlated and PAP transitions towards INF and BCC. This corresponds to the image domain where these tissue types adjoin as seen in <a href="#fig1">Fig. 1</a>. Therefore, these maps reveal a conceptual landscape which aligns with our biological expectations.</p>
						<figure id="fig6">
								<img src="./assets/UMAP_concept_projections.png">
								<figcaption>
										<strong>Figure 6</strong>: UMAP projections of 10,000 latent vectors from the training data. This indicates the space is organized in a biological meaningful way, where adjoining tissue types are closely located in the space. It also indicates that spatial orientation of tissue within the patch is an important factor.
								</figcaption>
						</figure>
						<h3 id="whole-image-characterization">3.6 Whole-Image Characterization</h3>
						<p>We next applied concept summaries to the whole-image context. Whole-images present the full tissue specimen, including regions of tissues that the network was not trained on. In a sliding window fashion (step size of 32×32) we generated a latent vector for a 128×128 patch and then saved the score for all six concept vectors to produce a multi-dimensional heat map. We then set a threshold for positive scores revealing which parts of the specimen contain each concept (<a href="#fig7">Fig. 7</a>). This was performed on whole images of varying sizes, including punch, shave and excision biopsies (<a href="#fig7">Fig. 7a &amp; 7b</a>). Qualitatively we see that the relevant tissue types have been outlined or segmented by the concept scores. High EPI scores occur along the epidermis, as does KER over keratin regions. INF is seen to surround the BCC regions, and BKG has identified the perimeter of the specimen. This is consistent with our observations in <a href="#fig6">Fig. 6</a>. As an interesting side note, the heat maps also indicate that the inclusion of tissue regions that were not in the training data (below the superficial layers) did not adversely affect the visualisations, with mostly negative or mildly positive scores for all concepts.</p>
						<figure id="fig7">
								<img src="./assets/whole_image_concepts.png">
								<figcaption>
										<strong>Figure 7</strong>: Conceptual summaries of whole-tissue images for shave, punch and excision biopsy specimens. The grey bar designates 1mm distance. All maps indicate positive concept scores &gt; 0 <strong>a)</strong> High-level overview of concept summaries for a variety of specimen types showing that the concept summaries highlight quite well the relevant tissue regions. <strong>b)</strong> A finer resolution example of an excision specimen where the concept scores provide accurate summaries of the tissue regions. Best viewed in color.
								</figcaption>
						</figure>
						<p>Scoring images at this scale provides a high-level summary which utilises human-meaningful descriptors. This follows a similar approach to previous work<span class="citation" data-cites="thomas2020interpretable"><sup><a href="#ref-thomas2020interpretable">37</a></sup></span>, which argues that naturally interpretable systems would characterise the problem domain in the same way a pathologist would. However, semantic segmentations provide only coarse descriptions of the tissue, whereas concept vectors are much more flexible and can be defined abstractly. For example, the concept of “mild inflammation” doesn’t directly correspond to pixels, but rather a region, within which another concept may be equally relevant. There is thus opportunity to extend this idea to include as many concepts as necessary to provide a complete pathological description of the tissue, depending on what scale the network is trained on. This is similar to how a pathologist works at various magnifications, utilizing different terminology and disease processes suitable for each context.</p>
						<h3 id="minimal-concept-vector-definitions">3.7 Minimal Concept Vector Definitions</h3>
						<p>The advantage of generative modeling is that it can be performed in an unsupervised manner. However, in order to interpret and utilise the model we require labels. In this work we were able to leverage previously labelled data. However, in most practical settings this is not possible. This therefore prompted us to ask, how many labels do we require in order to define quality concept vectors?</p>
						<p>For this we come to a surprising conclusion; not nearly as many as traditionally required for image classification tasks. <a href="#fig8">Fig. 8</a> shows the overall accuracy after training classifiers on a range of training dataset sizes. The performance metrics shown in <a href="#table2">Table 2</a> are from the models trained on the full dataset, approximately 10<sup>4</sup> images. In <a href="#fig8">Fig. 8</a> it can be seen that with 10<sup>3</sup> images, performance is maintained, and with 10<sup>2</sup> images, performance only drops by approximately 5%. For more obvious classes, such as BKG, 10 images each of positive and negative examples keeps performance at 90%. In practice, this may translate to only needing 100-500 labels of each class which is easily achievable.</p>
						<p>We propose that the small number of labels is possible because the network has already learned the image domain by training on a large body of unlabeled data. The subsequent use of labels is just a means for people to understand and interact with the knowledge instantiated in the network. Because only a small number of labels is needed, yet classification is high, we interpret this as further evidence for the usefulness of features for both generation and classification tasks.</p>
						<figure id="fig8">
								<img src="./assets/minimal_concept_vectors.png">
								<figcaption>
										<strong>Figure 8</strong>: Concept vector accuracies for decreasing numbers of training data. Performance decreases non-linearly with training set size, indicating that substantially smaller numbers of labels can be utilized without compromising accuracy. This means that performing classification after training an unsupervised generative model may be much less laborious. Best viewed in color.
								</figcaption>
						</figure>
						<h3 id="latent-space-complexity">3.8 Latent Space Complexity</h3>
						<p>In Section 1.8, the use of interpolating points through the latent space was introduced as an interpretability tool. Previous works show that interpolations between images of faces tend to follow natural journeys, where the in-between faces have similarities to both pairs and link semantically<span class="citation" data-cites="Karras2019 pidhorskyi2020adversarial"><sup><a href="#ref-Karras2019">27</a>,<a href="#ref-pidhorskyi2020adversarial">29</a></sup></span>. However, in the case of histology images, the meaning of intermediate images remains elusive. In <a href="#fig9">Fig. 9</a> we show some example interpolations where there is no clear reason why the intermediate images are what they are. An obvious example is in the last row, where the intermediate image between healthy epidermis and papillary dermis is inflammation with some BCC. This does not make sense in the context of histology. In fact, the endpoint images are almost rotations by 90 degrees. A realistic journey might thus rotate the image, not firstly traverse regions of unrelated tissue types. Indeed, this appears to be a problem for breast cancer images as well<span class="citation" data-cites="Quiros2020"><sup><a href="#ref-Quiros2020">36</a></sup></span>.</p>
						<figure id="fig9">
								<img src="./assets/interpolations.png">
								<figcaption>
										<strong>Figure 9</strong>: Linear interpolations between images pairs in the latent space results in unrealistic journeys, where intermediate images are realistic but have no obvious connection to the endpoints.<span class="math inline">\(\alpha\)</span> is the blend between latent vectors, where the intermediate image is generated from <span class="math inline">\(w_I = \alpha w_1 + (1-\alpha)w_2\)</span>.
								</figcaption>
						</figure>
						<p>The problem is further recognized when performing vector arithmetic using concept vectors. We can move any image linearly in the direction along the concept vector, such as seen with attribute editing with faces<span class="citation" data-cites="Dogan2020"><sup><a href="#ref-Dogan2020">52</a></sup></span>. The goal here is to increase the presence of a certain concept within the image. However, in our work we find that what often results are unrealistic images that score high for the concept, much like with adversarial attacks<span class="citation" data-cites="Engstrom2019a"><sup><a href="#ref-Engstrom2019a">53</a></sup></span>. Although images can be usefully summarized when real, we can’t just move any image in the direction of a pure concept due to the complexity of the latent space.</p>
						<h3 id="image-domain-complexity">3.9 Image Domain Complexity</h3>
						<p>We have seen that while the latent space has a biologically meaningful structure <a href="#fig6">(Fig. 6)</a> which provides a powerful tool for discriminating and quantifying clinically important features (<a href="#fig5">(Fig. 5)</a> and <a href="#fig7">7</a>), it falls short of the linear semantic structure seen in some other work<span class="citation" data-cites="pidhorskyi2020adversarial"><sup><a href="#ref-pidhorskyi2020adversarial">29</a></sup></span>, as demonstrated via interpolations <a href="#fig9">(Fig. 9)</a> and extrapolation along concept vectors. We propose that this limitation is a result of the nature of the image domain. The major difference between modelling histological images compared to common deep learning image datasets such as faces, bedrooms, or cars, is that the latter contain consistent global landmarks within the imaging. For example, faces will reliably have a head in the center, with two eyes, a nose and mouth. The model learns all the variations around those major features. In contrast, histological imaging has no consistent global landmarks, but rather has varied landmarks and has similarities to the task of texture synthesis. In the arrangement of the latent space in <a href="#fig4">Fig. 4</a> we see that the encoding of spatial information and tissue features is entangled. Images that are essentially rotations of each other are placed in distinct areas of the latent space, rather than being rationally distinguished by a single “rotation” dimension. Lacking global landmarks to attach concepts to, we speculate that the same features are learned redundantly in different regions. This is a plausible explanation for the non-linearity seen in the latent space. It also indicates a potential limitation on the learning efficiency of the model - a pathologist does not learn a set of features for every possible orientation of the specimen.</p>
						<p>We further speculate that the training instability seen at the 256×256 scale is linked to image domain complexity and lack of global landmarks, and the demands that this places on the learning capacity of the model. Interestingly, when we tested our model on other datasets we saw a similar failure for breast cancer imaging but not for faces or dermatoscopic data, where global structure is present. We do note that other work has shown that histological images can be generated up to the size of 224×224<span class="citation" data-cites="Quiros2019"><sup><a href="#ref-Quiros2019">35</a></sup></span> and 1024×1024<span class="citation" data-cites="Levine2020"><sup><a href="#ref-Levine2020">33</a></sup></span> pixels using GANs. It may be that the addition of the autoencoding loss interferes with learning images without consistent global structure; the specific adversarial loss function used may also play a role<span class="citation" data-cites="Quiros2019 jolicoeur2018relativistic"><sup><a href="#ref-Quiros2019">35</a>,<a href="#ref-jolicoeur2018relativistic">54</a></sup></span>. Training at higher resolution would be desirable, allowing smaller scale features to be represented and potentially incorporated into tissue characterization and discrimination tasks. Thus resolving the issues around image domain complexity would likely have benefits in both latent space interpretability and in more powerful modelling.</p>
				</section>
				<section id="conclusion">
						<h2 id="conclusion">4. Conclusion</h2>
						<p>Here we presented a highly-interpretable way of using machine learning to perform tissue characterization of basal cell carcinoma via generative modelling. We trained a system to encode and generate realistic images of a variety of tissues types in the superficial layers of skin. The quality of the features the model has learned is explicit from the quality of the generated images, making the model naturally interpretable. In contrast to previous methods, our network training process was stable and reliably produced high quality images. Our results indicate that the network learned relevant, and consequently, highly discriminative features which enable accurate classification. This further served the purpose of performing tissue characterization via concept vectors. This allowed us to summarise image patches using high-level, human-meaningful descriptors which we can interpret. This was extended to whole-tissue specimens which results in a “conceptual summary” of the entire slide. We also found that the number of images needed to define accurate concept vectors is between 100 and 500 patches, which significantly reduces the time cost of developing high-performing classification models. This promising methodology is applicable to many histological datasets where characterizing and understanding morphological features could be improved with the use of concept vectors.</p>
				</section>
				<hr>
				<h2 id="declaration-of-competing-interest">Declaration of Competing Interest</h2>
				<p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
				<h2 id="acknowledgements">Acknowledgements</h2>
				<p>We wish to acknowledge The University of Queensland’s Research Computing Centre (RCC) for its support in this research. JL is supported by Australian Research Council Discovery Grant DP180101910. MyLab Pathology provided access to their archived histological collection. Financial support for SMT was provided by The Australian Government Research Training Program (RTP) and The Laurel Joy George Perpetual Scholarship.</p>
				<section id="appendix">
						<h2 id="appendix">Appendix</h2>
						<figure id="tableA1">
								<figcaption>
										<strong>Table A1</strong>: Data composition and class balances. Patches were allocated to positive and negative classes based on a threshold of 0.2 for proportion of the tissue. If a tissue type was absent, it was assigned to the negative class. The splits follow a 70:15:15% split based on the original whole slide images. The class balances were consistent across the training and test set, with the validation varying for some classes. The ratios for each class and data split are positive : negative class numbers.
								</figcaption>
								<table>
										<thead>
												<tr class="header">
														<th>Class</th>
														<th>Train +</th>
														<th>Train -</th>
														<th>Ratio</th>
														<th>Val +</th>
														<th>Val -</th>
														<th>Ratio</th>
														<th>Test +</th>
														<th>Test -</th>
														<th>Ratio</th>
												</tr>
										</thead>
										<tbody>
												<tr class="odd">
														<td>BCC</td>
														<td>21,745</td>
														<td>26,124</td>
														<td>0.81</td>
														<td>5,218</td>
														<td>9,572</td>
														<td>0.55</td>
														<td>5,392</td>
														<td>6,630</td>
														<td>0.81</td>
												</tr>
												<tr class="even">
														<td>KER</td>
														<td>12,691</td>
														<td>25,788</td>
														<td>0.34</td>
														<td>3,264</td>
														<td>8,480</td>
														<td>0.38</td>
														<td>2,656</td>
														<td>7,653</td>
														<td>0.35</td>
												</tr>
												<tr class="odd">
														<td>PAP</td>
														<td>10,698</td>
														<td>35,232</td>
														<td>0.28</td>
														<td>4,523</td>
														<td>9,859</td>
														<td>0.46</td>
														<td>2,745</td>
														<td>9,615</td>
														<td>0.29</td>
												</tr>
												<tr class="even">
														<td>BKG</td>
														<td>15,288</td>
														<td>27,340</td>
														<td>0.45</td>
														<td>4,736</td>
														<td>9,355</td>
														<td>0.51</td>
														<td>3,718</td>
														<td>8,174</td>
														<td>0.45</td>
												</tr>
												<tr class="odd">
														<td>INF</td>
														<td>12,905</td>
														<td>31,786</td>
														<td>0.44</td>
														<td>3,784</td>
														<td>10,700</td>
														<td>0.35</td>
														<td>3,691</td>
														<td>8,307</td>
														<td>0.44</td>
												</tr>
												<tr class="even">
														<td>EPI</td>
														<td>19,676</td>
														<td>22,450</td>
														<td>0.61</td>
														<td>5,271</td>
														<td>6,159</td>
														<td>0.86</td>
														<td>3,805</td>
														<td>6,241</td>
														<td>0.61</td>
												</tr>
										</tbody>
								</table>
						</figure>
						<figure id="figA1">
								<img src="./assets/FID_Plot.png">
								<figcaption>
										<strong>Figure A1</strong>: FID scores for the best model across the final 300 epochs of training. The EMA is presented only to aid the eye. The best FID score was 44, marked with the dotted line.
								</figcaption>
						</figure>
						<figure id="figA2">
								<img src="./assets/training_curves.png">
								<figcaption>
										<strong>Figure A2</strong>: Training curves for our best ALAE network. The adversarial loss components remain stable across all epochs. The gradient penalty converges once reaching full-scale (128×128). The reconstruction error steadily increases over the course of training, perhaps due to the improving quality of the generated images.
								</figcaption>
						</figure>
						<figure id="figA3">
								<img src="./assets/correlation_matrix_dark.png">
								<figcaption>
										<strong>Figure A3</strong>: Spearman Rank Correlation matrix for concept scores against the respective tissue proportion for an image. Scores tend to have strong positive correlations with their tissue proportion. Also, positive correlations are seen for associated classes, such as PAP, EPI, KER and BKG, as well as BCC and INF.
								</figcaption>
						</figure>
				</section>
				<!-- END DOCUMENT -->
				<!-- MODAL IMAGES -->
				<div id="myModal" class="modal">
						<p><!-- the close button --> <span class="close">×</span></p>
						<p><!-- Modal content (the image) --> <img class="modal-content" id="modal-img"></p>
						<!-- Model Caption (Image text) -->
						<div id="imgCaption">

						</div>
				</div>
				<script>

						let modal = document.getElementById("myModal");
let captionText = document.getElementById("imgCaption");
let modalImg = document.getElementById("modal-img");
let img;

// Get the image and insert it inside the modal
let imgs = document.getElementsByTagName("img");
for (let i=0;i<imgs.length;i++){
		// get image
		let img = imgs[i]
		img.onclick = function(){
				modalImg.src = this.src;    
				modal.style.display = "block";
				captionText.innerHTML = this.parentElement.getElementsByTagName("figcaption")[0].innerHTML

		}
}

// Get the <span> element that closes the modal;
let span = document.getElementsByClassName("close")[0];

// When the user clicks on X, close the modal
span.onclick = function(){
		modal.style.display = "none";
}


				</script>
<!-- END MODAL -->
<script>

		let highlight = function(elem){
				let ref = "" + elem;
				console.log(ref);
				let div = document.getElementById(ref);
				//div.style.borderBottom = "3px orangered solid";
				div.style.backgroundColor = "lightgrey";

				setTimeout(function(){
						//div.style.borderBottom = "none";
						div.style.backgroundColor = "white";
				}, 3000);

		}

window.onload = function(){

		// Set document title   
		document.title = "Digital Preprint";

		// Add references and toc
		let div = document.getElementById("refs");
		let h1 = document.createElement("h1");
		h1.innerText = "References";
		div.prepend(h1);

		div = document.getElementById("TOC");
		h1 = document.createElement("h1");
		h1.innerText = "Table of Contents";
		div.prepend(h1);    


		// Add citation highlight
		let citations = document.getElementsByClassName("citation");
		for (let i=0;i < citations.length; i++){
				let a = citations[i].getElementsByTagName("a");
				for (let j=0;j<a.length;j++){
						a[j].setAttribute('onclick',
								'highlight("' + a[j].href.split("#")[1] + '")');
				}
		}

		// Add model images


}


// LOOK UP - https://www.w3schools.com/howto/howto_css_modal_images.asp


</script>
						<div id="refs" class="references">
								<div id="ref-bulten2020artificial">
										<p>1. Bulten, W. <em>et al.</em> Artificial intelligence assistance significantly improves Gleason grading of prostate biopsies by pathologists. <em>Modern Pathology</em> 1–12 (2020).</p>
								</div>
								<div id="ref-Campanella2019">
										<p>2. Campanella, G. <em>et al.</em> Clinical-grade computational pathology using weakly supervised deep learning on whole slide images. <em>Nature Medicine</em> <strong>25</strong>, (2019).</p>
								</div>
								<div id="ref-sirinukunwattana2017gland">
										<p>3. Sirinukunwattana, K. <em>et al.</em> Gland segmentation in colon histology images: The glas challenge contest. <em>Medical image analysis</em> <strong>35</strong>, 489–502 (2017).</p>
								</div>
								<div id="ref-wu2018automatic">
										<p>4. Wu, M., Yan, C., Liu, H. &amp; Liu, Q. Automatic classification of ovarian cancer types from cytological images using deep convolutional neural networks. <em>Bioscience reports</em> <strong>38</strong>, (2018).</p>
								</div>
								<div id="ref-bera2019artificial">
										<p>5. Bera, K., Schalper, K. A., Rimm, D. L., Velcheti, V. &amp; Madabhushi, A. Artificial intelligence in digital pathology—new tools for diagnosis and precision oncology. <em>Nature reviews Clinical oncology</em> <strong>16</strong>, 703–715 (2019).</p>
								</div>
								<div id="ref-zhang2019pathologist">
										<p>6. Zhang, Z. <em>et al.</em> Pathologist-level interpretable whole-slide cancer diagnosis with deep learning. <em>Nature Machine Intelligence</em> <strong>1</strong>, 236–245 (2019).</p>
								</div>
								<div id="ref-wei2019pathologist">
										<p>7. Wei, J. W. <em>et al.</em> Pathologist-level classification of histologic patterns on resected lung adenocarcinoma slides with deep neural networks. <em>Scientific reports</em> <strong>9</strong>, 1–8 (2019).</p>
								</div>
								<div id="ref-Hekler2019">
										<p>8. Hekler, A. <em>et al.</em> Pathologist-level classification of histopathological melanoma images with deep neural networks. <em>European Journal of Cancer</em> <strong>115</strong>, 79–83 (2019).</p>
								</div>
								<div id="ref-Staples2006-kp">
										<p>9. Staples, M. P. <em>et al.</em> Non-melanoma skin cancer in Australia: The 2002 national survey and trends since 1985. <em>Med. J. Aust.</em> <strong>184</strong>, 6–10 (2006).</p>
								</div>
								<div id="ref-badgeley2019deep">
										<p>10. Badgeley, M. A. <em>et al.</em> Deep learning predicts hip fracture using confounding patient and healthcare variables. <em>NPJ digital medicine</em> <strong>2</strong>, 1–10 (2019).</p>
								</div>
								<div id="ref-Selvaraju2016">
										<p>11. Selvaraju, R. R. <em>et al.</em> Grad-CAM: Why did you say that? <a href="http://arxiv.org/abs/1611.07450" class="uri">http://arxiv.org/abs/1611.07450</a> (2016).</p>
								</div>
								<div id="ref-Dabkowski2017">
										<p>12. Dabkowski, P. &amp; Gal, Y. Real time image saliency for black box classifiers. in <em>Advances in neural information processing systems</em> 6967–6976 (2017).</p>
								</div>
								<div id="ref-Smilkov2017">
										<p>13. Smilkov, D., Thorat, N., Kim, B., Viégas, F. &amp; Wattenberg, M. Smoothgrad: Removing noise by adding noise. <a href="http://arxiv.org/abs/1706.03825" class="uri">http://arxiv.org/abs/1706.03825</a> (2017).</p>
								</div>
								<div id="ref-Selvaraju2017">
										<p>14. Selvaraju, R. R. <em>et al.</em> Grad-cam: Visual explanations from deep networks via gradient-based localization. in <em>Proceedings of the IEEE international conference on computer vision</em> 618–626 (2017).</p>
								</div>
								<div id="ref-Adebayo2018">
										<p>15. Adebayo, J. <em>et al.</em> Sanity checks for saliency maps. in <em>Advances in neural information processing systems</em> 9505–9515 (2018).</p>
								</div>
								<div id="ref-Olah2017">
										<p>16. Olah, C., Mordvintsev, A. &amp; Schubert, L. Feature visualization. <em>Distill</em> <strong>2</strong>, e7 (2017).</p>
								</div>
								<div id="ref-Olah2018">
										<p>17. Olah, C. <em>et al.</em> The building blocks of interpretability. <em>Distill</em> <strong>3</strong>, e10 (2018).</p>
								</div>
								<div id="ref-Carter2019">
										<p>18. Carter, S., Armstrong, Z., Schubert, L., Johnson, I. &amp; Olah, C. Activation atlas. <em>Distill</em> <strong>4</strong>, e15 (2019).</p>
								</div>
								<div id="ref-engstrom2019adversarial">
										<p>19. Engstrom, L. <em>et al.</em> Adversarial robustness as a prior for learned representations. <a href="http://arxiv.org/abs/1906.00945" class="uri">http://arxiv.org/abs/1906.00945</a> (2019).</p>
								</div>
								<div id="ref-Mahendran2015">
										<p>20. Mahendran, A. &amp; Vedaldi, A. Understanding deep image representations by inverting them. in <em>Proceedings of the IEEE computer society conference on computer vision and pattern recognition</em> vols 07-12-June-2015 5188–5196 (IEEE Computer Society, 2015).</p>
								</div>
								<div id="ref-nguyen2016synthesizing">
										<p>21. Nguyen, A., Dosovitskiy, A., Yosinski, J., Brox, T. &amp; Clune, J. Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. <a href="http://arxiv.org/abs/1605.09304" class="uri">http://arxiv.org/abs/1605.09304</a> (2016).</p>
								</div>
								<div id="ref-dosovitskiy2016inverting">
										<p>22. Dosovitskiy, A. &amp; Brox, T. Inverting visual representations with convolutional networks. in <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> 4829–4837 (2016).</p>
								</div>
								<div id="ref-nguyen2017plug">
										<p>23. Nguyen, A., Clune, J., Bengio, Y., Dosovitskiy, A. &amp; Yosinski, J. Plug &amp; play generative networks: Conditional iterative generation of images in latent space. in <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> 4467–4477 (2017).</p>
								</div>
								<div id="ref-rudinStopExplainingBlack2019">
										<p>24. Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. <em>Nature Machine Intelligence</em> <strong>1</strong>, 206–215 (2019).</p>
								</div>
								<div id="ref-Brock2018">
										<p>25. Brock, A., Donahue, J. &amp; Simonyan, K. Large scale gan training for high fidelity natural image synthesis. <a href="http://arxiv.org/abs/1809.11096" class="uri">http://arxiv.org/abs/1809.11096</a> (2018).</p>
								</div>
								<div id="ref-karras2017progressive">
										<p>26. Karras, T., Aila, T., Laine, S. &amp; Lehtinen, J. Progressive growing of gans for improved quality, stability, and variation. <a href="http://arxiv.org/abs/1710.10196" class="uri">http://arxiv.org/abs/1710.10196</a> (2017).</p>
								</div>
								<div id="ref-Karras2019">
										<p>27. Karras, T., Laine, S. &amp; Aila, T. A style-based generator architecture for generative adversarial networks. in <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> 4401–4410 (2019).</p>
								</div>
								<div id="ref-Karras2020">
										<p>28. Karras, T. <em>et al.</em> Analyzing and improving the image quality of stylegan. in <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em> 8110–8119 (2020).</p>
								</div>
								<div id="ref-pidhorskyi2020adversarial">
										<p>29. Pidhorskyi, S., Adjeroh, D. A. &amp; Doretto, G. Adversarial latent autoencoders. in <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em> 14104–14113 (2020).</p>
								</div>
								<div id="ref-cho2017neural">
										<p>30. Cho, H., Lim, S., Choi, G. &amp; Min, H. Neural stain-style transfer learning using gan for histopathological images. <a href="http://arxiv.org/abs/1710.08543" class="uri">http://arxiv.org/abs/1710.08543</a> (2017).</p>
								</div>
								<div id="ref-tarek2018staingan">
										<p>31. Tarek Shaban, M., Baur, C., Navab, N. &amp; Albarqouni, S. StainGAN: Stain style transfer for digital histological images. <em>arXiv e-prints</em> arXiv–1804 (2018).</p>
								</div>
								<div id="ref-kovalev2019examining">
										<p>32. Kovalev, V. &amp; Kazlouski, S. Examining the capability of GANs to replace real biomedical images in classification models training. in <em>International conference on pattern recognition and information processing</em> 98–107 (Springer, 2019).</p>
								</div>
								<div id="ref-Levine2020">
										<p>33. Levine, A. B. <em>et al.</em> Synthesis of diagnostic quality cancer pathology images by generative adversarial networks. <em>The Journal of Pathology</em> path.5509 (2020) doi:<a href="https://doi.org/10.1002/path.5509">10.1002/path.5509</a>.</p>
								</div>
								<div id="ref-teramoto2020deep">
										<p>34. Teramoto, A. <em>et al.</em> Deep learning approach to classification of lung cytological images: Two-step training using actual and synthesized images by progressive growing of generative adversarial networks. <em>PloS one</em> <strong>15</strong>, e0229951 (2020).</p>
								</div>
								<div id="ref-Quiros2019">
										<p>35. Quiros, A. C., Murray-Smith, R. &amp; Yuan, K. Pathology GAN: Learning deep representations of cancer tissue. <a href="http://arxiv.org/abs/1907.02644" class="uri">http://arxiv.org/abs/1907.02644</a> (2019).</p>
								</div>
								<div id="ref-Quiros2020">
										<p>36. Quiros, A. C., Murray-Smith, R. &amp; Yuan, K. Learning a low dimensional manifold of real cancer tissue with PathologyGAN. 1–24 <a href="http://arxiv.org/abs/2004.06517" class="uri">http://arxiv.org/abs/2004.06517</a> (2020).</p>
								</div>
								<div id="ref-thomas2020interpretable">
										<p>37. Thomas, S. M., Lefevre, J. G., Baxter, G. &amp; Hamilton, N. A. Interpretable deep learning systems for multi-class segmentation and classification of non-melanoma skin cancer. <em>Medical Image Analysis</em> <strong>68</strong>, 101915 (2021).</p>
								</div>
								<div id="ref-glorot2011deep">
										<p>38. Glorot, X., Bordes, A. &amp; Bengio, Y. Deep sparse rectifier neural networks. in <em>Proceedings of the fourteenth international conference on artificial intelligence and statistics</em> 315–323 (2011).</p>
								</div>
								<div id="ref-drucker1992improving">
										<p>39. Drucker, H. &amp; Le Cun, Y. Improving generalization performance using double backpropagation. <em>IEEE Transactions on Neural Networks</em> <strong>3</strong>, 991–997 (1992).</p>
								</div>
								<div id="ref-abadi2016tensorflow">
										<p>40. Abadi, M. <em>et al.</em> Tensorflow: A system for large-scale machine learning. in <em>12th {}USENIX{} symposium on operating systems design and implementation ({}OSDI{} 16)</em> 265–283 (2016).</p>
								</div>
								<div id="ref-karras2019style">
										<p>41. Karras, T., Laine, S. &amp; Aila, T. A style-based generator architecture for generative adversarial networks. in <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> 4401–4410 (2019).</p>
								</div>
								<div id="ref-Huang2018">
										<p>42. Huang, H., Li, Z., He, R., Sun, Z. &amp; Tan, T. Introvae: Introspective variational autoencoders for photographic image synthesis. <em>Advances in Neural Information Processing Systems</em> <strong>2018-Decem</strong>, 52–63 (2018).</p>
								</div>
								<div id="ref-Heljakka2019">
										<p>43. Heljakka, A., Solin, A. &amp; Kannala, J. Pioneer networks: Progressively growing generative autoencoder. <em>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</em> <strong>11361 LNCS</strong>, 22–38 (2019).</p>
								</div>
								<div id="ref-Heusel2017">
										<p>44. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B. &amp; Hochreiter, S. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. <em>Advances in Neural Information Processing Systems</em> <strong>2017-Decem</strong>, 6627–6638 (2017).</p>
								</div>
								<div id="ref-Kim2018">
										<p>45. Kim, B. <em>et al.</em> Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). in <em>International conference on machine learning</em> 2673–2682 (2018).</p>
								</div>
								<div id="ref-mConceptAttributionExplaining2020">
										<p>46. Graziani, M., Andrearczyk, V., Marchand-Maillet, S. &amp; Müller, H. Concept attribution: Explaining CNN decisions to physicians. <em>Computers in Biology and Medicine</em> <strong>123</strong>, 103865 (2020).</p>
								</div>
								<div id="ref-abdal2019image2stylegan">
										<p>47. Abdal, R., Qin, Y. &amp; Wonka, P. Image2stylegan: How to embed images into the stylegan latent space? in <em>Proceedings of the IEEE/CVF international conference on computer vision</em> 4432–4441 (2019).</p>
								</div>
								<div id="ref-gabbay2019style">
										<p>48. Gabbay, A. &amp; Hoshen, Y. Style generator inversion for image enhancement and animation. <a href="http://arxiv.org/abs/1906.11880" class="uri">http://arxiv.org/abs/1906.11880</a> (2019).</p>
								</div>
								<div id="ref-McInnes2018">
										<p>49. McInnes, L., Healy, J. &amp; Melville, J. Umap: Uniform manifold approximation and projection for dimension reduction. <a href="http://arxiv.org/abs/1802.03426" class="uri">http://arxiv.org/abs/1802.03426</a> (2018).</p>
								</div>
								<div id="ref-jonker1987shortest">
										<p>50. Jonker, R. &amp; Volgenant, A. A shortest augmenting path algorithm for dense and sparse linear assignment problems. <em>Computing</em> <strong>38</strong>, 325–340 (1987).</p>
								</div>
								<div id="ref-markovtsevLapjvGitHubRepository2017">
										<p>51. Markovtsev, V. Lapjv - GitHub repository. (2017).</p>
								</div>
								<div id="ref-Dogan2020">
										<p>52. Dogan, Y. &amp; Keles, H. Y. Semi-supervised image attribute editing using generative adversarial networks. <em>Neurocomputing</em> <strong>401</strong>, 338–352 (2020).</p>
								</div>
								<div id="ref-Engstrom2019a">
										<p>53. Engstrom, L. <em>et al.</em> Learning perceptually-aligned representations via adversarial robustness. <a href="http://arxiv.org/abs/1906.00945" class="uri">http://arxiv.org/abs/1906.00945</a> (2019).</p>
								</div>
								<div id="ref-jolicoeur2018relativistic">
										<p>54. Jolicoeur-Martineau, A. The relativistic discriminator: A key element missing from standard GAN. <a href="http://arxiv.org/abs/1807.00734" class="uri">http://arxiv.org/abs/1807.00734</a> (2018).</p>
								</div>
						</div>
		</body>
</html>
